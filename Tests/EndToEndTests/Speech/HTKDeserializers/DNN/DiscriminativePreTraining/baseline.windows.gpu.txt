CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3550 @ 3.07GHz
    Hardware threads: 4
    Total Memory: 12580388 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
08/04/2016 09:18:10: -------------------------------------------------------------------
08/04/2016 09:18:10: Build info: 

08/04/2016 09:18:10: 		Built time: Aug  4 2016 06:18:04
08/04/2016 09:18:10: 		Last modified date: Thu Aug  4 03:39:14 2016
08/04/2016 09:18:10: 		Build type: Release
08/04/2016 09:18:10: 		Build target: GPU
08/04/2016 09:18:10: 		With 1bit-SGD: no
08/04/2016 09:18:10: 		Math lib: mkl
08/04/2016 09:18:10: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
08/04/2016 09:18:10: 		CUB_PATH: C:\src\cub-1.4.1
08/04/2016 09:18:10: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
08/04/2016 09:18:10: 		Build Branch: HEAD
08/04/2016 09:18:10: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
08/04/2016 09:18:10: 		Built by svcphil on dphaim-26-new
08/04/2016 09:18:10: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
08/04/2016 09:18:10: -------------------------------------------------------------------
08/04/2016 09:18:10: -------------------------------------------------------------------
08/04/2016 09:18:10: GPU info:

08/04/2016 09:18:10: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
08/04/2016 09:18:10: -------------------------------------------------------------------

08/04/2016 09:18:10: Running on cntk-muc02 at 2016/08/04 09:18:10
08/04/2016 09:18:10: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu  DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu  DeviceId=0  timestamping=true



08/04/2016 09:18:10: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/04/2016 09:18:10: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

08/04/2016 09:18:10: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/04/2016 09:18:10: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/04/2016 09:18:10: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu
DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

08/04/2016 09:18:10: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/04/2016 09:18:10: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:DataDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf"
        labelMappingFile = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
08/04/2016 09:18:10: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/04/2016 09:18:10: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
08/04/2016 09:18:10: Precision = "float"
08/04/2016 09:18:10: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
08/04/2016 09:18:10: CNTKCommandTrainInfo: dptPre1 : 2
08/04/2016 09:18:10: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
08/04/2016 09:18:10: CNTKCommandTrainInfo: dptPre2 : 2
08/04/2016 09:18:10: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
08/04/2016 09:18:10: CNTKCommandTrainInfo: speechTrain : 4
08/04/2016 09:18:10: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

08/04/2016 09:18:10: ##############################################################################
08/04/2016 09:18:10: #                                                                            #
08/04/2016 09:18:10: # Action "train"                                                             #
08/04/2016 09:18:10: #                                                                            #
08/04/2016 09:18:10: ##############################################################################

08/04/2016 09:18:10: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 09:18:11: Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 09:18:11: Created model with 19 nodes on GPU 0.

08/04/2016 09:18:11: Training criterion node(s):
08/04/2016 09:18:11: 	ce = CrossEntropyWithSoftmax

08/04/2016 09:18:11: Evaluation criterion node(s):

08/04/2016 09:18:11: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
00000004E80866C0: {[features Value[363 x *]] }
00000004E80870C0: {[labels Value[132 x *]] }
00000004E8087160: {[globalMean Value[363 x 1]] }
00000004E80872A0: {[globalInvStd Value[363 x 1]] }
00000004E8087660: {[globalPrior Value[132 x 1]] }
00000004E8087A20: {[HL1.W Value[512 x 363]] }
00000004EE468B70: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
00000004EE468F30: {[logPrior Value[132 x 1]] }
00000004EE4691B0: {[err Value[1]] }
00000004EE469250: {[HL1.t Value[512 x *]] }
00000004EE4692F0: {[OL.b Gradient[132 x 1]] }
00000004EE469610: {[scaledLogLikelihood Value[132 x 1 x *]] }
00000004EE4697F0: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
00000004EE469890: {[featNorm Value[363 x *]] }
00000004EE469C50: {[ce Gradient[1]] }
00000004EE469CF0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
00000004EE46A1F0: {[HL1.b Value[512 x 1]] }
00000004EE46A290: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
00000004EE46A330: {[OL.W Value[132 x 512]] }
00000004EE46A470: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
00000004EE46A5B0: {[OL.b Value[132 x 1]] }
00000004EE46A650: {[OL.t Gradient[132 x 1 x *]] }
00000004EE46A790: {[ce Value[1]] }

08/04/2016 09:18:11: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 09:18:11: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 09:18:11: Starting minibatch loop.
08/04/2016 09:18:11:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.77997551 * 2560; err = 0.81484375 * 2560; time = 0.2366s; samplesPerSecond = 10818.9
08/04/2016 09:18:11:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.92382545 * 2560; err = 0.71328125 * 2560; time = 0.0239s; samplesPerSecond = 107018.9
08/04/2016 09:18:11:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55630035 * 2560; err = 0.65859375 * 2560; time = 0.0240s; samplesPerSecond = 106858.1
08/04/2016 09:18:11:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.30158844 * 2560; err = 0.61523438 * 2560; time = 0.0239s; samplesPerSecond = 106889.4
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.04897461 * 2560; err = 0.56328125 * 2560; time = 0.0223s; samplesPerSecond = 114721.0
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.85978241 * 2560; err = 0.52070313 * 2560; time = 0.0203s; samplesPerSecond = 126282.6
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.76588745 * 2560; err = 0.50312500 * 2560; time = 0.0203s; samplesPerSecond = 126301.2
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.74416809 * 2560; err = 0.49375000 * 2560; time = 0.0203s; samplesPerSecond = 125947.1
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.64636383 * 2560; err = 0.47539063 * 2560; time = 0.0203s; samplesPerSecond = 126276.3
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.60569763 * 2560; err = 0.46171875 * 2560; time = 0.0203s; samplesPerSecond = 126021.5
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.51603851 * 2560; err = 0.44648437 * 2560; time = 0.0203s; samplesPerSecond = 126002.9
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51614990 * 2560; err = 0.44765625 * 2560; time = 0.0203s; samplesPerSecond = 126108.4
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.47455750 * 2560; err = 0.43398437 * 2560; time = 0.0203s; samplesPerSecond = 125829.4
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.46740112 * 2560; err = 0.42929688 * 2560; time = 0.0206s; samplesPerSecond = 124217.6
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.41627502 * 2560; err = 0.42734375 * 2560; time = 0.0204s; samplesPerSecond = 125786.2
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.42307129 * 2560; err = 0.41015625 * 2560; time = 0.0204s; samplesPerSecond = 125718.2
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.38768005 * 2560; err = 0.40781250 * 2560; time = 0.0204s; samplesPerSecond = 125755.3
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.38003235 * 2560; err = 0.40039063 * 2560; time = 0.0221s; samplesPerSecond = 115706.2
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33673401 * 2560; err = 0.39921875 * 2560; time = 0.0228s; samplesPerSecond = 112458.3
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.33659973 * 2560; err = 0.40117188 * 2560; time = 0.0229s; samplesPerSecond = 112000.7
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.33715210 * 2560; err = 0.40507813 * 2560; time = 0.0239s; samplesPerSecond = 107054.7
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.36712036 * 2560; err = 0.41406250 * 2560; time = 0.0239s; samplesPerSecond = 106893.8
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.36091919 * 2560; err = 0.42304687 * 2560; time = 0.0239s; samplesPerSecond = 107072.7
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27734070 * 2560; err = 0.38320312 * 2560; time = 0.0238s; samplesPerSecond = 107621.8
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.27654419 * 2560; err = 0.38593750 * 2560; time = 0.0203s; samplesPerSecond = 126295.0
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27127075 * 2560; err = 0.38906250 * 2560; time = 0.0203s; samplesPerSecond = 126089.7
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.27654114 * 2560; err = 0.38671875 * 2560; time = 0.0225s; samplesPerSecond = 113777.8
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29538574 * 2560; err = 0.40000000 * 2560; time = 0.0240s; samplesPerSecond = 106849.2
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.23519592 * 2560; err = 0.37226562 * 2560; time = 0.0221s; samplesPerSecond = 116047.1
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.27269897 * 2560; err = 0.37734375 * 2560; time = 0.0234s; samplesPerSecond = 109233.7
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.29835205 * 2560; err = 0.38554688 * 2560; time = 0.0237s; samplesPerSecond = 108167.5
08/04/2016 09:18:12:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.27160950 * 2560; err = 0.38125000 * 2560; time = 0.0234s; samplesPerSecond = 109364.3
08/04/2016 09:18:12: Finished Epoch[ 1 of 2]: [Training] ce = 1.62585106 * 81920; err = 0.46021729 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.06882s
08/04/2016 09:18:12: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech.1'

08/04/2016 09:18:12: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 09:18:12: Starting minibatch loop.
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.21941128 * 2560; err = 0.38007812 * 2560; time = 0.0252s; samplesPerSecond = 101672.0
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.16789989 * 2560; err = 0.35898438 * 2560; time = 0.0239s; samplesPerSecond = 107319.5
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.22954521 * 2560; err = 0.37929687 * 2560; time = 0.0240s; samplesPerSecond = 106697.8
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.23093147 * 2560; err = 0.37421875 * 2560; time = 0.0240s; samplesPerSecond = 106880.4
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.23917122 * 2560; err = 0.36835937 * 2560; time = 0.0239s; samplesPerSecond = 107113.0
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.12390594 * 2560; err = 0.34140625 * 2560; time = 0.0240s; samplesPerSecond = 106653.3
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.17119217 * 2560; err = 0.35273437 * 2560; time = 0.0239s; samplesPerSecond = 106951.9
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16904068 * 2560; err = 0.34492187 * 2560; time = 0.0239s; samplesPerSecond = 107068.2
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.14490128 * 2560; err = 0.34687500 * 2560; time = 0.0240s; samplesPerSecond = 106777.9
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.23116379 * 2560; err = 0.36484375 * 2560; time = 0.0240s; samplesPerSecond = 106831.4
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.18010635 * 2560; err = 0.36132813 * 2560; time = 0.0240s; samplesPerSecond = 106844.7
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.16575470 * 2560; err = 0.34765625 * 2560; time = 0.0239s; samplesPerSecond = 107068.2
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.15317993 * 2560; err = 0.36484375 * 2560; time = 0.0240s; samplesPerSecond = 106791.3
08/04/2016 09:18:12:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.15339508 * 2560; err = 0.35664062 * 2560; time = 0.0240s; samplesPerSecond = 106884.9
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.21201782 * 2560; err = 0.35976562 * 2560; time = 0.0240s; samplesPerSecond = 106804.6
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.11846924 * 2560; err = 0.33750000 * 2560; time = 0.0239s; samplesPerSecond = 107090.6
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.15517273 * 2560; err = 0.35703125 * 2560; time = 0.0240s; samplesPerSecond = 106755.6
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.16442261 * 2560; err = 0.34648438 * 2560; time = 0.0239s; samplesPerSecond = 106969.7
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10736237 * 2560; err = 0.34570313 * 2560; time = 0.0240s; samplesPerSecond = 106813.5
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.13406830 * 2560; err = 0.36210938 * 2560; time = 0.0240s; samplesPerSecond = 106835.8
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.09407654 * 2560; err = 0.34687500 * 2560; time = 0.0238s; samplesPerSecond = 107748.6
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.16549683 * 2560; err = 0.35195312 * 2560; time = 0.0237s; samplesPerSecond = 107957.7
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12700806 * 2560; err = 0.34687500 * 2560; time = 0.0237s; samplesPerSecond = 108026.0
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.04887695 * 2560; err = 0.32812500 * 2560; time = 0.0233s; samplesPerSecond = 109682.9
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.16085510 * 2560; err = 0.34843750 * 2560; time = 0.0202s; samplesPerSecond = 126971.5
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.17593079 * 2560; err = 0.34453125 * 2560; time = 0.0203s; samplesPerSecond = 126413.5
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.10052795 * 2560; err = 0.34570313 * 2560; time = 0.0203s; samplesPerSecond = 126332.4
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09912109 * 2560; err = 0.33554688 * 2560; time = 0.0203s; samplesPerSecond = 126369.8
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.12586365 * 2560; err = 0.33828125 * 2560; time = 0.0202s; samplesPerSecond = 126927.5
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10348206 * 2560; err = 0.34257813 * 2560; time = 0.0202s; samplesPerSecond = 126738.9
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.09067688 * 2560; err = 0.33242187 * 2560; time = 0.0202s; samplesPerSecond = 126451.0
08/04/2016 09:18:13:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.11611328 * 2560; err = 0.34609375 * 2560; time = 0.0200s; samplesPerSecond = 127891.3
08/04/2016 09:18:13: Finished Epoch[ 2 of 2]: [Training] ce = 1.15247316 * 81920; err = 0.35181885 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.739135s
08/04/2016 09:18:13: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech'
08/04/2016 09:18:13: CNTKCommandTrainEnd: dptPre1

08/04/2016 09:18:13: Action "train" complete.


08/04/2016 09:18:13: ##############################################################################
08/04/2016 09:18:13: #                                                                            #
08/04/2016 09:18:13: # Action "edit"                                                              #
08/04/2016 09:18:13: #                                                                            #
08/04/2016 09:18:13: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 09:18:13: Action "edit" complete.


08/04/2016 09:18:13: ##############################################################################
08/04/2016 09:18:13: #                                                                            #
08/04/2016 09:18:13: # Action "train"                                                             #
08/04/2016 09:18:13: #                                                                            #
08/04/2016 09:18:13: ##############################################################################

08/04/2016 09:18:13: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 09:18:13: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 09:18:13: Loaded model with 24 nodes on GPU 0.

08/04/2016 09:18:13: Training criterion node(s):
08/04/2016 09:18:13: 	ce = CrossEntropyWithSoftmax

08/04/2016 09:18:13: Evaluation criterion node(s):

08/04/2016 09:18:13: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
00000004D9AC09A0: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
00000004D9AC0C20: {[OL.t Gradient[132 x 1 x *3]] }
00000004D9AC0E00: {[OL.b Gradient[132 x 1]] }
00000004E8085CC0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
00000004E8086A80: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
00000004E80870C0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
00000004E8087160: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
00000004E8087200: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
00000004E80872A0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
00000004E8087520: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
00000004E8087980: {[ce Gradient[1]] }
00000004E8087A20: {[HL1.t Value[512 x *3]] }
00000004EE469110: {[globalMean Value[363 x 1]] }
00000004EE4691B0: {[HL1.b Value[512 x 1]] }
00000004EE4699D0: {[globalInvStd Value[363 x 1]] }
00000004EE469ED0: {[HL1.W Value[512 x 363]] }
00000004EE46A0B0: {[HL2.b Value[512 x 1]] }
00000004EE46A330: {[features Value[363 x *3]] }
00000004EE46A830: {[globalPrior Value[132 x 1]] }
00000004EE952440: {[HL2.W Value[512 x 512]] }
00000004EE952620: {[labels Value[132 x *3]] }
00000004EE952940: {[OL.b Value[132 x 1]] }
00000004EE952BC0: {[OL.W Value[132 x 512]] }
00000004EE9530C0: {[err Value[1]] }
00000004EE9532A0: {[scaledLogLikelihood Value[132 x 1 x *3]] }
00000004EE953340: {[ce Value[1]] }
00000004EE9535C0: {[logPrior Value[132 x 1]] }
00000004EE953700: {[featNorm Value[363 x *3]] }

08/04/2016 09:18:13: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 09:18:13: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 09:18:14: Starting minibatch loop.
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.80204735 * 2560; err = 0.81562500 * 2560; time = 0.0362s; samplesPerSecond = 70743.6
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.58233833 * 2560; err = 0.65273437 * 2560; time = 0.0289s; samplesPerSecond = 88529.2
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.10747833 * 2560; err = 0.58125000 * 2560; time = 0.0271s; samplesPerSecond = 94464.9
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.87515259 * 2560; err = 0.53828125 * 2560; time = 0.0277s; samplesPerSecond = 92402.1
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.71971283 * 2560; err = 0.49062500 * 2560; time = 0.0277s; samplesPerSecond = 92492.2
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.58809204 * 2560; err = 0.46757813 * 2560; time = 0.0277s; samplesPerSecond = 92428.8
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.49401703 * 2560; err = 0.43828125 * 2560; time = 0.0279s; samplesPerSecond = 91647.9
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.46566162 * 2560; err = 0.42539063 * 2560; time = 0.0279s; samplesPerSecond = 91684.0
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.40990295 * 2560; err = 0.41757813 * 2560; time = 0.0274s; samplesPerSecond = 93274.1
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.40757446 * 2560; err = 0.42265625 * 2560; time = 0.0279s; samplesPerSecond = 91641.3
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.34674072 * 2560; err = 0.40195313 * 2560; time = 0.0285s; samplesPerSecond = 89717.5
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.35474243 * 2560; err = 0.40507813 * 2560; time = 0.0287s; samplesPerSecond = 89260.8
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.33014374 * 2560; err = 0.38984375 * 2560; time = 0.0286s; samplesPerSecond = 89651.5
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.27635803 * 2560; err = 0.37070313 * 2560; time = 0.0286s; samplesPerSecond = 89488.6
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.27579498 * 2560; err = 0.38867188 * 2560; time = 0.0286s; samplesPerSecond = 89407.3
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.27351990 * 2560; err = 0.36875000 * 2560; time = 0.0284s; samplesPerSecond = 90242.5
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.27338867 * 2560; err = 0.37695313 * 2560; time = 0.0286s; samplesPerSecond = 89523.0
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28978577 * 2560; err = 0.37890625 * 2560; time = 0.0286s; samplesPerSecond = 89376.1
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.24952393 * 2560; err = 0.38125000 * 2560; time = 0.0285s; samplesPerSecond = 89830.9
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.22552185 * 2560; err = 0.36328125 * 2560; time = 0.0284s; samplesPerSecond = 90042.6
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.23660278 * 2560; err = 0.37851563 * 2560; time = 0.0283s; samplesPerSecond = 90421.0
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.27010498 * 2560; err = 0.38632813 * 2560; time = 0.0286s; samplesPerSecond = 89657.8
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.24254456 * 2560; err = 0.38476563 * 2560; time = 0.0280s; samplesPerSecond = 91490.7
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.17969666 * 2560; err = 0.35859375 * 2560; time = 0.0269s; samplesPerSecond = 95043.6
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.18419189 * 2560; err = 0.35000000 * 2560; time = 0.0278s; samplesPerSecond = 92209.1
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.22044678 * 2560; err = 0.37656250 * 2560; time = 0.0276s; samplesPerSecond = 92817.5
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.16026001 * 2560; err = 0.34335938 * 2560; time = 0.0283s; samplesPerSecond = 90382.7
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.20395508 * 2560; err = 0.37812500 * 2560; time = 0.0284s; samplesPerSecond = 90007.7
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.17126465 * 2560; err = 0.35000000 * 2560; time = 0.0285s; samplesPerSecond = 89938.2
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20445862 * 2560; err = 0.35625000 * 2560; time = 0.0285s; samplesPerSecond = 89944.5
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.22648621 * 2560; err = 0.35742188 * 2560; time = 0.0273s; samplesPerSecond = 93649.4
08/04/2016 09:18:14:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.20564880 * 2560; err = 0.36210938 * 2560; time = 0.0279s; samplesPerSecond = 91874.8
08/04/2016 09:18:14: Finished Epoch[ 1 of 2]: [Training] ce = 1.46416121 * 81920; err = 0.42054443 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.06261s
08/04/2016 09:18:14: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.1'

08/04/2016 09:18:14: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 09:18:14: Starting minibatch loop.
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.14285164 * 2560; err = 0.35781250 * 2560; time = 0.0290s; samplesPerSecond = 88218.1
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.10878162 * 2560; err = 0.34882812 * 2560; time = 0.0277s; samplesPerSecond = 92342.1
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.16119900 * 2560; err = 0.35390625 * 2560; time = 0.0271s; samplesPerSecond = 94552.2
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.15388832 * 2560; err = 0.34375000 * 2560; time = 0.0263s; samplesPerSecond = 97220.1
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.20692520 * 2560; err = 0.36601563 * 2560; time = 0.0269s; samplesPerSecond = 95086.0
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.06982346 * 2560; err = 0.32656250 * 2560; time = 0.0287s; samplesPerSecond = 89139.6
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.15059509 * 2560; err = 0.34023437 * 2560; time = 0.0293s; samplesPerSecond = 87351.1
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.11620331 * 2560; err = 0.33320312 * 2560; time = 0.0289s; samplesPerSecond = 88520.1
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.10543747 * 2560; err = 0.33476563 * 2560; time = 0.0280s; samplesPerSecond = 91529.9
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.14773560 * 2560; err = 0.34921875 * 2560; time = 0.0274s; samplesPerSecond = 93372.7
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.11638947 * 2560; err = 0.33437500 * 2560; time = 0.0286s; samplesPerSecond = 89494.8
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.10666580 * 2560; err = 0.33281250 * 2560; time = 0.0288s; samplesPerSecond = 88882.7
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.10289612 * 2560; err = 0.34687500 * 2560; time = 0.0279s; samplesPerSecond = 91670.8
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.09333954 * 2560; err = 0.33671875 * 2560; time = 0.0277s; samplesPerSecond = 92468.8
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.17858887 * 2560; err = 0.35429688 * 2560; time = 0.0281s; samplesPerSecond = 90951.1
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.08380585 * 2560; err = 0.31875000 * 2560; time = 0.0272s; samplesPerSecond = 94062.3
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.09735413 * 2560; err = 0.33671875 * 2560; time = 0.0279s; samplesPerSecond = 91618.4
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11444092 * 2560; err = 0.34023437 * 2560; time = 0.0288s; samplesPerSecond = 88882.7
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.07517548 * 2560; err = 0.33828125 * 2560; time = 0.0290s; samplesPerSecond = 88349.0
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08728180 * 2560; err = 0.35507813 * 2560; time = 0.0273s; samplesPerSecond = 93769.5
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.05493469 * 2560; err = 0.33125000 * 2560; time = 0.0281s; samplesPerSecond = 91249.3
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.14903259 * 2560; err = 0.34765625 * 2560; time = 0.0280s; samplesPerSecond = 91520.1
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.10326538 * 2560; err = 0.34492187 * 2560; time = 0.0277s; samplesPerSecond = 92295.5
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.05078735 * 2560; err = 0.32421875 * 2560; time = 0.0273s; samplesPerSecond = 93656.3
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.14259338 * 2560; err = 0.34843750 * 2560; time = 0.0277s; samplesPerSecond = 92415.4
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13173523 * 2560; err = 0.33320312 * 2560; time = 0.0275s; samplesPerSecond = 92952.3
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.06638184 * 2560; err = 0.33359375 * 2560; time = 0.0280s; samplesPerSecond = 91565.9
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.07181702 * 2560; err = 0.33906250 * 2560; time = 0.0285s; samplesPerSecond = 89931.8
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.08930969 * 2560; err = 0.32695313 * 2560; time = 0.0284s; samplesPerSecond = 90055.2
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.06550903 * 2560; err = 0.33046875 * 2560; time = 0.0287s; samplesPerSecond = 89180.0
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06104736 * 2560; err = 0.32304688 * 2560; time = 0.0284s; samplesPerSecond = 89985.6
08/04/2016 09:18:15:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.03882141 * 2560; err = 0.33359375 * 2560; time = 0.0280s; samplesPerSecond = 91366.6
08/04/2016 09:18:15: Finished Epoch[ 2 of 2]: [Training] ce = 1.10764418 * 81920; err = 0.33952637 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.900077s
08/04/2016 09:18:15: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech'
08/04/2016 09:18:15: CNTKCommandTrainEnd: dptPre2

08/04/2016 09:18:15: Action "train" complete.


08/04/2016 09:18:15: ##############################################################################
08/04/2016 09:18:15: #                                                                            #
08/04/2016 09:18:15: # Action "edit"                                                              #
08/04/2016 09:18:15: #                                                                            #
08/04/2016 09:18:15: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 09:18:16: Action "edit" complete.


08/04/2016 09:18:16: ##############################################################################
08/04/2016 09:18:16: #                                                                            #
08/04/2016 09:18:16: # Action "train"                                                             #
08/04/2016 09:18:16: #                                                                            #
08/04/2016 09:18:16: ##############################################################################

08/04/2016 09:18:16: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.scp ... 948 entries
total 132 state names in state list C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/state.list
htkmlfreader: reading MLF file C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 09:18:16: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 09:18:16: Loaded model with 29 nodes on GPU 0.

08/04/2016 09:18:16: Training criterion node(s):
08/04/2016 09:18:16: 	ce = CrossEntropyWithSoftmax

08/04/2016 09:18:16: Evaluation criterion node(s):

08/04/2016 09:18:16: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
00000004D9AC0C20: {[OL.b Value[132 x 1]] }
00000004E8085CC0: {[HL3.W Value[512 x 512]] }
00000004E80870C0: {[labels Value[132 x *6]] }
00000004E8087660: {[HL3.b Value[512 x 1]] }
00000004EE468A30: {[globalPrior Value[132 x 1]] }
00000004EE468FD0: {[globalMean Value[363 x 1]] }
00000004EE469430: {[HL1.b Value[512 x 1]] }
00000004EE4699D0: {[globalInvStd Value[363 x 1]] }
00000004EE469D90: {[features Value[363 x *6]] }
00000004EE952B20: {[HL2.b Value[512 x 1]] }
00000004EE9532A0: {[HL2.W Value[512 x 512]] }
00000004EE953AC0: {[HL1.W Value[512 x 363]] }
00000004F8B6E340: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
00000004F8B6E3E0: {[OL.t Gradient[132 x 1 x *6]] }
00000004F8B6E480: {[HL1.t Value[512 x *6]] }
00000004F8B6E5C0: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
00000004F8B6E660: {[OL.b Gradient[132 x 1]] }
00000004F8B6E700: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
00000004F8B6EA20: {[logPrior Value[132 x 1]] }
00000004F8B6EAC0: {[ce Gradient[1]] }
00000004F8B6EE80: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
00000004F8B6F1A0: {[err Value[1]] }
00000004F8B6F2E0: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
00000004F8B6F380: {[scaledLogLikelihood Value[132 x 1 x *6]] }
00000004F8B6F4C0: {[featNorm Value[363 x *6]] }
00000004F8B6F560: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
00000004F8B6F6A0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
00000004F8B6F9C0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
00000004F8B6FCE0: {[OL.W Value[132 x 512]] }
00000004F8B6FEC0: {[ce Value[1]] }
00000004F8B6FF60: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
00000004F8B70000: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
00000004F8B701E0: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }

08/04/2016 09:18:16: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 09:18:16: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 09:18:16: Starting minibatch loop.
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 3.97832451 * 2560; err = 0.81679687 * 2560; time = 0.0449s; samplesPerSecond = 57060.1
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.51557732 * 2560; err = 0.62617188 * 2560; time = 0.0341s; samplesPerSecond = 75099.7
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 1.98839569 * 2560; err = 0.54296875 * 2560; time = 0.0343s; samplesPerSecond = 74685.6
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.74622650 * 2560; err = 0.49648437 * 2560; time = 0.0342s; samplesPerSecond = 74840.7
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.59862823 * 2560; err = 0.45937500 * 2560; time = 0.0330s; samplesPerSecond = 77507.6
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.47477646 * 2560; err = 0.43085937 * 2560; time = 0.0326s; samplesPerSecond = 78602.4
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.38905487 * 2560; err = 0.39882812 * 2560; time = 0.0339s; samplesPerSecond = 75509.5
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.36244354 * 2560; err = 0.39687500 * 2560; time = 0.0341s; samplesPerSecond = 75172.5
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.30323334 * 2560; err = 0.39218750 * 2560; time = 0.0342s; samplesPerSecond = 74766.4
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.31909485 * 2560; err = 0.39531250 * 2560; time = 0.0343s; samplesPerSecond = 74674.8
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.25892334 * 2560; err = 0.37812500 * 2560; time = 0.0342s; samplesPerSecond = 74794.8
08/04/2016 09:18:16:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.26951752 * 2560; err = 0.37890625 * 2560; time = 0.0334s; samplesPerSecond = 76747.8
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.23129730 * 2560; err = 0.35898438 * 2560; time = 0.0346s; samplesPerSecond = 73952.1
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.19243317 * 2560; err = 0.35273437 * 2560; time = 0.0350s; samplesPerSecond = 73092.7
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.20105743 * 2560; err = 0.36796875 * 2560; time = 0.0354s; samplesPerSecond = 72373.6
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.20111847 * 2560; err = 0.35507813 * 2560; time = 0.0344s; samplesPerSecond = 74399.1
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.21232910 * 2560; err = 0.36328125 * 2560; time = 0.0342s; samplesPerSecond = 74755.4
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.22917175 * 2560; err = 0.36406250 * 2560; time = 0.0343s; samplesPerSecond = 74574.7
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.18077393 * 2560; err = 0.35937500 * 2560; time = 0.0343s; samplesPerSecond = 74657.3
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.16705627 * 2560; err = 0.34804687 * 2560; time = 0.0342s; samplesPerSecond = 74755.4
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.17594604 * 2560; err = 0.35195312 * 2560; time = 0.0342s; samplesPerSecond = 74884.5
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.21995544 * 2560; err = 0.37070313 * 2560; time = 0.0340s; samplesPerSecond = 75210.1
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.18804016 * 2560; err = 0.36210938 * 2560; time = 0.0343s; samplesPerSecond = 74738.0
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.12375488 * 2560; err = 0.34609375 * 2560; time = 0.0341s; samplesPerSecond = 75132.8
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.13181458 * 2560; err = 0.34062500 * 2560; time = 0.0342s; samplesPerSecond = 74827.5
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.17164307 * 2560; err = 0.35664062 * 2560; time = 0.0342s; samplesPerSecond = 74818.8
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.10361328 * 2560; err = 0.32890625 * 2560; time = 0.0340s; samplesPerSecond = 75280.8
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.15154419 * 2560; err = 0.36054687 * 2560; time = 0.0343s; samplesPerSecond = 74703.1
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12030029 * 2560; err = 0.33828125 * 2560; time = 0.0340s; samplesPerSecond = 75265.3
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.16241760 * 2560; err = 0.34453125 * 2560; time = 0.0342s; samplesPerSecond = 74862.6
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.19964294 * 2560; err = 0.35234375 * 2560; time = 0.0339s; samplesPerSecond = 75465.0
08/04/2016 09:18:17:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.16530151 * 2560; err = 0.35000000 * 2560; time = 0.0340s; samplesPerSecond = 75234.4
08/04/2016 09:18:17: Finished Epoch[ 1 of 4]: [Training] ce = 1.39791899 * 81920; err = 0.39953613 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.25659s
08/04/2016 09:18:17: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.1'

08/04/2016 09:18:17: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 09:18:17: Starting minibatch loop.
08/04/2016 09:18:17:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.27437363 * 5120; err = 0.37675781 * 5120; time = 0.0553s; samplesPerSecond = 92585.9
08/04/2016 09:18:17:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.46351871 * 5120; err = 0.39257813 * 5120; time = 0.0458s; samplesPerSecond = 111831.9
08/04/2016 09:18:17:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.21849728 * 5120; err = 0.36445312 * 5120; time = 0.0456s; samplesPerSecond = 112197.0
08/04/2016 09:18:17:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.12572250 * 5120; err = 0.33671875 * 5120; time = 0.0448s; samplesPerSecond = 114252.6
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14999657 * 5120; err = 0.34902344 * 5120; time = 0.0456s; samplesPerSecond = 112224.1
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.12802773 * 5120; err = 0.34570313 * 5120; time = 0.0457s; samplesPerSecond = 111993.4
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08616943 * 5120; err = 0.34511719 * 5120; time = 0.0461s; samplesPerSecond = 111082.2
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.11425552 * 5120; err = 0.33242187 * 5120; time = 0.0463s; samplesPerSecond = 110523.5
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.11112213 * 5120; err = 0.34355469 * 5120; time = 0.0463s; samplesPerSecond = 110552.1
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.07531433 * 5120; err = 0.33750000 * 5120; time = 0.0462s; samplesPerSecond = 110870.5
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.07992172 * 5120; err = 0.33066406 * 5120; time = 0.0462s; samplesPerSecond = 110887.3
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06415253 * 5120; err = 0.32500000 * 5120; time = 0.0459s; samplesPerSecond = 111476.4
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.14421082 * 5120; err = 0.35175781 * 5120; time = 0.0458s; samplesPerSecond = 111809.9
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.08667908 * 5120; err = 0.33789063 * 5120; time = 0.0457s; samplesPerSecond = 112155.3
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.09553375 * 5120; err = 0.33144531 * 5120; time = 0.0464s; samplesPerSecond = 110382.9
08/04/2016 09:18:18:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06522064 * 5120; err = 0.33632812 * 5120; time = 0.0462s; samplesPerSecond = 110841.7
08/04/2016 09:18:18: Finished Epoch[ 2 of 4]: [Training] ce = 1.14266977 * 81920; err = 0.34605713 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.746757s
08/04/2016 09:18:18: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.2'

08/04/2016 09:18:18: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

08/04/2016 09:18:18: Starting minibatch loop.
08/04/2016 09:18:18:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.12845669 * 5120; err = 0.35507813 * 5120; time = 0.0457s; samplesPerSecond = 112037.5
08/04/2016 09:18:18:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.08134251 * 5120; err = 0.33417969 * 5120; time = 0.0460s; samplesPerSecond = 111331.0
08/04/2016 09:18:18:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.07063103 * 5120; err = 0.33007813 * 5120; time = 0.0464s; samplesPerSecond = 110311.5
08/04/2016 09:18:18:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.08787003 * 5120; err = 0.33808594 * 5120; time = 0.0464s; samplesPerSecond = 110406.7
08/04/2016 09:18:18:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.10042801 * 5120; err = 0.33593750 * 5120; time = 0.0464s; samplesPerSecond = 110230.8
08/04/2016 09:18:18:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.08486290 * 5120; err = 0.33144531 * 5120; time = 0.0461s; samplesPerSecond = 111159.4
08/04/2016 09:18:18:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08302765 * 5120; err = 0.33515625 * 5120; time = 0.0455s; samplesPerSecond = 112564.6
08/04/2016 09:18:18:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.04633789 * 5120; err = 0.32304688 * 5120; time = 0.0451s; samplesPerSecond = 113460.1
08/04/2016 09:18:19:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.06315689 * 5120; err = 0.32832031 * 5120; time = 0.0462s; samplesPerSecond = 110745.8
08/04/2016 09:18:19:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.05130920 * 5120; err = 0.32812500 * 5120; time = 0.0462s; samplesPerSecond = 110765.0
08/04/2016 09:18:19:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.08840027 * 5120; err = 0.33593750 * 5120; time = 0.0443s; samplesPerSecond = 115701.0
08/04/2016 09:18:19:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.09818268 * 5120; err = 0.34179688 * 5120; time = 0.0463s; samplesPerSecond = 110683.6
08/04/2016 09:18:19:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05762177 * 5120; err = 0.32832031 * 5120; time = 0.0463s; samplesPerSecond = 110621.4
08/04/2016 09:18:19:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02346344 * 5120; err = 0.32382813 * 5120; time = 0.0462s; samplesPerSecond = 110815.3
08/04/2016 09:18:19:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.07292633 * 5120; err = 0.33769531 * 5120; time = 0.0464s; samplesPerSecond = 110363.9
08/04/2016 09:18:19:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.03026276 * 5120; err = 0.31855469 * 5120; time = 0.0464s; samplesPerSecond = 110278.3
08/04/2016 09:18:19: Finished Epoch[ 3 of 4]: [Training] ce = 1.07301750 * 81920; err = 0.33284912 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.738625s
08/04/2016 09:18:19: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.3'

08/04/2016 09:18:19: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

08/04/2016 09:18:19: Starting minibatch loop.
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.04442225 * 5120; err = 0.32519531 * 5120; time = 0.0474s; samplesPerSecond = 107916.7
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.01133960 * 4926; err = 0.31201786 * 4926; time = 0.1025s; samplesPerSecond = 48059.5
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01365585 * 5120; err = 0.31640625 * 5120; time = 0.0464s; samplesPerSecond = 110292.5
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.02049294 * 5120; err = 0.31562500 * 5120; time = 0.0457s; samplesPerSecond = 112113.5
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.18170624 * 5120; err = 0.37226562 * 5120; time = 0.0449s; samplesPerSecond = 113988.0
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.03420181 * 5120; err = 0.32128906 * 5120; time = 0.0465s; samplesPerSecond = 110169.1
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.97196693 * 5120; err = 0.30273438 * 5120; time = 0.0465s; samplesPerSecond = 110218.9
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.00869904 * 5120; err = 0.33007813 * 5120; time = 0.0464s; samplesPerSecond = 110245.0
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.98327942 * 5120; err = 0.30332031 * 5120; time = 0.0463s; samplesPerSecond = 110614.2
08/04/2016 09:18:19:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.96378937 * 5120; err = 0.30468750 * 5120; time = 0.0465s; samplesPerSecond = 110209.4
08/04/2016 09:18:20:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.00055084 * 5120; err = 0.31113281 * 5120; time = 0.0464s; samplesPerSecond = 110375.8
08/04/2016 09:18:20:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.99678116 * 5120; err = 0.30625000 * 5120; time = 0.0463s; samplesPerSecond = 110497.2
08/04/2016 09:18:20:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.97792740 * 5120; err = 0.31308594 * 5120; time = 0.0462s; samplesPerSecond = 110748.2
08/04/2016 09:18:20:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97973938 * 5120; err = 0.30683594 * 5120; time = 0.0463s; samplesPerSecond = 110468.6
08/04/2016 09:18:20:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.00156555 * 5120; err = 0.31269531 * 5120; time = 0.0464s; samplesPerSecond = 110356.7
08/04/2016 09:18:20:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.98950653 * 5120; err = 0.30507812 * 5120; time = 0.0465s; samplesPerSecond = 110164.4
08/04/2016 09:18:20: Finished Epoch[ 4 of 4]: [Training] ce = 1.01127386 * 81920; err = 0.31624756 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.803738s
08/04/2016 09:18:20: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech'
08/04/2016 09:18:20: CNTKCommandTrainEnd: speechTrain

08/04/2016 09:18:20: Action "train" complete.

08/04/2016 09:18:20: __COMPLETED__