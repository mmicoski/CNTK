CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3550 @ 3.07GHz
    Hardware threads: 4
    Total Memory: 12580388 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData
08/04/2016 09:20:55: -------------------------------------------------------------------
08/04/2016 09:20:55: Build info: 

08/04/2016 09:20:55: 		Built time: Aug  4 2016 06:18:04
08/04/2016 09:20:55: 		Last modified date: Thu Aug  4 03:39:14 2016
08/04/2016 09:20:55: 		Build type: Release
08/04/2016 09:20:55: 		Build target: GPU
08/04/2016 09:20:55: 		With 1bit-SGD: no
08/04/2016 09:20:55: 		Math lib: mkl
08/04/2016 09:20:55: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
08/04/2016 09:20:55: 		CUB_PATH: C:\src\cub-1.4.1
08/04/2016 09:20:55: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
08/04/2016 09:20:55: 		Build Branch: HEAD
08/04/2016 09:20:55: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
08/04/2016 09:20:55: 		Built by svcphil on dphaim-26-new
08/04/2016 09:20:55: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
08/04/2016 09:20:55: -------------------------------------------------------------------
08/04/2016 09:20:55: -------------------------------------------------------------------
08/04/2016 09:20:55: GPU info:

08/04/2016 09:20:55: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
08/04/2016 09:20:55: -------------------------------------------------------------------

08/04/2016 09:20:55: Running on cntk-muc02 at 2016/08/04 09:20:55
08/04/2016 09:20:55: Command line: 
C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu  DeviceId=0  timestamping=true



08/04/2016 09:20:55: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/04/2016 09:20:55: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "$RunDir$/models/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.sequence.0"
    editPath  = "$ConfigDir$/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "$DataDir$/glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "$DataDir$/model.overalltying"
            transpFile = "$DataDir$/model.transprob"
        ]
        lattices = [
            denlatTocFile = "$DataDir$/*.lats.toc"
        ]
    ]
]
currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu
DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

08/04/2016 09:20:55: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/04/2016 09:20:55: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/04/2016 09:20:55: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]
currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData
RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu
DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData
ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

08/04/2016 09:20:55: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/04/2016 09:20:55: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
configparameters: cntk_sequence.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:replaceCriterionNode=[
    action = "edit"
    currModel = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]

configparameters: cntk_sequence.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
08/04/2016 09:20:55: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/04/2016 09:20:55: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain replaceCriterionNode sequenceTrain
08/04/2016 09:20:55: Precision = "float"
08/04/2016 09:20:55: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech
08/04/2016 09:20:55: CNTKCommandTrainInfo: dptPre1 : 2
08/04/2016 09:20:55: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech
08/04/2016 09:20:55: CNTKCommandTrainInfo: dptPre2 : 2
08/04/2016 09:20:55: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech
08/04/2016 09:20:55: CNTKCommandTrainInfo: speechTrain : 4
08/04/2016 09:20:55: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence
08/04/2016 09:20:55: CNTKCommandTrainInfo: sequenceTrain : 3
08/04/2016 09:20:55: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11

08/04/2016 09:20:55: ##############################################################################
08/04/2016 09:20:55: #                                                                            #
08/04/2016 09:20:55: # Action "train"                                                             #
08/04/2016 09:20:55: #                                                                            #
08/04/2016 09:20:55: ##############################################################################

08/04/2016 09:20:55: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 09:20:56: Creating virgin network.
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 09:20:56: Created model with 19 nodes on GPU 0.

08/04/2016 09:20:56: Training criterion node(s):
08/04/2016 09:20:56: 	ce = CrossEntropyWithSoftmax

08/04/2016 09:20:56: Evaluation criterion node(s):

08/04/2016 09:20:56: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
00000010D5226240: {[featNorm Value[363 x *]] }
00000010D52266A0: {[OL.t Gradient[132 x 1 x *]] }
00000010D5226740: {[err Value[1]] }
00000010D52267E0: {[logPrior Value[132 x 1]] }
00000010D5226880: {[ce Value[1]] }
00000010D5226920: {[HL1.t Value[512 x *]] }
00000010D5226BA0: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
00000010D5226CE0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
00000010D5226EC0: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
00000010D5226F60: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
00000010D5227000: {[ce Gradient[1]] }
00000010D52270A0: {[OL.b Value[132 x 1]] }
00000010D52275A0: {[OL.b Gradient[132 x 1]] }
00000010D5227780: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
00000010D5227B40: {[scaledLogLikelihood Value[132 x 1 x *]] }
00000010D5227DC0: {[HL1.b Value[512 x 1]] }
00000010D5228040: {[OL.W Value[132 x 512]] }
00000010DE870540: {[features Value[363 x *]] }
00000010DE870EA0: {[globalPrior Value[132 x 1]] }
00000010DE871440: {[HL1.W Value[512 x 363]] }
00000010DE871B20: {[labels Value[132 x *]] }
00000010DE871E40: {[globalMean Value[363 x 1]] }
00000010DE8720C0: {[globalInvStd Value[363 x 1]] }

08/04/2016 09:20:56: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 09:20:56: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 09:20:56: Starting minibatch loop.
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.77997551 * 2560; err = 0.81484375 * 2560; time = 0.2029s; samplesPerSecond = 12617.0
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.92382545 * 2560; err = 0.71328125 * 2560; time = 0.0221s; samplesPerSecond = 115931.5
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55630035 * 2560; err = 0.65859375 * 2560; time = 0.0223s; samplesPerSecond = 114726.2
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.30158844 * 2560; err = 0.61523438 * 2560; time = 0.0221s; samplesPerSecond = 115685.3
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.04897461 * 2560; err = 0.56328125 * 2560; time = 0.0224s; samplesPerSecond = 114352.1
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.85978241 * 2560; err = 0.52070313 * 2560; time = 0.0220s; samplesPerSecond = 116310.8
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.76588745 * 2560; err = 0.50312500 * 2560; time = 0.0204s; samplesPerSecond = 125619.5
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.74416809 * 2560; err = 0.49375000 * 2560; time = 0.0203s; samplesPerSecond = 125848.0
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.64636383 * 2560; err = 0.47539063 * 2560; time = 0.0203s; samplesPerSecond = 125916.1
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.60569763 * 2560; err = 0.46171875 * 2560; time = 0.0204s; samplesPerSecond = 125724.4
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.51603851 * 2560; err = 0.44648437 * 2560; time = 0.0212s; samplesPerSecond = 120806.0
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51614990 * 2560; err = 0.44765625 * 2560; time = 0.0226s; samplesPerSecond = 113460.1
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.47455750 * 2560; err = 0.43398437 * 2560; time = 0.0227s; samplesPerSecond = 112989.4
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.46740112 * 2560; err = 0.42929688 * 2560; time = 0.0223s; samplesPerSecond = 114880.6
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.41627502 * 2560; err = 0.42734375 * 2560; time = 0.0204s; samplesPerSecond = 125656.5
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.42307129 * 2560; err = 0.41015625 * 2560; time = 0.0204s; samplesPerSecond = 125514.8
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.38768005 * 2560; err = 0.40781250 * 2560; time = 0.0204s; samplesPerSecond = 125557.9
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.38003235 * 2560; err = 0.40039063 * 2560; time = 0.0204s; samplesPerSecond = 125724.4
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33673401 * 2560; err = 0.39921875 * 2560; time = 0.0204s; samplesPerSecond = 125668.8
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.33659973 * 2560; err = 0.40117188 * 2560; time = 0.0204s; samplesPerSecond = 125557.9
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.33715210 * 2560; err = 0.40507813 * 2560; time = 0.0204s; samplesPerSecond = 125428.7
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.36712036 * 2560; err = 0.41406250 * 2560; time = 0.0204s; samplesPerSecond = 125570.2
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.36091919 * 2560; err = 0.42304687 * 2560; time = 0.0204s; samplesPerSecond = 125780.0
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27734070 * 2560; err = 0.38320312 * 2560; time = 0.0204s; samplesPerSecond = 125527.1
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.27654419 * 2560; err = 0.38593750 * 2560; time = 0.0219s; samplesPerSecond = 116884.3
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27127075 * 2560; err = 0.38906250 * 2560; time = 0.0213s; samplesPerSecond = 120216.0
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.27654114 * 2560; err = 0.38671875 * 2560; time = 0.0204s; samplesPerSecond = 125551.7
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29538574 * 2560; err = 0.40000000 * 2560; time = 0.0213s; samplesPerSecond = 120357.3
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.23519592 * 2560; err = 0.37226562 * 2560; time = 0.0204s; samplesPerSecond = 125484.0
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.27269897 * 2560; err = 0.37734375 * 2560; time = 0.0204s; samplesPerSecond = 125496.3
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.29835205 * 2560; err = 0.38554688 * 2560; time = 0.0203s; samplesPerSecond = 125848.0
08/04/2016 09:20:57:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.27160950 * 2560; err = 0.38125000 * 2560; time = 0.0208s; samplesPerSecond = 122787.7
08/04/2016 09:20:57: Finished Epoch[ 1 of 2]: [Training] ce = 1.62585106 * 81920; err = 0.46021729 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.01666s
08/04/2016 09:20:57: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech.1'

08/04/2016 09:20:57: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 09:20:57: Starting minibatch loop.
08/04/2016 09:20:57:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.21941128 * 2560; err = 0.38007812 * 2560; time = 0.0265s; samplesPerSecond = 96735.2
08/04/2016 09:20:57:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.16789989 * 2560; err = 0.35898438 * 2560; time = 0.0224s; samplesPerSecond = 114244.9
08/04/2016 09:20:57:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.22954521 * 2560; err = 0.37929687 * 2560; time = 0.0226s; samplesPerSecond = 113510.4
08/04/2016 09:20:57:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.23093147 * 2560; err = 0.37421875 * 2560; time = 0.0228s; samplesPerSecond = 112404.0
08/04/2016 09:20:57:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.23917122 * 2560; err = 0.36835937 * 2560; time = 0.0228s; samplesPerSecond = 112492.9
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.12390594 * 2560; err = 0.34140625 * 2560; time = 0.0237s; samplesPerSecond = 107953.1
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.17119217 * 2560; err = 0.35273437 * 2560; time = 0.0239s; samplesPerSecond = 106942.9
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16904068 * 2560; err = 0.34492187 * 2560; time = 0.0240s; samplesPerSecond = 106867.0
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.14490128 * 2560; err = 0.34687500 * 2560; time = 0.0240s; samplesPerSecond = 106858.1
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.23116379 * 2560; err = 0.36484375 * 2560; time = 0.0239s; samplesPerSecond = 106907.2
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.18010635 * 2560; err = 0.36132813 * 2560; time = 0.0239s; samplesPerSecond = 106978.7
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.16575470 * 2560; err = 0.34765625 * 2560; time = 0.0240s; samplesPerSecond = 106849.2
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.15317993 * 2560; err = 0.36484375 * 2560; time = 0.0240s; samplesPerSecond = 106720.0
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.15339508 * 2560; err = 0.35664062 * 2560; time = 0.0239s; samplesPerSecond = 107054.7
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.21201782 * 2560; err = 0.35976562 * 2560; time = 0.0240s; samplesPerSecond = 106791.3
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.11846924 * 2560; err = 0.33750000 * 2560; time = 0.0239s; samplesPerSecond = 106983.2
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.15517273 * 2560; err = 0.35703125 * 2560; time = 0.0242s; samplesPerSecond = 105951.5
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.16442261 * 2560; err = 0.34648438 * 2560; time = 0.0241s; samplesPerSecond = 106334.4
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10736237 * 2560; err = 0.34570313 * 2560; time = 0.0240s; samplesPerSecond = 106591.2
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.13406830 * 2560; err = 0.36210938 * 2560; time = 0.0239s; samplesPerSecond = 107032.4
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.09407654 * 2560; err = 0.34687500 * 2560; time = 0.0239s; samplesPerSecond = 106889.4
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.16549683 * 2560; err = 0.35195312 * 2560; time = 0.0220s; samplesPerSecond = 116384.8
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12700806 * 2560; err = 0.34687500 * 2560; time = 0.0204s; samplesPerSecond = 125786.2
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.04887695 * 2560; err = 0.32812500 * 2560; time = 0.0222s; samplesPerSecond = 115481.8
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.16085510 * 2560; err = 0.34843750 * 2560; time = 0.0239s; samplesPerSecond = 107072.7
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.17593079 * 2560; err = 0.34453125 * 2560; time = 0.0240s; samplesPerSecond = 106862.6
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.10052795 * 2560; err = 0.34570313 * 2560; time = 0.0239s; samplesPerSecond = 106911.7
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09912109 * 2560; err = 0.33554688 * 2560; time = 0.0239s; samplesPerSecond = 106893.8
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.12586365 * 2560; err = 0.33828125 * 2560; time = 0.0240s; samplesPerSecond = 106795.7
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10348206 * 2560; err = 0.34257813 * 2560; time = 0.0239s; samplesPerSecond = 106978.7
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.09067688 * 2560; err = 0.33242187 * 2560; time = 0.0240s; samplesPerSecond = 106849.2
08/04/2016 09:20:58:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.11611328 * 2560; err = 0.34609375 * 2560; time = 0.0236s; samplesPerSecond = 108626.5
08/04/2016 09:20:58: Finished Epoch[ 2 of 2]: [Training] ce = 1.15247316 * 81920; err = 0.35181885 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.758965s
08/04/2016 09:20:58: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech'
08/04/2016 09:20:58: CNTKCommandTrainEnd: dptPre1

08/04/2016 09:20:58: Action "train" complete.


08/04/2016 09:20:58: ##############################################################################
08/04/2016 09:20:58: #                                                                            #
08/04/2016 09:20:58: # Action "edit"                                                              #
08/04/2016 09:20:58: #                                                                            #
08/04/2016 09:20:58: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 09:20:58: Action "edit" complete.


08/04/2016 09:20:58: ##############################################################################
08/04/2016 09:20:58: #                                                                            #
08/04/2016 09:20:58: # Action "train"                                                             #
08/04/2016 09:20:58: #                                                                            #
08/04/2016 09:20:58: ##############################################################################

08/04/2016 09:20:58: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 09:20:59: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 09:20:59: Loaded model with 24 nodes on GPU 0.

08/04/2016 09:20:59: Training criterion node(s):
08/04/2016 09:20:59: 	ce = CrossEntropyWithSoftmax

08/04/2016 09:20:59: Evaluation criterion node(s):

08/04/2016 09:20:59: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
00000010D5226380: {[OL.b Value[132 x 1]] }
00000010D52266A0: {[err Value[1]] }
00000010D5226740: {[OL.W Value[132 x 512]] }
00000010D5226B00: {[labels Value[132 x *3]] }
00000010D52270A0: {[scaledLogLikelihood Value[132 x 1 x *3]] }
00000010D52275A0: {[ce Value[1]] }
00000010D52276E0: {[logPrior Value[132 x 1]] }
00000010D5228040: {[HL2.W Value[512 x 512]] }
00000010DC3ED9E0: {[HL1.b Value[512 x 1]] }
00000010DC3EDE40: {[HL2.b Value[512 x 1]] }
00000010DC3EDF80: {[globalInvStd Value[363 x 1]] }
00000010DC3EEF20: {[globalMean Value[363 x 1]] }
00000010DC3EEFC0: {[globalPrior Value[132 x 1]] }
00000010DC3EF240: {[HL1.W Value[512 x 363]] }
00000010DC3EF740: {[features Value[363 x *3]] }
00000010DE870540: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
00000010DE870C20: {[featNorm Value[363 x *3]] }
00000010DE870CC0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
00000010DE870EA0: {[HL1.t Value[512 x *3]] }
00000010DE871300: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
00000010DE871440: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
00000010DE871580: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
00000010DE871B20: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
00000010DE871D00: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
00000010DE87C370: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
00000010DE87C4B0: {[ce Gradient[1]] }
00000010DE87C550: {[OL.t Gradient[132 x 1 x *3]] }
00000010DE87C730: {[OL.b Gradient[132 x 1]] }

08/04/2016 09:20:59: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 09:20:59: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 09:20:59: Starting minibatch loop.
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.80204735 * 2560; err = 0.81562500 * 2560; time = 0.0377s; samplesPerSecond = 67973.0
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.58233833 * 2560; err = 0.65273437 * 2560; time = 0.0281s; samplesPerSecond = 91028.7
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.10747833 * 2560; err = 0.58125000 * 2560; time = 0.0286s; samplesPerSecond = 89523.0
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.87515259 * 2560; err = 0.53828125 * 2560; time = 0.0287s; samplesPerSecond = 89239.0
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.71971283 * 2560; err = 0.49062500 * 2560; time = 0.0286s; samplesPerSecond = 89444.8
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.58809204 * 2560; err = 0.46757813 * 2560; time = 0.0287s; samplesPerSecond = 89148.9
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.49401703 * 2560; err = 0.43828125 * 2560; time = 0.0287s; samplesPerSecond = 89142.7
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.46566162 * 2560; err = 0.42539063 * 2560; time = 0.0285s; samplesPerSecond = 89683.0
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.40990295 * 2560; err = 0.41757813 * 2560; time = 0.0286s; samplesPerSecond = 89488.6
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.40757446 * 2560; err = 0.42265625 * 2560; time = 0.0282s; samplesPerSecond = 90934.9
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.34674072 * 2560; err = 0.40195313 * 2560; time = 0.0250s; samplesPerSecond = 102592.9
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.35474243 * 2560; err = 0.40507813 * 2560; time = 0.0257s; samplesPerSecond = 99490.9
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.33014374 * 2560; err = 0.38984375 * 2560; time = 0.0279s; samplesPerSecond = 91595.4
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.27635803 * 2560; err = 0.37070313 * 2560; time = 0.0278s; samplesPerSecond = 92119.5
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.27579498 * 2560; err = 0.38867188 * 2560; time = 0.0287s; samplesPerSecond = 89276.4
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.27351990 * 2560; err = 0.36875000 * 2560; time = 0.0285s; samplesPerSecond = 89780.5
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.27338867 * 2560; err = 0.37695313 * 2560; time = 0.0286s; samplesPerSecond = 89388.6
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.28978577 * 2560; err = 0.37890625 * 2560; time = 0.0286s; samplesPerSecond = 89529.3
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.24952393 * 2560; err = 0.38125000 * 2560; time = 0.0284s; samplesPerSecond = 90290.3
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.22552185 * 2560; err = 0.36328125 * 2560; time = 0.0285s; samplesPerSecond = 89805.7
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.23660278 * 2560; err = 0.37851563 * 2560; time = 0.0271s; samplesPerSecond = 94374.4
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.27010498 * 2560; err = 0.38632813 * 2560; time = 0.0275s; samplesPerSecond = 93182.4
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.24254456 * 2560; err = 0.38476563 * 2560; time = 0.0281s; samplesPerSecond = 91006.0
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.17969666 * 2560; err = 0.35859375 * 2560; time = 0.0286s; samplesPerSecond = 89404.2
08/04/2016 09:20:59:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.18419189 * 2560; err = 0.35000000 * 2560; time = 0.0285s; samplesPerSecond = 89705.0
08/04/2016 09:21:00:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.22044678 * 2560; err = 0.37656250 * 2560; time = 0.0285s; samplesPerSecond = 89821.4
08/04/2016 09:21:00:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.16026001 * 2560; err = 0.34335938 * 2560; time = 0.0269s; samplesPerSecond = 95022.5
08/04/2016 09:21:00:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.20395508 * 2560; err = 0.37812500 * 2560; time = 0.0284s; samplesPerSecond = 90121.8
08/04/2016 09:21:00:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.17126465 * 2560; err = 0.35000000 * 2560; time = 0.0286s; samplesPerSecond = 89422.9
08/04/2016 09:21:00:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20445862 * 2560; err = 0.35625000 * 2560; time = 0.0291s; samplesPerSecond = 87881.9
08/04/2016 09:21:00:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.22648621 * 2560; err = 0.35742188 * 2560; time = 0.0290s; samplesPerSecond = 88269.8
08/04/2016 09:21:00:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.20564880 * 2560; err = 0.36210938 * 2560; time = 0.0250s; samplesPerSecond = 102404.1
08/04/2016 09:21:00: Finished Epoch[ 1 of 2]: [Training] ce = 1.46416121 * 81920; err = 0.42054443 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.07024s
08/04/2016 09:21:00: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.1'

08/04/2016 09:21:00: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 09:21:00: Starting minibatch loop.
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.14285164 * 2560; err = 0.35781250 * 2560; time = 0.0319s; samplesPerSecond = 80255.8
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.10878162 * 2560; err = 0.34882812 * 2560; time = 0.0270s; samplesPerSecond = 94744.6
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.16119900 * 2560; err = 0.35390625 * 2560; time = 0.0267s; samplesPerSecond = 95797.6
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.15388832 * 2560; err = 0.34375000 * 2560; time = 0.0264s; samplesPerSecond = 96826.7
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.20692520 * 2560; err = 0.36601563 * 2560; time = 0.0265s; samplesPerSecond = 96592.8
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.06982346 * 2560; err = 0.32656250 * 2560; time = 0.0266s; samplesPerSecond = 96208.0
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.15059509 * 2560; err = 0.34023437 * 2560; time = 0.0263s; samplesPerSecond = 97501.5
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.11620331 * 2560; err = 0.33320312 * 2560; time = 0.0265s; samplesPerSecond = 96574.6
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.10543747 * 2560; err = 0.33476563 * 2560; time = 0.0266s; samplesPerSecond = 96407.3
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.14773560 * 2560; err = 0.34921875 * 2560; time = 0.0263s; samplesPerSecond = 97438.4
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.11638947 * 2560; err = 0.33437500 * 2560; time = 0.0263s; samplesPerSecond = 97260.7
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.10666580 * 2560; err = 0.33281250 * 2560; time = 0.0263s; samplesPerSecond = 97427.3
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.10289612 * 2560; err = 0.34687500 * 2560; time = 0.0263s; samplesPerSecond = 97271.8
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.09333954 * 2560; err = 0.33671875 * 2560; time = 0.0262s; samplesPerSecond = 97531.2
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.17858887 * 2560; err = 0.35429688 * 2560; time = 0.0257s; samplesPerSecond = 99494.8
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.08380585 * 2560; err = 0.31875000 * 2560; time = 0.0253s; samplesPerSecond = 101002.1
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.09735413 * 2560; err = 0.33671875 * 2560; time = 0.0263s; samplesPerSecond = 97442.1
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11444092 * 2560; err = 0.34023437 * 2560; time = 0.0265s; samplesPerSecond = 96665.8
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.07517548 * 2560; err = 0.33828125 * 2560; time = 0.0252s; samplesPerSecond = 101765.0
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08728180 * 2560; err = 0.35507813 * 2560; time = 0.0260s; samplesPerSecond = 98620.8
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.05493469 * 2560; err = 0.33125000 * 2560; time = 0.0279s; samplesPerSecond = 91713.5
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.14903259 * 2560; err = 0.34765625 * 2560; time = 0.0285s; samplesPerSecond = 89802.5
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.10326538 * 2560; err = 0.34492187 * 2560; time = 0.0266s; samplesPerSecond = 96407.3
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.05078735 * 2560; err = 0.32421875 * 2560; time = 0.0286s; samplesPerSecond = 89560.6
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.14259338 * 2560; err = 0.34843750 * 2560; time = 0.0284s; samplesPerSecond = 90045.7
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13173523 * 2560; err = 0.33320312 * 2560; time = 0.0286s; samplesPerSecond = 89369.9
08/04/2016 09:21:00:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.06638184 * 2560; err = 0.33359375 * 2560; time = 0.0285s; samplesPerSecond = 89846.6
08/04/2016 09:21:01:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.07181702 * 2560; err = 0.33906250 * 2560; time = 0.0285s; samplesPerSecond = 89966.6
08/04/2016 09:21:01:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.08930969 * 2560; err = 0.32695313 * 2560; time = 0.0287s; samplesPerSecond = 89130.3
08/04/2016 09:21:01:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.06550903 * 2560; err = 0.33046875 * 2560; time = 0.0284s; samplesPerSecond = 90026.7
08/04/2016 09:21:01:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06104736 * 2560; err = 0.32304688 * 2560; time = 0.0286s; samplesPerSecond = 89363.6
08/04/2016 09:21:01:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.03882141 * 2560; err = 0.33359375 * 2560; time = 0.0280s; samplesPerSecond = 91562.6
08/04/2016 09:21:01: Finished Epoch[ 2 of 2]: [Training] ce = 1.10764418 * 81920; err = 0.33952637 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.873205s
08/04/2016 09:21:01: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech'
08/04/2016 09:21:01: CNTKCommandTrainEnd: dptPre2

08/04/2016 09:21:01: Action "train" complete.


08/04/2016 09:21:01: ##############################################################################
08/04/2016 09:21:01: #                                                                            #
08/04/2016 09:21:01: # Action "edit"                                                              #
08/04/2016 09:21:01: #                                                                            #
08/04/2016 09:21:01: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 09:21:01: Action "edit" complete.


08/04/2016 09:21:01: ##############################################################################
08/04/2016 09:21:01: #                                                                            #
08/04/2016 09:21:01: # Action "train"                                                             #
08/04/2016 09:21:01: #                                                                            #
08/04/2016 09:21:01: ##############################################################################

08/04/2016 09:21:01: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 09:21:01: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 09:21:01: Loaded model with 29 nodes on GPU 0.

08/04/2016 09:21:01: Training criterion node(s):
08/04/2016 09:21:01: 	ce = CrossEntropyWithSoftmax

08/04/2016 09:21:01: Evaluation criterion node(s):

08/04/2016 09:21:01: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
00000010D5226CE0: {[HL2.W Value[512 x 512]] }
00000010D5226EC0: {[HL3.b Value[512 x 1]] }
00000010D52270A0: {[HL3.W Value[512 x 512]] }
00000010D5227BE0: {[HL2.b Value[512 x 1]] }
00000010DBEF77E0: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
00000010DBEF79C0: {[OL.W Value[132 x 512]] }
00000010DBEF7B00: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
00000010DBEF7C40: {[featNorm Value[363 x *6]] }
00000010DBEF7D80: {[ce Gradient[1]] }
00000010DBEF8000: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
00000010DBEF80A0: {[ce Value[1]] }
00000010DBEF8140: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
00000010DBEF81E0: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
00000010DBEF8280: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
00000010DBEF8320: {[OL.b Gradient[132 x 1]] }
00000010DBEF83C0: {[scaledLogLikelihood Value[132 x 1 x *6]] }
00000010DBEF8460: {[logPrior Value[132 x 1]] }
00000010DBEF8500: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
00000010DBEF8640: {[HL1.t Value[512 x *6]] }
00000010DBEF86E0: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
00000010DBEF88C0: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
00000010DBEF8A00: {[OL.t Gradient[132 x 1 x *6]] }
00000010DBEF8AA0: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
00000010DBEF8F00: {[err Value[1]] }
00000010DBEF90E0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
00000010DC3EDB20: {[globalMean Value[363 x 1]] }
00000010DC3EDDA0: {[HL1.b Value[512 x 1]] }
00000010DC3EE0C0: {[HL1.W Value[512 x 363]] }
00000010DC3EE160: {[globalInvStd Value[363 x 1]] }
00000010DC3EEE80: {[features Value[363 x *6]] }
00000010DC3EEFC0: {[globalPrior Value[132 x 1]] }
00000010DE871B20: {[OL.b Value[132 x 1]] }
00000010DE87C550: {[labels Value[132 x *6]] }

08/04/2016 09:21:01: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 09:21:01: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 09:21:01: Starting minibatch loop.
08/04/2016 09:21:01:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 3.97832451 * 2560; err = 0.81679687 * 2560; time = 0.0441s; samplesPerSecond = 57997.3
08/04/2016 09:21:01:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.51557732 * 2560; err = 0.62617188 * 2560; time = 0.0333s; samplesPerSecond = 76842.3
08/04/2016 09:21:01:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 1.98839569 * 2560; err = 0.54296875 * 2560; time = 0.0331s; samplesPerSecond = 77406.9
08/04/2016 09:21:01:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.74622650 * 2560; err = 0.49648437 * 2560; time = 0.0323s; samplesPerSecond = 79234.9
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.59862823 * 2560; err = 0.45937500 * 2560; time = 0.0326s; samplesPerSecond = 78467.4
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.47477646 * 2560; err = 0.43085937 * 2560; time = 0.0330s; samplesPerSecond = 77533.5
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.38905487 * 2560; err = 0.39882812 * 2560; time = 0.0332s; samplesPerSecond = 77182.8
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.36244354 * 2560; err = 0.39687500 * 2560; time = 0.0337s; samplesPerSecond = 75878.8
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.30323334 * 2560; err = 0.39218750 * 2560; time = 0.0342s; samplesPerSecond = 74941.5
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.31909485 * 2560; err = 0.39531250 * 2560; time = 0.0340s; samplesPerSecond = 75263.1
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.25892334 * 2560; err = 0.37812500 * 2560; time = 0.0342s; samplesPerSecond = 74836.3
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.26951752 * 2560; err = 0.37890625 * 2560; time = 0.0337s; samplesPerSecond = 75989.2
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.23129730 * 2560; err = 0.35898438 * 2560; time = 0.0341s; samplesPerSecond = 75022.7
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.19243317 * 2560; err = 0.35273437 * 2560; time = 0.0341s; samplesPerSecond = 75141.6
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.20105743 * 2560; err = 0.36796875 * 2560; time = 0.0342s; samplesPerSecond = 74842.9
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.20111847 * 2560; err = 0.35507813 * 2560; time = 0.0341s; samplesPerSecond = 75130.6
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.21232910 * 2560; err = 0.36328125 * 2560; time = 0.0342s; samplesPerSecond = 74847.2
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.22917175 * 2560; err = 0.36406250 * 2560; time = 0.0340s; samplesPerSecond = 75309.6
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.18077393 * 2560; err = 0.35937500 * 2560; time = 0.0342s; samplesPerSecond = 74748.9
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.16705627 * 2560; err = 0.34804687 * 2560; time = 0.0340s; samplesPerSecond = 75256.5
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.17594604 * 2560; err = 0.35195312 * 2560; time = 0.0340s; samplesPerSecond = 75331.8
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.21995544 * 2560; err = 0.37070313 * 2560; time = 0.0341s; samplesPerSecond = 74965.6
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.18804016 * 2560; err = 0.36210938 * 2560; time = 0.0340s; samplesPerSecond = 75194.6
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.12375488 * 2560; err = 0.34609375 * 2560; time = 0.0340s; samplesPerSecond = 75376.1
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.13181458 * 2560; err = 0.34062500 * 2560; time = 0.0336s; samplesPerSecond = 76165.5
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.17164307 * 2560; err = 0.35664062 * 2560; time = 0.0342s; samplesPerSecond = 74952.4
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.10361328 * 2560; err = 0.32890625 * 2560; time = 0.0340s; samplesPerSecond = 75210.1
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.15154419 * 2560; err = 0.36054687 * 2560; time = 0.0340s; samplesPerSecond = 75358.4
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12030029 * 2560; err = 0.33828125 * 2560; time = 0.0339s; samplesPerSecond = 75447.2
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.16241760 * 2560; err = 0.34453125 * 2560; time = 0.0336s; samplesPerSecond = 76156.5
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.19964294 * 2560; err = 0.35234375 * 2560; time = 0.0343s; samplesPerSecond = 74676.9
08/04/2016 09:21:02:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.16530151 * 2560; err = 0.35000000 * 2560; time = 0.0335s; samplesPerSecond = 76358.6
08/04/2016 09:21:02: Finished Epoch[ 1 of 4]: [Training] ce = 1.39791899 * 81920; err = 0.39953613 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=1.25797s
08/04/2016 09:21:02: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.1'

08/04/2016 09:21:03: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 09:21:03: Starting minibatch loop.
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.27437363 * 5120; err = 0.37675781 * 5120; time = 0.0548s; samplesPerSecond = 93492.1
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.46351871 * 5120; err = 0.39257813 * 5120; time = 0.0457s; samplesPerSecond = 112030.1
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.21849728 * 5120; err = 0.36445312 * 5120; time = 0.0460s; samplesPerSecond = 111272.9
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.12572250 * 5120; err = 0.33671875 * 5120; time = 0.0462s; samplesPerSecond = 110724.2
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.14999657 * 5120; err = 0.34902344 * 5120; time = 0.0458s; samplesPerSecond = 111775.7
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.12802773 * 5120; err = 0.34570313 * 5120; time = 0.0457s; samplesPerSecond = 111971.3
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08616943 * 5120; err = 0.34511719 * 5120; time = 0.0457s; samplesPerSecond = 112030.1
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.11425552 * 5120; err = 0.33242187 * 5120; time = 0.0459s; samplesPerSecond = 111474.0
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.11112213 * 5120; err = 0.34355469 * 5120; time = 0.0460s; samplesPerSecond = 111384.3
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.07531433 * 5120; err = 0.33750000 * 5120; time = 0.0461s; samplesPerSecond = 111000.3
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.07992172 * 5120; err = 0.33066406 * 5120; time = 0.0456s; samplesPerSecond = 112238.9
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.06415253 * 5120; err = 0.32500000 * 5120; time = 0.0445s; samplesPerSecond = 115063.9
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.14421082 * 5120; err = 0.35175781 * 5120; time = 0.0452s; samplesPerSecond = 113357.1
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.08667908 * 5120; err = 0.33789063 * 5120; time = 0.0453s; samplesPerSecond = 112979.4
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.09553375 * 5120; err = 0.33144531 * 5120; time = 0.0457s; samplesPerSecond = 112143.0
08/04/2016 09:21:03:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06522064 * 5120; err = 0.33632812 * 5120; time = 0.0457s; samplesPerSecond = 111968.9
08/04/2016 09:21:03: Finished Epoch[ 2 of 4]: [Training] ce = 1.14266977 * 81920; err = 0.34605713 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.743s
08/04/2016 09:21:03: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.2'

08/04/2016 09:21:04: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

08/04/2016 09:21:04: Starting minibatch loop.
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.12845669 * 5120; err = 0.35507813 * 5120; time = 0.0472s; samplesPerSecond = 108580.4
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.08134251 * 5120; err = 0.33417969 * 5120; time = 0.0459s; samplesPerSecond = 111585.7
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.07063103 * 5120; err = 0.33007813 * 5120; time = 0.0459s; samplesPerSecond = 111432.7
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.08787003 * 5120; err = 0.33808594 * 5120; time = 0.0459s; samplesPerSecond = 111632.0
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.10042801 * 5120; err = 0.33593750 * 5120; time = 0.0459s; samplesPerSecond = 111495.8
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.08486290 * 5120; err = 0.33144531 * 5120; time = 0.0457s; samplesPerSecond = 111922.4
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.08302765 * 5120; err = 0.33515625 * 5120; time = 0.0459s; samplesPerSecond = 111546.8
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.04633789 * 5120; err = 0.32304688 * 5120; time = 0.0460s; samplesPerSecond = 111227.0
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.06315689 * 5120; err = 0.32832031 * 5120; time = 0.0459s; samplesPerSecond = 111471.6
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.05130920 * 5120; err = 0.32812500 * 5120; time = 0.0460s; samplesPerSecond = 111335.8
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.08840027 * 5120; err = 0.33593750 * 5120; time = 0.0446s; samplesPerSecond = 114728.8
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.09818268 * 5120; err = 0.34179688 * 5120; time = 0.0456s; samplesPerSecond = 112179.8
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05762177 * 5120; err = 0.32832031 * 5120; time = 0.0453s; samplesPerSecond = 112966.9
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02346344 * 5120; err = 0.32382813 * 5120; time = 0.0446s; samplesPerSecond = 114872.9
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.07292633 * 5120; err = 0.33769531 * 5120; time = 0.0456s; samplesPerSecond = 112236.4
08/04/2016 09:21:04:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.03026276 * 5120; err = 0.31855469 * 5120; time = 0.0458s; samplesPerSecond = 111817.2
08/04/2016 09:21:04: Finished Epoch[ 3 of 4]: [Training] ce = 1.07301750 * 81920; err = 0.33284912 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.734686s
08/04/2016 09:21:04: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.3'

08/04/2016 09:21:04: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

08/04/2016 09:21:04: Starting minibatch loop.
08/04/2016 09:21:04:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.04442225 * 5120; err = 0.32519531 * 5120; time = 0.0469s; samplesPerSecond = 109068.4
08/04/2016 09:21:04:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.01133960 * 4926; err = 0.31201786 * 4926; time = 0.1032s; samplesPerSecond = 47726.5
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01365585 * 5120; err = 0.31640625 * 5120; time = 0.0457s; samplesPerSecond = 111971.3
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.02049294 * 5120; err = 0.31562500 * 5120; time = 0.0443s; samplesPerSecond = 115560.0
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.18170624 * 5120; err = 0.37226562 * 5120; time = 0.0454s; samplesPerSecond = 112735.6
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.03420181 * 5120; err = 0.32128906 * 5120; time = 0.0457s; samplesPerSecond = 111927.2
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.97196693 * 5120; err = 0.30273438 * 5120; time = 0.0458s; samplesPerSecond = 111829.5
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.00869904 * 5120; err = 0.33007813 * 5120; time = 0.0463s; samplesPerSecond = 110633.3
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.98327942 * 5120; err = 0.30332031 * 5120; time = 0.0459s; samplesPerSecond = 111644.1
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.96378937 * 5120; err = 0.30468750 * 5120; time = 0.0447s; samplesPerSecond = 114610.6
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.00055084 * 5120; err = 0.31113281 * 5120; time = 0.0466s; samplesPerSecond = 109944.4
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 0.99678116 * 5120; err = 0.30625000 * 5120; time = 0.0463s; samplesPerSecond = 110556.9
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.97792740 * 5120; err = 0.31308594 * 5120; time = 0.0464s; samplesPerSecond = 110449.6
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97973938 * 5120; err = 0.30683594 * 5120; time = 0.0447s; samplesPerSecond = 114418.5
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.00156555 * 5120; err = 0.31269531 * 5120; time = 0.0463s; samplesPerSecond = 110580.8
08/04/2016 09:21:05:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.98950653 * 5120; err = 0.30507812 * 5120; time = 0.0463s; samplesPerSecond = 110521.1
08/04/2016 09:21:05: Finished Epoch[ 4 of 4]: [Training] ce = 1.01127386 * 81920; err = 0.31624756 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.796966s
08/04/2016 09:21:05: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech'
08/04/2016 09:21:05: CNTKCommandTrainEnd: speechTrain

08/04/2016 09:21:05: Action "train" complete.


08/04/2016 09:21:05: ##############################################################################
08/04/2016 09:21:05: #                                                                            #
08/04/2016 09:21:05: # Action "edit"                                                              #
08/04/2016 09:21:05: #                                                                            #
08/04/2016 09:21:05: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 09:21:05: Action "edit" complete.


08/04/2016 09:21:05: ##############################################################################
08/04/2016 09:21:05: #                                                                            #
08/04/2016 09:21:05: # Action "train"                                                             #
08/04/2016 09:21:05: #                                                                            #
08/04/2016 09:21:05: ##############################################################################

08/04/2016 09:21:05: CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying', 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list', 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu\TestData\CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames

08/04/2016 09:21:06: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0'.

Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *9]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *9]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *9], [363 x 1], [363 x 1] -> [363 x *9]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *9] -> [512 x *9]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *9] -> [132 x 1 x *9]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *9], [132 x 1 x *9], [132 x 1 x *9] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *9], [132 x 1 x *9] -> [1]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 09:21:06: Loaded model with 29 nodes on GPU 0.

08/04/2016 09:21:06: Training criterion node(s):
08/04/2016 09:21:06: 	ce = SequenceWithSoftmax

08/04/2016 09:21:06: Evaluation criterion node(s):

08/04/2016 09:21:06: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

0000000000000000: {[err Gradient[1]] [featNorm Gradient[363 x *9]] [features Gradient[363 x *9]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *9]] [logPrior Gradient[132 x 1]] }
00000010D5227640: {[HL2.b Value[512 x 1]] }
00000010DC3EE0C0: {[globalInvStd Value[363 x 1]] }
00000010DC3EEE80: {[features Value[363 x *9]] }
00000010DE870C20: {[HL1.W Value[512 x 363]] }
00000010DE871300: {[HL1.b Value[512 x 1]] }
00000010DE87C4B0: {[globalMean Value[363 x 1]] }
00000010DE87C730: {[globalPrior Value[132 x 1]] }
00000010E943CBF0: {[err Value[1]] }
00000010E943CC90: {[featNorm Value[363 x *9]] }
00000010E943CD30: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *9]] }
00000010E943CDD0: {[HL2.t Gradient[512 x 1 x *9]] [HL2.y Value[512 x 1 x *9]] }
00000010E943CFB0: {[ce Gradient[1]] }
00000010E943D050: {[OL.t Gradient[132 x 1 x *9]] [scaledLogLikelihood Gradient[132 x 1 x *9]] }
00000010E943D230: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *9]] }
00000010E943D2D0: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *9]] }
00000010E943D410: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *9]] [OL.z Gradient[132 x 1 x *9]] }
00000010E943D4B0: {[OL.b Gradient[132 x 1]] }
00000010E943D7D0: {[HL2.W Value[512 x 512]] }
00000010E943D9B0: {[HL3.W Value[512 x 512]] }
00000010E943DA50: {[OL.W Value[132 x 512]] }
00000010E943DAF0: {[scaledLogLikelihood Value[132 x 1 x *9]] }
00000010E943DC30: {[ce Value[1]] }
00000010E943DD70: {[HL1.t Value[512 x *9]] }
00000010E943DE10: {[labels Value[132 x *9]] }
00000010E943DEB0: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *9]] }
00000010E943DF50: {[HL3.t Gradient[512 x 1 x *9]] [HL3.y Value[512 x 1 x *9]] }
00000010E943DFF0: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *9]] [HL3.z Gradient[512 x 1 x *9]] [OL.t Value[132 x 1 x *9]] }
00000010E943E130: {[HL3.b Value[512 x 1]] }
00000010E943E4F0: {[OL.b Value[132 x 1]] }
00000010E943E770: {[logPrior Value[132 x 1]] }
00000010E943E810: {[HL1.t Gradient[512 x *9]] [HL1.y Value[512 x 1 x *9]] }
00000010E943E950: {[HL1.z Gradient[512 x 1 x *9]] [HL2.t Value[512 x 1 x *9]] }
00000010E943E9F0: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *9]] [HL2.z Gradient[512 x 1 x *9]] [HL3.t Value[512 x 1 x *9]] }

08/04/2016 09:21:06: No PreCompute nodes found, skipping PreCompute step.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-010
Setting SeqGammar-related parameters: amf=14.00, lmf=14.00, wp=0.00, bMMIFactor=0.00, usesMBR=false

08/04/2016 09:21:06: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 09:21:13: Starting minibatch loop.
dengamma value 1.032930
dengamma value 1.017072
dengamma value 0.995529
dengamma value 0.943792
dengamma value 0.999778
dengamma value 1.080238
dengamma value 1.054341
dengamma value 1.003360
dengamma value 0.918589
dengamma value 0.978100
dengamma value 1.060493
dengamma value 0.994261
dengamma value 1.001029
dengamma value 1.104213
dengamma value 1.028304
dengamma value 0.953865
dengamma value 1.010607
dengamma value 1.005487
dengamma value 1.051135
dengamma value 1.065819
dengamma value 0.952841
dengamma value 1.044800
dengamma value 1.021089
08/04/2016 09:21:17:  Epoch[ 1 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.06313987 * 6504; err = 0.40836408 * 6504; time = 3.3713s; samplesPerSecond = 1929.2
dengamma value 1.045052
dengamma value 1.050421
dengamma value 1.079312
dengamma value 1.050000
dengamma value 1.025979
dengamma value 1.040214
dengamma value 1.046335
dengamma value 1.014507
dengamma value 1.026442
dengamma value 1.007373
dengamma value 0.950596
dengamma value 0.982916
dengamma value 1.023350
dengamma value 1.047873
dengamma value 1.039228
dengamma value 1.063174
dengamma value 0.998058
dengamma value 0.988794
dengamma value 1.018219
dengamma value 1.036222
dengamma value 0.962813
dengamma value 1.054469
dengamma value 1.036924
dengamma value 1.008076
08/04/2016 09:21:18:  Epoch[ 1 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07072674 * 5642; err = 0.39294576 * 5642; time = 1.0027s; samplesPerSecond = 5626.9
dengamma value 1.076463
dengamma value 1.035111
dengamma value 1.045179
dengamma value 1.040804
dengamma value 1.001891
dengamma value 0.979155
dengamma value 1.024427
dengamma value 1.016152
dengamma value 1.006109
dengamma value 1.065238
dengamma value 1.030340
dengamma value 0.984913
dengamma value 1.098194
dengamma value 1.036025
dengamma value 1.066185
dengamma value 1.031934
dengamma value 0.956798
dengamma value 0.968604
dengamma value 1.039656
dengamma value 1.005915
dengamma value 1.018864
dengamma value 0.989352
dengamma value 1.032285
dengamma value 1.042304
08/04/2016 09:21:19:  Epoch[ 1 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.06989642 * 6102; err = 0.37905605 * 6102; time = 1.1047s; samplesPerSecond = 5523.4
dengamma value 1.009364
dengamma value 1.047711
dengamma value 1.043841
dengamma value 1.013197
dengamma value 1.077445
dengamma value 1.078813
dengamma value 1.015843
dengamma value 1.022461
dengamma value 0.923835
dengamma value 1.066195
dengamma value 1.009229
dengamma value 0.970098
dengamma value 1.004223
dengamma value 1.062803
dengamma value 1.015564
dengamma value 0.997486
dengamma value 1.030924
dengamma value 1.015634
dengamma value 1.025858
dengamma value 1.024587
dengamma value 0.993753
dengamma value 1.039177
dengamma value 1.016422
dengamma value 0.989412
08/04/2016 09:21:20:  Epoch[ 1 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.07623289 * 7412; err = 0.36103616 * 7412; time = 1.3670s; samplesPerSecond = 5421.9
dengamma value 1.034825
dengamma value 1.040345
dengamma value 1.074090
dengamma value 1.007000
dengamma value 1.005487
dengamma value 1.037304
dengamma value 1.041552
dengamma value 1.078137
dengamma value 1.076376
dengamma value 0.999827
dengamma value 1.031736
dengamma value 1.062597
dengamma value 1.023630
dengamma value 0.993611
dengamma value 1.019928
dengamma value 1.000977
dengamma value 0.969117
dengamma value 1.024439
dengamma value 1.017447
dengamma value 0.959662
dengamma value 0.877904
08/04/2016 09:21:21:  Epoch[ 1 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.07409070 * 4878; err = 0.37617876 * 4878; time = 0.8516s; samplesPerSecond = 5728.2
dengamma value 1.041551
dengamma value 0.987232
dengamma value 1.013712
dengamma value 1.001278
dengamma value 1.042677
dengamma value 1.049071
dengamma value 1.101908
dengamma value 1.041023
dengamma value 1.051869
dengamma value 1.035274
dengamma value 1.010203
dengamma value 1.065103
dengamma value 1.006161
dengamma value 1.031259
dengamma value 1.053939
dengamma value 0.936879
dengamma value 0.988026
dengamma value 1.052656
dengamma value 1.059403
dengamma value 1.005154
dengamma value 1.019475
dengamma value 1.057108
dengamma value 1.041198
dengamma value 0.926834
dengamma value 1.001256
08/04/2016 09:21:22:  Epoch[ 1 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.07251364 * 7050; err = 0.35602837 * 7050; time = 1.2376s; samplesPerSecond = 5696.5
dengamma value 1.009442
dengamma value 1.011052
dengamma value 0.961314
dengamma value 0.972684
dengamma value 1.003703
dengamma value 1.113578
dengamma value 1.010428
dengamma value 1.028900
dengamma value 1.045865
dengamma value 0.948063
dengamma value 1.033918
dengamma value 0.916772
dengamma value 0.934009
dengamma value 0.983662
dengamma value 0.987169
dengamma value 0.984945
dengamma value 1.115601
dengamma value 1.102381
dengamma value 1.062813
dengamma value 1.054907
dengamma value 1.000334
dengamma value 1.064765
dengamma value 1.050434
dengamma value 1.035718
dengamma value 1.038400
08/04/2016 09:21:23:  Epoch[ 1 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07486549 * 5870; err = 0.37853492 * 5870; time = 0.9624s; samplesPerSecond = 6099.2
dengamma value 1.012295
dengamma value 1.019298
dengamma value 1.007204
dengamma value 1.084344
dengamma value 1.010436
dengamma value 1.010134
dengamma value 1.027266
dengamma value 1.022305
dengamma value 1.043341
dengamma value 1.002383
dengamma value 1.065132
dengamma value 0.930257
dengamma value 0.967655
dengamma value 1.036302
dengamma value 1.089392
dengamma value 0.982975
dengamma value 0.988981
dengamma value 1.016464
dengamma value 1.026615
dengamma value 1.059013
dengamma value 1.050780
dengamma value 1.025141
08/04/2016 09:21:24:  Epoch[ 1 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.07633137 * 5936; err = 0.35057278 * 5936; time = 1.1028s; samplesPerSecond = 5382.5
dengamma value 0.987659
dengamma value 1.024881
dengamma value 1.026181
dengamma value 0.994608
dengamma value 0.951251
dengamma value 1.083389
dengamma value 1.065132
dengamma value 1.024501
dengamma value 1.007601
dengamma value 1.048609
dengamma value 1.069277
dengamma value 1.019544
dengamma value 1.028171
dengamma value 1.016258
dengamma value 1.062904
dengamma value 1.023485
dengamma value 1.019628
dengamma value 1.032589
dengamma value 1.010059
dengamma value 1.042495
dengamma value 1.057877
dengamma value 1.008328
08/04/2016 09:21:25:  Epoch[ 1 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08027268 * 5116; err = 0.31802189 * 5116; time = 0.9477s; samplesPerSecond = 5398.4
dengamma value 1.009493
dengamma value 0.958953
dengamma value 0.998657
dengamma value 1.070267
dengamma value 1.043303
dengamma value 1.029989
dengamma value 0.978140
dengamma value 0.990090
dengamma value 1.079364
dengamma value 1.035141
dengamma value 0.985624
dengamma value 1.036484
dengamma value 0.974827
dengamma value 1.055861
dengamma value 1.008585
dengamma value 0.967845
dengamma value 1.060422
dengamma value 1.018346
dengamma value 0.945878
dengamma value 1.021652
dengamma value 1.067466
dengamma value 0.967850
dengamma value 1.040497
dengamma value 1.095572
08/04/2016 09:21:26:  Epoch[ 1 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.07424730 * 6182; err = 0.36040116 * 6182; time = 1.0809s; samplesPerSecond = 5719.2
dengamma value 1.045945
dengamma value 1.000003
dengamma value 1.016445
dengamma value 0.984709
dengamma value 0.967931
dengamma value 0.950742
dengamma value 1.006809
dengamma value 0.960716
dengamma value 1.037531
dengamma value 0.971430
dengamma value 1.042041
dengamma value 1.015395
dengamma value 1.004787
dengamma value 0.968728
dengamma value 1.000931
dengamma value 1.036195
dengamma value 1.039624
dengamma value 1.019305
dengamma value 1.068165
dengamma value 1.020729
dengamma value 1.032468
dengamma value 1.044051
dengamma value 0.955515
08/04/2016 09:21:28:  Epoch[ 1 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08250942 * 6614; err = 0.33308134 * 6614; time = 1.1603s; samplesPerSecond = 5700.1
dengamma value 0.972496
dengamma value 1.038638
dengamma value 1.018866
dengamma value 1.082548
dengamma value 1.019638
dengamma value 1.011909
dengamma value 0.979440
dengamma value 1.046567
dengamma value 0.997456
dengamma value 0.989397
dengamma value 1.024716
dengamma value 0.994574
dengamma value 1.028044
dengamma value 1.018737
dengamma value 0.936951
dengamma value 0.987959
dengamma value 1.002326
dengamma value 0.908423
dengamma value 0.973563
dengamma value 1.030594
dengamma value 1.004817
08/04/2016 09:21:28:  Epoch[ 1 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08356113 * 5008; err = 0.36920927 * 5008; time = 0.8263s; samplesPerSecond = 6060.4
dengamma value 1.036291
dengamma value 1.037623
dengamma value 0.953278
dengamma value 0.995781
dengamma value 1.015423
dengamma value 1.001886
dengamma value 1.044882
dengamma value 1.136278
dengamma value 1.069047
dengamma value 1.042090
dengamma value 1.034122
dengamma value 0.942550
dengamma value 1.001498
dengamma value 1.038342
dengamma value 1.007316
dengamma value 1.041029
dengamma value 1.045310
dengamma value 1.110471
dengamma value 1.053616
dengamma value 1.048242
dengamma value 1.036523
dengamma value 1.056822
dengamma value 1.038703
08/04/2016 09:21:30:  Epoch[ 1 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.07612632 * 6764; err = 0.32318155 * 6764; time = 1.1988s; samplesPerSecond = 5642.5
dengamma value 0.992022
dengamma value 0.992202
dengamma value 1.055169
dengamma value 1.010616
dengamma value 1.032460
dengamma value 0.968629
dengamma value 0.969052
dengamma value 0.998141
dengamma value 1.053167
dengamma value 0.994103
dengamma value 0.994103
dengamma value 0.981448
dengamma value 0.994156
dengamma value 0.994156
08/04/2016 09:21:30: Finished Epoch[ 1 of 3]: [Training] ce = 0.07532619 * 82650; err = 0.36171809 * 82650; totalSamplesSeen = 82650; learningRatePerSample = 2e-006; epochTime=24.2218s
08/04/2016 09:21:30: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.1'

08/04/2016 09:21:30: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 82176), data subset 0 of 1, with 1 datapasses

08/04/2016 09:21:30: Starting minibatch loop.
dengamma value 1.033945
dengamma value 1.006680
dengamma value 1.028574
dengamma value 0.906680
dengamma value 1.081381
dengamma value 0.986895
dengamma value 1.089571
dengamma value 1.000057
dengamma value 0.982977
dengamma value 0.962884
dengamma value 1.057586
dengamma value 1.040058
dengamma value 1.007086
dengamma value 1.078715
dengamma value 1.105198
dengamma value 0.988019
dengamma value 1.017972
dengamma value 0.984047
dengamma value 1.053387
dengamma value 1.056225
dengamma value 1.065012
dengamma value 1.069927
dengamma value 1.025975
dengamma value 1.001430
08/04/2016 09:21:31:  Epoch[ 2 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08053228 * 6272; err = 0.31919643 * 6272; time = 1.1056s; samplesPerSecond = 5673.0
dengamma value 0.991099
dengamma value 1.038649
dengamma value 0.964275
dengamma value 1.026619
dengamma value 1.008353
dengamma value 0.973431
dengamma value 1.052334
dengamma value 1.030841
dengamma value 1.036174
dengamma value 1.016068
dengamma value 0.990414
dengamma value 1.031451
dengamma value 1.051705
dengamma value 1.034401
dengamma value 1.000838
dengamma value 1.073818
dengamma value 1.005335
dengamma value 1.049992
dengamma value 1.039461
dengamma value 0.995990
dengamma value 1.025119
dengamma value 0.981779
dengamma value 0.961677
08/04/2016 09:21:32:  Epoch[ 2 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08055802 * 6264; err = 0.34849936 * 6264; time = 1.0706s; samplesPerSecond = 5850.8
dengamma value 0.928094
dengamma value 1.029735
dengamma value 1.013285
dengamma value 1.080848
dengamma value 1.016265
dengamma value 0.945801
dengamma value 1.030283
dengamma value 1.070470
dengamma value 1.040505
dengamma value 1.011241
dengamma value 1.030810
dengamma value 0.935294
dengamma value 0.974961
dengamma value 0.963178
dengamma value 0.996021
dengamma value 1.032622
dengamma value 1.006422
dengamma value 0.968174
dengamma value 1.008637
dengamma value 0.986688
dengamma value 1.051981
dengamma value 1.031164
08/04/2016 09:21:34:  Epoch[ 2 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08836690 * 6616; err = 0.34204958 * 6616; time = 1.0688s; samplesPerSecond = 6190.1
dengamma value 0.985753
dengamma value 1.086454
dengamma value 0.924311
dengamma value 1.019175
dengamma value 0.997637
dengamma value 1.064776
dengamma value 0.982200
dengamma value 1.035606
dengamma value 1.026938
dengamma value 1.063322
dengamma value 1.025417
dengamma value 1.011159
dengamma value 1.011006
dengamma value 1.018617
dengamma value 1.028207
dengamma value 1.050129
dengamma value 1.013359
dengamma value 0.977193
dengamma value 0.988940
dengamma value 0.925835
dengamma value 0.985120
dengamma value 1.060643
dengamma value 1.034020
dengamma value 1.020912
08/04/2016 09:21:35:  Epoch[ 2 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.07975924 * 6732; err = 0.35710042 * 6732; time = 1.2472s; samplesPerSecond = 5397.6
dengamma value 1.013682
dengamma value 1.086323
dengamma value 0.989848
dengamma value 0.980633
dengamma value 0.994519
dengamma value 1.023389
dengamma value 1.052964
dengamma value 0.999925
dengamma value 1.024683
dengamma value 1.062350
dengamma value 1.042255
dengamma value 1.036961
dengamma value 0.903382
dengamma value 1.071361
dengamma value 0.982110
dengamma value 1.019476
dengamma value 1.011159
dengamma value 1.034542
dengamma value 0.943321
dengamma value 0.994785
dengamma value 1.070710
dengamma value 1.083755
dengamma value 1.063199
08/04/2016 09:21:36:  Epoch[ 2 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08086405 * 5914; err = 0.33107880 * 5914; time = 1.0195s; samplesPerSecond = 5801.1
dengamma value 0.947046
dengamma value 1.046696
dengamma value 0.990143
dengamma value 1.008673
dengamma value 1.058359
dengamma value 1.083220
dengamma value 0.999529
dengamma value 1.051615
dengamma value 1.037819
dengamma value 1.067123
dengamma value 0.972171
dengamma value 1.038343
dengamma value 1.099266
dengamma value 0.995566
dengamma value 1.069454
dengamma value 0.953922
dengamma value 1.041794
dengamma value 0.957857
dengamma value 1.023667
dengamma value 0.996399
dengamma value 1.068204
dengamma value 0.964727
dengamma value 1.013438
dengamma value 0.991319
dengamma value 0.949314
08/04/2016 09:21:37:  Epoch[ 2 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08125801 * 6460; err = 0.32507740 * 6460; time = 1.1289s; samplesPerSecond = 5722.5
dengamma value 1.075670
dengamma value 1.091829
dengamma value 1.006491
dengamma value 1.028504
dengamma value 0.985500
dengamma value 0.977190
dengamma value 0.992663
dengamma value 1.022800
dengamma value 0.972979
dengamma value 0.995565
dengamma value 1.052289
dengamma value 1.036886
dengamma value 0.960046
dengamma value 1.009312
dengamma value 1.038000
dengamma value 1.051800
dengamma value 0.981272
dengamma value 1.035416
dengamma value 1.008643
dengamma value 0.974839
08/04/2016 09:21:38:  Epoch[ 2 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07780790 * 6060; err = 0.34554455 * 6060; time = 1.0189s; samplesPerSecond = 5947.6
dengamma value 1.035427
dengamma value 1.004448
dengamma value 0.941854
dengamma value 0.972981
dengamma value 1.026107
dengamma value 1.002748
dengamma value 1.047200
dengamma value 1.003019
dengamma value 1.011796
dengamma value 1.070854
dengamma value 1.051521
dengamma value 1.006466
dengamma value 0.989469
dengamma value 1.083181
dengamma value 1.052035
dengamma value 1.051354
dengamma value 0.991848
dengamma value 0.998419
dengamma value 1.029343
dengamma value 1.030066
dengamma value 1.059151
dengamma value 1.014816
08/04/2016 09:21:39:  Epoch[ 2 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08746924 * 6996; err = 0.31761006 * 6996; time = 1.2454s; samplesPerSecond = 5617.6
dengamma value 1.009731
dengamma value 0.987205
dengamma value 1.034273
dengamma value 1.023783
dengamma value 0.929099
dengamma value 1.024510
dengamma value 1.050336
dengamma value 0.968949
dengamma value 1.022547
dengamma value 0.977185
dengamma value 1.069202
dengamma value 1.043634
dengamma value 1.008667
dengamma value 1.024323
dengamma value 0.968555
dengamma value 0.958275
dengamma value 0.956987
dengamma value 0.973398
dengamma value 0.979259
dengamma value 1.022882
dengamma value 1.004032
08/04/2016 09:21:40:  Epoch[ 2 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08041585 * 6668; err = 0.36727654 * 6668; time = 1.1153s; samplesPerSecond = 5978.9
dengamma value 1.015955
dengamma value 1.063048
dengamma value 1.002619
dengamma value 1.072923
dengamma value 1.016796
dengamma value 1.041014
dengamma value 1.043817
dengamma value 1.014189
dengamma value 1.024710
dengamma value 1.012759
dengamma value 1.026799
dengamma value 1.030268
dengamma value 1.035544
dengamma value 1.035722
dengamma value 0.964769
dengamma value 1.007577
dengamma value 0.996651
dengamma value 1.036025
dengamma value 1.084062
dengamma value 1.088165
dengamma value 0.968659
dengamma value 1.014430
dengamma value 0.986871
dengamma value 1.011215
dengamma value 0.983422
08/04/2016 09:21:41:  Epoch[ 2 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08129749 * 5830; err = 0.32967410 * 5830; time = 1.0266s; samplesPerSecond = 5678.8
dengamma value 0.945633
dengamma value 1.022828
dengamma value 0.971205
dengamma value 1.028810
dengamma value 1.042086
dengamma value 1.072088
dengamma value 1.008030
dengamma value 1.049787
dengamma value 1.030002
dengamma value 0.963180
dengamma value 1.026755
dengamma value 1.028365
dengamma value 0.986602
dengamma value 1.081241
dengamma value 1.020833
dengamma value 1.053318
dengamma value 1.134834
dengamma value 1.013404
dengamma value 0.980273
dengamma value 1.024591
dengamma value 1.068678
dengamma value 1.043389
dengamma value 1.117812
dengamma value 1.051396
08/04/2016 09:21:42:  Epoch[ 2 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08309077 * 5762; err = 0.33026727 * 5762; time = 0.9829s; samplesPerSecond = 5862.2
dengamma value 1.012720
dengamma value 0.992477
dengamma value 1.036789
dengamma value 0.915351
dengamma value 0.993034
dengamma value 1.060998
dengamma value 1.076515
dengamma value 1.037863
dengamma value 0.942464
dengamma value 0.990559
dengamma value 0.997560
dengamma value 1.023462
dengamma value 1.034180
dengamma value 1.078366
dengamma value 1.037663
dengamma value 0.986956
dengamma value 1.030685
dengamma value 0.991166
dengamma value 1.047874
dengamma value 1.078630
dengamma value 1.037578
dengamma value 1.020267
08/04/2016 09:21:43:  Epoch[ 2 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08136527 * 5776; err = 0.34851108 * 5776; time = 0.9679s; samplesPerSecond = 5967.8
dengamma value 1.033051
dengamma value 1.035435
dengamma value 1.002848
dengamma value 1.064165
dengamma value 1.047031
dengamma value 1.012539
dengamma value 1.019765
dengamma value 0.981099
dengamma value 1.015986
dengamma value 1.049859
dengamma value 0.998211
dengamma value 0.974342
dengamma value 1.061259
dengamma value 1.038575
dengamma value 0.976173
dengamma value 1.032694
dengamma value 1.054799
dengamma value 1.008598
dengamma value 1.006666
dengamma value 1.024447
dengamma value 0.982521
dengamma value 0.955392
dengamma value 0.970233
dengamma value 0.970233
08/04/2016 09:21:44:  Epoch[ 2 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.07962897 * 6552; err = 0.36019536 * 6552; time = 1.0821s; samplesPerSecond = 6054.9
08/04/2016 09:21:44: Finished Epoch[ 2 of 3]: [Training] ce = 0.08178722 * 81902; err = 0.34032136 * 81902; totalSamplesSeen = 164552; learningRatePerSample = 2e-006; epochTime=14.0813s
08/04/2016 09:21:44: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.2'

08/04/2016 09:21:45: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163920), data subset 0 of 1, with 1 datapasses

08/04/2016 09:21:45: Starting minibatch loop.
dengamma value 1.024196
dengamma value 1.068357
dengamma value 0.981224
dengamma value 0.995150
dengamma value 1.000649
dengamma value 0.975254
dengamma value 1.043252
dengamma value 1.058426
dengamma value 1.038708
dengamma value 0.989601
dengamma value 0.983836
dengamma value 1.011192
dengamma value 0.997297
dengamma value 1.006108
dengamma value 1.042645
dengamma value 0.998277
dengamma value 1.044398
dengamma value 0.967989
dengamma value 1.045377
dengamma value 1.031966
dengamma value 1.048886
08/04/2016 09:21:45:  Epoch[ 3 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08044900 * 5488; err = 0.35532070 * 5488; time = 0.9357s; samplesPerSecond = 5865.2
dengamma value 1.010577
dengamma value 1.094567
dengamma value 0.977347
dengamma value 0.989614
dengamma value 1.047557
dengamma value 1.056031
dengamma value 1.037954
dengamma value 1.031019
dengamma value 1.006978
dengamma value 1.049202
dengamma value 1.094954
dengamma value 0.998002
dengamma value 1.061326
dengamma value 0.979301
dengamma value 1.027232
dengamma value 1.025756
dengamma value 0.993540
dengamma value 1.039618
dengamma value 1.048994
dengamma value 0.968953
dengamma value 1.014070
dengamma value 1.050422
dengamma value 1.029709
dengamma value 0.980345
08/04/2016 09:21:47:  Epoch[ 3 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07892952 * 6472; err = 0.32586527 * 6472; time = 1.1125s; samplesPerSecond = 5817.5
dengamma value 1.051613
dengamma value 0.991213
dengamma value 1.023830
dengamma value 1.067883
dengamma value 0.974991
dengamma value 1.069068
dengamma value 1.008147
dengamma value 1.016270
dengamma value 0.998723
dengamma value 1.033369
dengamma value 1.044760
dengamma value 1.029072
dengamma value 1.016555
dengamma value 1.058387
dengamma value 1.107874
dengamma value 1.085839
dengamma value 1.097667
dengamma value 1.073682
dengamma value 1.023660
dengamma value 1.026086
dengamma value 1.003376
08/04/2016 09:21:48:  Epoch[ 3 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08245775 * 5858; err = 0.28576306 * 5858; time = 1.0592s; samplesPerSecond = 5530.4
dengamma value 1.062638
dengamma value 1.004672
dengamma value 1.050961
dengamma value 1.050980
dengamma value 1.020099
dengamma value 1.032122
dengamma value 1.104589
dengamma value 0.993560
dengamma value 1.044515
dengamma value 1.039550
dengamma value 1.051496
dengamma value 1.086201
dengamma value 0.989326
dengamma value 1.026862
dengamma value 1.096904
dengamma value 0.996766
dengamma value 1.014308
dengamma value 0.910536
dengamma value 1.069821
dengamma value 1.059809
dengamma value 1.007777
dengamma value 1.029620
08/04/2016 09:21:49:  Epoch[ 3 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.07759849 * 6336; err = 0.30981692 * 6336; time = 1.1284s; samplesPerSecond = 5615.3
dengamma value 1.027424
dengamma value 0.986306
dengamma value 1.039548
dengamma value 0.976810
dengamma value 1.033543
dengamma value 1.042978
dengamma value 0.997485
dengamma value 0.972448
dengamma value 1.036096
dengamma value 1.100342
dengamma value 1.067433
dengamma value 0.976176
dengamma value 1.026417
dengamma value 0.974861
dengamma value 0.929954
dengamma value 1.012404
dengamma value 1.053082
dengamma value 1.002184
dengamma value 0.966939
dengamma value 0.998356
dengamma value 1.048354
08/04/2016 09:21:50:  Epoch[ 3 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08558139 * 6178; err = 0.33052768 * 6178; time = 1.0267s; samplesPerSecond = 6017.4
dengamma value 0.976663
dengamma value 1.080377
dengamma value 0.972833
dengamma value 1.067081
dengamma value 0.985467
dengamma value 0.995556
dengamma value 0.989763
dengamma value 1.014841
dengamma value 0.997718
dengamma value 1.012959
dengamma value 1.059050
dengamma value 1.056361
dengamma value 1.035317
dengamma value 1.058610
dengamma value 1.028157
dengamma value 1.037738
dengamma value 1.029574
dengamma value 0.919863
dengamma value 1.043081
dengamma value 1.014958
dengamma value 1.024907
dengamma value 0.976417
dengamma value 1.080505
08/04/2016 09:21:51:  Epoch[ 3 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08364232 * 5074; err = 0.35435554 * 5074; time = 0.8832s; samplesPerSecond = 5744.7
dengamma value 1.042134
dengamma value 1.092040
dengamma value 1.066930
dengamma value 0.992251
dengamma value 1.002399
dengamma value 0.984567
dengamma value 1.052720
dengamma value 1.079683
dengamma value 1.024308
dengamma value 1.048644
dengamma value 1.041131
dengamma value 1.017459
dengamma value 1.068457
dengamma value 0.955917
dengamma value 0.992149
dengamma value 1.062866
dengamma value 1.020239
dengamma value 1.039553
dengamma value 0.992339
dengamma value 1.003381
dengamma value 1.008162
dengamma value 1.052749
dengamma value 1.053204
dengamma value 0.988726
08/04/2016 09:21:52:  Epoch[ 3 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07911726 * 6222; err = 0.33060109 * 6222; time = 1.0932s; samplesPerSecond = 5691.7
dengamma value 0.988667
dengamma value 1.043348
dengamma value 1.042659
dengamma value 1.026527
dengamma value 0.999689
dengamma value 1.069335
dengamma value 1.033113
dengamma value 1.006744
dengamma value 1.061459
dengamma value 1.104555
dengamma value 1.061142
dengamma value 1.070604
dengamma value 0.987329
dengamma value 1.057908
dengamma value 0.977946
dengamma value 1.032314
dengamma value 1.055720
dengamma value 0.949872
dengamma value 1.004220
dengamma value 1.022522
dengamma value 0.997624
dengamma value 0.994524
dengamma value 1.030840
08/04/2016 09:21:53:  Epoch[ 3 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08978137 * 6074; err = 0.31280869 * 6074; time = 1.0253s; samplesPerSecond = 5924.3
dengamma value 1.058658
dengamma value 1.052588
dengamma value 1.052786
dengamma value 1.040439
dengamma value 1.059698
dengamma value 1.016804
dengamma value 0.931856
dengamma value 1.032194
dengamma value 1.075336
dengamma value 1.041704
dengamma value 1.023907
dengamma value 0.948426
dengamma value 1.037165
dengamma value 1.028379
dengamma value 1.082997
dengamma value 0.985475
dengamma value 1.053963
dengamma value 0.981137
dengamma value 1.043322
dengamma value 1.061561
08/04/2016 09:21:54:  Epoch[ 3 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.07723539 * 6640; err = 0.31867470 * 6640; time = 1.1220s; samplesPerSecond = 5918.2
dengamma value 1.058125
dengamma value 1.051827
dengamma value 0.992558
dengamma value 1.054553
dengamma value 0.980461
dengamma value 1.035130
dengamma value 1.033420
dengamma value 0.988122
dengamma value 0.952950
dengamma value 1.033117
dengamma value 0.983310
dengamma value 1.070975
dengamma value 1.056195
dengamma value 1.037376
dengamma value 1.055058
dengamma value 1.004370
dengamma value 1.024791
dengamma value 1.056479
dengamma value 0.999827
dengamma value 1.030378
dengamma value 1.037158
dengamma value 1.046068
08/04/2016 09:21:55:  Epoch[ 3 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08144338 * 6266; err = 0.35062241 * 6266; time = 1.1309s; samplesPerSecond = 5540.7
dengamma value 1.016521
dengamma value 0.986372
dengamma value 0.871312
dengamma value 1.033443
dengamma value 0.972844
dengamma value 0.956005
dengamma value 1.023625
dengamma value 1.000247
dengamma value 1.092032
dengamma value 0.988042
dengamma value 1.053765
dengamma value 0.981680
dengamma value 1.032771
dengamma value 1.005852
dengamma value 1.062140
dengamma value 1.044272
dengamma value 0.973372
dengamma value 1.029554
dengamma value 1.073644
dengamma value 0.987815
dengamma value 1.045035
dengamma value 1.036861
dengamma value 1.011068
dengamma value 1.035165
dengamma value 1.011312
08/04/2016 09:21:56:  Epoch[ 3 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08714723 * 7250; err = 0.34220690 * 7250; time = 1.2265s; samplesPerSecond = 5911.3
dengamma value 1.044232
dengamma value 1.099563
dengamma value 1.083263
dengamma value 1.099036
dengamma value 0.979694
dengamma value 1.033876
dengamma value 0.976252
dengamma value 1.019430
dengamma value 1.009851
dengamma value 1.047603
dengamma value 0.991036
dengamma value 0.990490
dengamma value 0.945325
dengamma value 1.016177
dengamma value 0.994993
dengamma value 1.093646
dengamma value 1.100877
dengamma value 1.024707
dengamma value 1.028312
dengamma value 1.052140
dengamma value 1.035631
08/04/2016 09:21:57:  Epoch[ 3 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08213578 * 5708; err = 0.31902593 * 5708; time = 1.0332s; samplesPerSecond = 5524.4
dengamma value 1.029151
dengamma value 0.961884
dengamma value 0.993241
dengamma value 1.017728
dengamma value 1.026959
dengamma value 1.128933
dengamma value 1.017078
dengamma value 1.054759
dengamma value 0.992117
dengamma value 1.074400
dengamma value 1.019931
dengamma value 1.022798
dengamma value 1.080649
dengamma value 1.016458
dengamma value 1.029231
dengamma value 1.083466
dengamma value 1.035093
dengamma value 1.053909
dengamma value 1.011318
dengamma value 0.971024
dengamma value 1.000899
dengamma value 1.017685
dengamma value 1.046261
dengamma value 1.089655
dengamma value 0.996400
dengamma value 0.908412
dengamma value 1.074665
dengamma value 1.041874
dengamma value 1.023071
dengamma value 1.029237
08/04/2016 09:21:59:  Epoch[ 3 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.07810398 * 8500; err = 0.34364706 * 8500; time = 1.3952s; samplesPerSecond = 6092.3
08/04/2016 09:21:59: Finished Epoch[ 3 of 3]: [Training] ce = 0.08170813 * 82066; err = 0.32935686 * 82066; totalSamplesSeen = 246618; learningRatePerSample = 2e-006; epochTime=14.1737s
08/04/2016 09:21:59: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence'
08/04/2016 09:21:59: CNTKCommandTrainEnd: sequenceTrain

08/04/2016 09:21:59: Action "train" complete.

08/04/2016 09:21:59: __COMPLETED__