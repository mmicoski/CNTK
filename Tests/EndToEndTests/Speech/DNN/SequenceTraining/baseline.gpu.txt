CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/cntk_sequence.cntk currentDirectory=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu DataDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData
08/04/2016 13:54:15: -------------------------------------------------------------------
08/04/2016 13:54:15: Build info: 

08/04/2016 13:54:15: 		Built time: Aug  4 2016 13:05:36
08/04/2016 13:54:15: 		Last modified date: Thu Aug  4 12:33:33 2016
08/04/2016 13:54:15: 		Build type: release
08/04/2016 13:54:15: 		Build target: GPU
08/04/2016 13:54:15: 		With 1bit-SGD: no
08/04/2016 13:54:15: 		Math lib: mkl
08/04/2016 13:54:15: 		CUDA_PATH: /usr/local/cuda-7.5
08/04/2016 13:54:15: 		CUB_PATH: /usr/local/cub-1.4.1
08/04/2016 13:54:15: 		CUDNN_PATH: /usr/local/cudnn-4.0
08/04/2016 13:54:15: 		Build Branch: HEAD
08/04/2016 13:54:15: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
08/04/2016 13:54:15: 		Built by philly on 643085f7f8c2
08/04/2016 13:54:15: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
08/04/2016 13:54:15: -------------------------------------------------------------------
08/04/2016 13:54:15: -------------------------------------------------------------------
08/04/2016 13:54:15: GPU info:

08/04/2016 13:54:15: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/04/2016 13:54:15: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/04/2016 13:54:15: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/04/2016 13:54:15: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/04/2016 13:54:15: -------------------------------------------------------------------

08/04/2016 13:54:15: Running on localhost at 2016/08/04 13:54:15
08/04/2016 13:54:15: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/cntk_sequence.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu  DataDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining  OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu  DeviceId=0  timestamping=true



08/04/2016 13:54:15: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/04/2016 13:54:15: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "$RunDir$/models/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.sequence.0"
    editPath  = "$ConfigDir$/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "$DataDir$/glob_0000.scp"
        ]
        labels = [
            mlfFile = "$DataDir$/glob_0000.mlf"
            labelMappingFile = "$DataDir$/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "$DataDir$/model.overalltying"
            transpFile = "$DataDir$/model.transprob"
        ]
        lattices = [
            denlatTocFile = "$DataDir$/*.lats.toc"
        ]
    ]
]
currentDirectory=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData
RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu
DataDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

08/04/2016 13:54:15: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/04/2016 13:54:15: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/04/2016 13:54:15: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
ndlMacros = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
truncated = false
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]
AddLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf"
        labelMappingFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
replaceCriterionNode = [
    action = "edit"
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/replace_ce_with_sequence_criterion.mel"
]
sequenceTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf"
            labelMappingFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/model.overalltying"
            transpFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/*.lats.toc"
        ]
    ]
]
currentDirectory=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData
RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu
DataDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu
DeviceId=0
timestamping=true

08/04/2016 13:54:15: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/04/2016 13:54:15: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining
configparameters: cntk_sequence.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData
configparameters: cntk_sequence.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf"
        labelMappingFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:replaceCriterionNode=[
    action = "edit"
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/replace_ce_with_sequence_criterion.mel"
]

configparameters: cntk_sequence.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf"
            labelMappingFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/model.overalltying"
            transpFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/*.lats.toc"
        ]
    ]
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
08/04/2016 13:54:15: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/04/2016 13:54:15: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain replaceCriterionNode sequenceTrain
08/04/2016 13:54:15: Precision = "float"
08/04/2016 13:54:15: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech
08/04/2016 13:54:15: CNTKCommandTrainInfo: dptPre1 : 2
08/04/2016 13:54:15: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech
08/04/2016 13:54:15: CNTKCommandTrainInfo: dptPre2 : 2
08/04/2016 13:54:15: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech
08/04/2016 13:54:15: CNTKCommandTrainInfo: speechTrain : 4
08/04/2016 13:54:15: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence
08/04/2016 13:54:15: CNTKCommandTrainInfo: sequenceTrain : 3
08/04/2016 13:54:15: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 11

08/04/2016 13:54:15: ##############################################################################
08/04/2016 13:54:15: #                                                                            #
08/04/2016 13:54:15: # Action "train"                                                             #
08/04/2016 13:54:15: #                                                                            #
08/04/2016 13:54:15: ##############################################################################

08/04/2016 13:54:15: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 13:54:16: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 13:54:16: Created model with 19 nodes on GPU 0.

08/04/2016 13:54:16: Training criterion node(s):
08/04/2016 13:54:16: 	ce = CrossEntropyWithSoftmax

08/04/2016 13:54:16: Evaluation criterion node(s):

08/04/2016 13:54:16: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
0x7fe804f25b58: {[logPrior Value[132 x 1]] }
0x7fe804f27038: {[featNorm Value[363 x *]] }
0x7fe804f275a8: {[HL1.t Value[512 x *]] }
0x7fe804f27958: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
0x7fe804f27ab8: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
0x7fe804f27c18: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
0x7fe804f27d78: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
0x7fe804f28738: {[ce Gradient[1]] }
0x7fe804f288f8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
0x7fe804f28ab8: {[OL.t Gradient[132 x 1 x *]] }
0x7fe804f28c78: {[OL.b Gradient[132 x 1]] }
0x7fe807340538: {[labels Value[132 x *]] }
0x7fe807341038: {[globalMean Value[363 x 1]] }
0x7fe807341dc8: {[globalInvStd Value[363 x 1]] }
0x7fe8073427c8: {[globalPrior Value[132 x 1]] }
0x7fe807343168: {[HL1.W Value[512 x 363]] }
0x7fe807344668: {[HL1.b Value[512 x 1]] }
0x7fe807345348: {[OL.W Value[132 x 512]] }
0x7fe807346168: {[OL.b Value[132 x 1]] }
0x7fe80b306d48: {[err Value[1]] }
0x7fe80b306f08: {[scaledLogLikelihood Value[132 x 1 x *]] }
0x7fe80b3070c8: {[ce Value[1]] }
0x7fe80b325398: {[features Value[363 x *]] }

08/04/2016 13:54:16: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 13:54:16: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 13:54:16: Starting minibatch loop.
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.77997551 * 2560; err = 0.81484375 * 2560; time = 0.1189s; samplesPerSecond = 21532.3
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.92382545 * 2560; err = 0.71328125 * 2560; time = 0.0084s; samplesPerSecond = 306000.5
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55630035 * 2560; err = 0.65859375 * 2560; time = 0.0081s; samplesPerSecond = 315387.5
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.30158844 * 2560; err = 0.61523438 * 2560; time = 0.0080s; samplesPerSecond = 319520.7
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 2.04897461 * 2560; err = 0.56328125 * 2560; time = 0.0080s; samplesPerSecond = 320561.0
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.85978241 * 2560; err = 0.52070313 * 2560; time = 0.0081s; samplesPerSecond = 317224.3
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.76588593 * 2560; err = 0.50312500 * 2560; time = 0.0079s; samplesPerSecond = 323109.9
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.74416962 * 2560; err = 0.49375000 * 2560; time = 0.0080s; samplesPerSecond = 319162.2
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.64636383 * 2560; err = 0.47539063 * 2560; time = 0.0080s; samplesPerSecond = 320160.1
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.60569763 * 2560; err = 0.46171875 * 2560; time = 0.0080s; samplesPerSecond = 321164.2
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.51603851 * 2560; err = 0.44648437 * 2560; time = 0.0080s; samplesPerSecond = 320440.6
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51614990 * 2560; err = 0.44765625 * 2560; time = 0.0080s; samplesPerSecond = 321931.6
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.47455750 * 2560; err = 0.43398437 * 2560; time = 0.0079s; samplesPerSecond = 323641.0
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.46740112 * 2560; err = 0.42929688 * 2560; time = 0.0080s; samplesPerSecond = 318606.1
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.41627502 * 2560; err = 0.42734375 * 2560; time = 0.0080s; samplesPerSecond = 320681.4
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.42307129 * 2560; err = 0.41015625 * 2560; time = 0.0080s; samplesPerSecond = 320160.1
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.38768005 * 2560; err = 0.40781250 * 2560; time = 0.0080s; samplesPerSecond = 318091.5
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.38003235 * 2560; err = 0.40039062 * 2560; time = 0.0080s; samplesPerSecond = 321244.8
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33673401 * 2560; err = 0.39921875 * 2560; time = 0.0079s; samplesPerSecond = 323028.4
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.33659973 * 2560; err = 0.40117188 * 2560; time = 0.0080s; samplesPerSecond = 320761.8
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.33715210 * 2560; err = 0.40507813 * 2560; time = 0.0081s; samplesPerSecond = 315232.1
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.36712036 * 2560; err = 0.41406250 * 2560; time = 0.0081s; samplesPerSecond = 314457.7
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.36091919 * 2560; err = 0.42304687 * 2560; time = 0.0081s; samplesPerSecond = 317854.5
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27734070 * 2560; err = 0.38320312 * 2560; time = 0.0082s; samplesPerSecond = 310868.2
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.27654419 * 2560; err = 0.38593750 * 2560; time = 0.0079s; samplesPerSecond = 322784.0
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27127075 * 2560; err = 0.38906250 * 2560; time = 0.0079s; samplesPerSecond = 322824.7
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.27654114 * 2560; err = 0.38671875 * 2560; time = 0.0080s; samplesPerSecond = 319760.2
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29538574 * 2560; err = 0.40000000 * 2560; time = 0.0080s; samplesPerSecond = 321972.1
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.23519592 * 2560; err = 0.37226562 * 2560; time = 0.0080s; samplesPerSecond = 319281.6
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.27269897 * 2560; err = 0.37734375 * 2560; time = 0.0080s; samplesPerSecond = 320842.2
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.29835205 * 2560; err = 0.38554688 * 2560; time = 0.0080s; samplesPerSecond = 320721.6
08/04/2016 13:54:16:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.27160950 * 2560; err = 0.38125000 * 2560; time = 0.0079s; samplesPerSecond = 322458.7
08/04/2016 13:54:16: Finished Epoch[ 1 of 2]: [Training] ce = 1.62585106 * 81920; err = 0.46021729 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.451518s
08/04/2016 13:54:16: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech.1'

08/04/2016 13:54:16: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 13:54:16: Starting minibatch loop.
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.21941137 * 2560; err = 0.38007812 * 2560; time = 0.0093s; samplesPerSecond = 275446.5
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.16789980 * 2560; err = 0.35898438 * 2560; time = 0.0080s; samplesPerSecond = 319202.0
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.22954521 * 2560; err = 0.37929687 * 2560; time = 0.0080s; samplesPerSecond = 320160.1
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.23093147 * 2560; err = 0.37421875 * 2560; time = 0.0080s; samplesPerSecond = 321123.9
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.23917122 * 2560; err = 0.36835937 * 2560; time = 0.0080s; samplesPerSecond = 320480.7
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.12390594 * 2560; err = 0.34140625 * 2560; time = 0.0080s; samplesPerSecond = 320641.3
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.17119217 * 2560; err = 0.35273437 * 2560; time = 0.0081s; samplesPerSecond = 317775.6
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16904068 * 2560; err = 0.34492187 * 2560; time = 0.0079s; samplesPerSecond = 323395.7
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.14490128 * 2560; err = 0.34687500 * 2560; time = 0.0079s; samplesPerSecond = 323314.0
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.23116379 * 2560; err = 0.36484375 * 2560; time = 0.0079s; samplesPerSecond = 323191.5
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.18010635 * 2560; err = 0.36132812 * 2560; time = 0.0080s; samplesPerSecond = 321527.3
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.16575470 * 2560; err = 0.34765625 * 2560; time = 0.0079s; samplesPerSecond = 322946.9
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.15317993 * 2560; err = 0.36484375 * 2560; time = 0.0080s; samplesPerSecond = 321003.1
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.15339508 * 2560; err = 0.35664062 * 2560; time = 0.0081s; samplesPerSecond = 314187.5
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.21201782 * 2560; err = 0.35976562 * 2560; time = 0.0081s; samplesPerSecond = 315659.7
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.11846924 * 2560; err = 0.33750000 * 2560; time = 0.0081s; samplesPerSecond = 316322.7
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.15517273 * 2560; err = 0.35703125 * 2560; time = 0.0082s; samplesPerSecond = 311208.4
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.16442261 * 2560; err = 0.34648438 * 2560; time = 0.0080s; samplesPerSecond = 320240.2
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10736237 * 2560; err = 0.34570312 * 2560; time = 0.0080s; samplesPerSecond = 321486.9
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.13406830 * 2560; err = 0.36210938 * 2560; time = 0.0079s; samplesPerSecond = 323069.2
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.09407654 * 2560; err = 0.34687500 * 2560; time = 0.0079s; samplesPerSecond = 322946.9
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.16549683 * 2560; err = 0.35195312 * 2560; time = 0.0079s; samplesPerSecond = 323559.2
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12700806 * 2560; err = 0.34687500 * 2560; time = 0.0079s; samplesPerSecond = 325782.6
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.04887695 * 2560; err = 0.32812500 * 2560; time = 0.0079s; samplesPerSecond = 324790.7
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.16085510 * 2560; err = 0.34843750 * 2560; time = 0.0078s; samplesPerSecond = 326405.7
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.17593079 * 2560; err = 0.34453125 * 2560; time = 0.0079s; samplesPerSecond = 324132.7
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.10052795 * 2560; err = 0.34570312 * 2560; time = 0.0079s; samplesPerSecond = 323886.6
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09912109 * 2560; err = 0.33554688 * 2560; time = 0.0079s; samplesPerSecond = 324584.8
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.12586365 * 2560; err = 0.33828125 * 2560; time = 0.0079s; samplesPerSecond = 324338.0
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10348206 * 2560; err = 0.34257813 * 2560; time = 0.0079s; samplesPerSecond = 324214.8
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.09067688 * 2560; err = 0.33242187 * 2560; time = 0.0079s; samplesPerSecond = 323518.3
08/04/2016 13:54:16:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.11611328 * 2560; err = 0.34609375 * 2560; time = 0.0079s; samplesPerSecond = 323968.6
08/04/2016 13:54:16: Finished Epoch[ 2 of 2]: [Training] ce = 1.15247316 * 81920; err = 0.35181885 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.259383s
08/04/2016 13:54:16: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech'
08/04/2016 13:54:16: CNTKCommandTrainEnd: dptPre1

08/04/2016 13:54:16: Action "train" complete.


08/04/2016 13:54:16: ##############################################################################
08/04/2016 13:54:16: #                                                                            #
08/04/2016 13:54:16: # Action "edit"                                                              #
08/04/2016 13:54:16: #                                                                            #
08/04/2016 13:54:16: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 13:54:17: Action "edit" complete.


08/04/2016 13:54:17: ##############################################################################
08/04/2016 13:54:17: #                                                                            #
08/04/2016 13:54:17: # Action "train"                                                             #
08/04/2016 13:54:17: #                                                                            #
08/04/2016 13:54:17: ##############################################################################

08/04/2016 13:54:17: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 13:54:17: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 13:54:17: Loaded model with 24 nodes on GPU 0.

08/04/2016 13:54:17: Training criterion node(s):
08/04/2016 13:54:17: 	ce = CrossEntropyWithSoftmax

08/04/2016 13:54:17: Evaluation criterion node(s):

08/04/2016 13:54:17: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
0x7fe7fc915cb8: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
0x7fe7fc9160b8: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
0x7fe7fc916218: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
0x7fe7fc9c2298: {[OL.W Value[132 x 512]] }
0x7fe7fc9cb6d8: {[HL2.b Value[512 x 1]] }
0x7fe7fc9d7578: {[labels Value[132 x *3]] }
0x7fe7fc9dd528: {[HL1.t Value[512 x *3]] }
0x7fe7fc9dd6a8: {[logPrior Value[132 x 1]] }
0x7fe7fc9e2838: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
0x7fe7fc9e5588: {[scaledLogLikelihood Value[132 x 1 x *3]] }
0x7fe7fc9eb368: {[ce Gradient[1]] }
0x7fe804d2af58: {[globalMean Value[363 x 1]] }
0x7fe804f3edb8: {[ce Value[1]] }
0x7fe804f3ef78: {[featNorm Value[363 x *3]] }
0x7fe804f3f728: {[err Value[1]] }
0x7fe807342438: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
0x7fe8073425f8: {[OL.t Gradient[132 x 1 x *3]] }
0x7fe8073427b8: {[OL.b Gradient[132 x 1]] }
0x7fe80b325238: {[globalPrior Value[132 x 1]] }
0x7fe80b327b08: {[HL1.b Value[512 x 1]] }
0x7fe80b328b78: {[features Value[363 x *3]] }
0x7fe80b3834e8: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
0x7fe80b3836a8: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
0x7fe80b383868: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
0x7fe80b3c47c8: {[OL.b Value[132 x 1]] }
0x7fe80c55b6c8: {[HL2.W Value[512 x 512]] }
0x7fe80c57a648: {[globalInvStd Value[363 x 1]] }
0x7fe80c59cb48: {[HL1.W Value[512 x 363]] }

08/04/2016 13:54:17: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 13:54:17: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 13:54:17: Starting minibatch loop.
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.92264137 * 2560; err = 0.82890625 * 2560; time = 0.0154s; samplesPerSecond = 166666.7
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.62011375 * 2560; err = 0.66601562 * 2560; time = 0.0122s; samplesPerSecond = 210059.9
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.13756485 * 2560; err = 0.58476562 * 2560; time = 0.0122s; samplesPerSecond = 210266.9
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.87948074 * 2560; err = 0.54023438 * 2560; time = 0.0122s; samplesPerSecond = 210128.9
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.72052612 * 2560; err = 0.48476562 * 2560; time = 0.0122s; samplesPerSecond = 209355.6
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.59273834 * 2560; err = 0.45820312 * 2560; time = 0.0123s; samplesPerSecond = 208741.0
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.48152161 * 2560; err = 0.43281250 * 2560; time = 0.0123s; samplesPerSecond = 208028.6
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.46213989 * 2560; err = 0.41953125 * 2560; time = 0.0123s; samplesPerSecond = 207472.2
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.40166626 * 2560; err = 0.41953125 * 2560; time = 0.0124s; samplesPerSecond = 207170.0
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.40336304 * 2560; err = 0.42382812 * 2560; time = 0.0123s; samplesPerSecond = 208928.4
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.34085541 * 2560; err = 0.40429688 * 2560; time = 0.0122s; samplesPerSecond = 209013.7
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.34828033 * 2560; err = 0.40390625 * 2560; time = 0.0122s; samplesPerSecond = 209099.1
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.32214508 * 2560; err = 0.38867188 * 2560; time = 0.0122s; samplesPerSecond = 210595.6
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.27623749 * 2560; err = 0.37304688 * 2560; time = 0.0121s; samplesPerSecond = 210821.0
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.27629395 * 2560; err = 0.38906250 * 2560; time = 0.0122s; samplesPerSecond = 210405.2
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.27472229 * 2560; err = 0.36562500 * 2560; time = 0.0121s; samplesPerSecond = 210786.3
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.27291870 * 2560; err = 0.37734375 * 2560; time = 0.0121s; samplesPerSecond = 211308.3
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.29111328 * 2560; err = 0.38164063 * 2560; time = 0.0122s; samplesPerSecond = 210560.9
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.24976807 * 2560; err = 0.37851563 * 2560; time = 0.0121s; samplesPerSecond = 211203.7
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.22633057 * 2560; err = 0.36015625 * 2560; time = 0.0122s; samplesPerSecond = 209441.2
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.23699646 * 2560; err = 0.38164063 * 2560; time = 0.0122s; samplesPerSecond = 210647.6
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.26840210 * 2560; err = 0.38125000 * 2560; time = 0.0122s; samplesPerSecond = 209578.4
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.24623718 * 2560; err = 0.38710937 * 2560; time = 0.0122s; samplesPerSecond = 210215.1
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.18269958 * 2560; err = 0.36210938 * 2560; time = 0.0122s; samplesPerSecond = 209904.9
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.18463745 * 2560; err = 0.35429688 * 2560; time = 0.0121s; samplesPerSecond = 211012.2
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.22700806 * 2560; err = 0.38125000 * 2560; time = 0.0122s; samplesPerSecond = 209389.8
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.15788269 * 2560; err = 0.34218750 * 2560; time = 0.0121s; samplesPerSecond = 211203.7
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.20556641 * 2560; err = 0.38398437 * 2560; time = 0.0121s; samplesPerSecond = 211447.9
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.17214050 * 2560; err = 0.35195312 * 2560; time = 0.0122s; samplesPerSecond = 210526.3
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.21809082 * 2560; err = 0.35898438 * 2560; time = 0.0122s; samplesPerSecond = 209767.3
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.22475891 * 2560; err = 0.35703125 * 2560; time = 0.0121s; samplesPerSecond = 211762.8
08/04/2016 13:54:17:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.20566711 * 2560; err = 0.35859375 * 2560; time = 0.0121s; samplesPerSecond = 210942.6
08/04/2016 13:54:17: Finished Epoch[ 1 of 2]: [Training] ce = 1.46970339 * 81920; err = 0.42128906 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.476685s
08/04/2016 13:54:17: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.1'

08/04/2016 13:54:17: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 13:54:17: Starting minibatch loop.
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.15010700 * 2560; err = 0.36132812 * 2560; time = 0.0132s; samplesPerSecond = 193499.6
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.11401196 * 2560; err = 0.34570312 * 2560; time = 0.0121s; samplesPerSecond = 211047.0
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.16447182 * 2560; err = 0.35664062 * 2560; time = 0.0122s; samplesPerSecond = 209801.7
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.15266151 * 2560; err = 0.34218750 * 2560; time = 0.0121s; samplesPerSecond = 211605.2
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.20112534 * 2560; err = 0.35781250 * 2560; time = 0.0122s; samplesPerSecond = 210387.9
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.06493530 * 2560; err = 0.31796875 * 2560; time = 0.0121s; samplesPerSecond = 210994.8
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.15083237 * 2560; err = 0.34453125 * 2560; time = 0.0122s; samplesPerSecond = 210180.6
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.11611252 * 2560; err = 0.33320312 * 2560; time = 0.0122s; samplesPerSecond = 210387.9
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.11194077 * 2560; err = 0.33828125 * 2560; time = 0.0122s; samplesPerSecond = 209904.9
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.15132294 * 2560; err = 0.34609375 * 2560; time = 0.0122s; samplesPerSecond = 210077.1
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.11250153 * 2560; err = 0.33125000 * 2560; time = 0.0122s; samplesPerSecond = 210630.2
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.10859451 * 2560; err = 0.33164063 * 2560; time = 0.0121s; samplesPerSecond = 211395.5
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.10353088 * 2560; err = 0.34648438 * 2560; time = 0.0122s; samplesPerSecond = 209664.2
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.09240570 * 2560; err = 0.33671875 * 2560; time = 0.0122s; samplesPerSecond = 210077.1
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.17526245 * 2560; err = 0.34921875 * 2560; time = 0.0122s; samplesPerSecond = 209836.1
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.07992859 * 2560; err = 0.32539062 * 2560; time = 0.0122s; samplesPerSecond = 210509.0
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.09457855 * 2560; err = 0.33554688 * 2560; time = 0.0122s; samplesPerSecond = 210387.9
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11868744 * 2560; err = 0.34414062 * 2560; time = 0.0121s; samplesPerSecond = 210786.3
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.07582703 * 2560; err = 0.33671875 * 2560; time = 0.0121s; samplesPerSecond = 210769.0
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08641357 * 2560; err = 0.34726563 * 2560; time = 0.0121s; samplesPerSecond = 210716.9
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.05171814 * 2560; err = 0.33437500 * 2560; time = 0.0121s; samplesPerSecond = 210942.6
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.14046021 * 2560; err = 0.34414062 * 2560; time = 0.0121s; samplesPerSecond = 211692.7
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.10008392 * 2560; err = 0.34218750 * 2560; time = 0.0122s; samplesPerSecond = 210664.9
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.04127808 * 2560; err = 0.31953125 * 2560; time = 0.0121s; samplesPerSecond = 211099.2
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.13776855 * 2560; err = 0.34140625 * 2560; time = 0.0122s; samplesPerSecond = 210682.2
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13193970 * 2560; err = 0.33789062 * 2560; time = 0.0122s; samplesPerSecond = 210578.3
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.07574158 * 2560; err = 0.33125000 * 2560; time = 0.0123s; samplesPerSecond = 208673.0
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.07451172 * 2560; err = 0.33710937 * 2560; time = 0.0122s; samplesPerSecond = 210111.6
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.09063721 * 2560; err = 0.32851562 * 2560; time = 0.0121s; samplesPerSecond = 211081.8
08/04/2016 13:54:17:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.05981750 * 2560; err = 0.32617188 * 2560; time = 0.0121s; samplesPerSecond = 210925.3
08/04/2016 13:54:18:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06223145 * 2560; err = 0.32226562 * 2560; time = 0.0121s; samplesPerSecond = 210769.0
08/04/2016 13:54:18:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04121399 * 2560; err = 0.32851562 * 2560; time = 0.0122s; samplesPerSecond = 210353.3
08/04/2016 13:54:18: Finished Epoch[ 2 of 2]: [Training] ce = 1.10727043 * 81920; err = 0.33817139 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.393421s
08/04/2016 13:54:18: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech'
08/04/2016 13:54:18: CNTKCommandTrainEnd: dptPre2

08/04/2016 13:54:18: Action "train" complete.


08/04/2016 13:54:18: ##############################################################################
08/04/2016 13:54:18: #                                                                            #
08/04/2016 13:54:18: # Action "edit"                                                              #
08/04/2016 13:54:18: #                                                                            #
08/04/2016 13:54:18: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 13:54:18: Action "edit" complete.


08/04/2016 13:54:18: ##############################################################################
08/04/2016 13:54:18: #                                                                            #
08/04/2016 13:54:18: # Action "train"                                                             #
08/04/2016 13:54:18: #                                                                            #
08/04/2016 13:54:18: ##############################################################################

08/04/2016 13:54:18: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp ... 948 entries
total 132 state names in state list /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 13:54:18: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 13:54:18: Loaded model with 29 nodes on GPU 0.

08/04/2016 13:54:18: Training criterion node(s):
08/04/2016 13:54:18: 	ce = CrossEntropyWithSoftmax

08/04/2016 13:54:18: Evaluation criterion node(s):

08/04/2016 13:54:18: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
0x7fe7fc99da08: {[ce Gradient[1]] }
0x7fe7fc99dbc8: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
0x7fe7fc99dd88: {[OL.t Gradient[132 x 1 x *6]] }
0x7fe7fc99df48: {[OL.b Gradient[132 x 1]] }
0x7fe7fc99e208: {[HL2.W Value[512 x 512]] }
0x7fe7fc9a98a8: {[globalPrior Value[132 x 1]] }
0x7fe7fc9aa208: {[globalMean Value[363 x 1]] }
0x7fe7fc9abc98: {[ce Value[1]] }
0x7fe7fc9ac128: {[featNorm Value[363 x *6]] }
0x7fe7fc9ae488: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
0x7fe7fc9ae648: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
0x7fe7fc9ae808: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
0x7fe7fc9ae9c8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
0x7fe7fc9aee18: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
0x7fe7fc9aefd8: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
0x7fe7fc9af198: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
0x7fe7fc9af358: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
0x7fe7fc9b0698: {[HL3.W Value[512 x 512]] }
0x7fe7fc9b4858: {[err Value[1]] }
0x7fe7fc9c86a8: {[scaledLogLikelihood Value[132 x 1 x *6]] }
0x7fe7fc9d6188: {[HL3.b Value[512 x 1]] }
0x7fe807343e88: {[HL1.W Value[512 x 363]] }
0x7fe80b3457e8: {[globalInvStd Value[363 x 1]] }
0x7fe80b34ae08: {[HL2.b Value[512 x 1]] }
0x7fe80b373768: {[features Value[363 x *6]] }
0x7fe80b383878: {[OL.b Value[132 x 1]] }
0x7fe80b383918: {[OL.W Value[132 x 512]] }
0x7fe80b38f978: {[logPrior Value[132 x 1]] }
0x7fe80b3c54c8: {[HL1.t Value[512 x *6]] }
0x7fe80b3c5918: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
0x7fe80b3c5a78: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
0x7fe80c55b468: {[HL1.b Value[512 x 1]] }
0x7fe80c5f4c68: {[labels Value[132 x *6]] }

08/04/2016 13:54:18: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 13:54:18: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 13:54:18: Starting minibatch loop.
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 3.95473633 * 2560; err = 0.81445312 * 2560; time = 0.0192s; samplesPerSecond = 133201.5
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.51360703 * 2560; err = 0.62578125 * 2560; time = 0.0166s; samplesPerSecond = 154524.1
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 1.98941879 * 2560; err = 0.54570312 * 2560; time = 0.0166s; samplesPerSecond = 154263.3
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.74144974 * 2560; err = 0.49492188 * 2560; time = 0.0166s; samplesPerSecond = 154645.4
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.59180908 * 2560; err = 0.46054688 * 2560; time = 0.0166s; samplesPerSecond = 154003.5
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.46800308 * 2560; err = 0.43164062 * 2560; time = 0.0167s; samplesPerSecond = 153330.1
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.38572540 * 2560; err = 0.40312500 * 2560; time = 0.0166s; samplesPerSecond = 154440.2
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.36116180 * 2560; err = 0.39531250 * 2560; time = 0.0166s; samplesPerSecond = 154393.6
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.29876099 * 2560; err = 0.39218750 * 2560; time = 0.0166s; samplesPerSecond = 154682.8
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.31320801 * 2560; err = 0.39257812 * 2560; time = 0.0166s; samplesPerSecond = 154077.6
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.26076813 * 2560; err = 0.37382813 * 2560; time = 0.0166s; samplesPerSecond = 154189.0
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.26728516 * 2560; err = 0.37460938 * 2560; time = 0.0165s; samplesPerSecond = 154738.9
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.23206482 * 2560; err = 0.36406250 * 2560; time = 0.0165s; samplesPerSecond = 154813.7
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.19322510 * 2560; err = 0.35273437 * 2560; time = 0.0166s; samplesPerSecond = 154570.7
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.19844055 * 2560; err = 0.36445312 * 2560; time = 0.0166s; samplesPerSecond = 154673.4
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.20307465 * 2560; err = 0.34882812 * 2560; time = 0.0166s; samplesPerSecond = 154263.3
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.20918579 * 2560; err = 0.35976562 * 2560; time = 0.0166s; samplesPerSecond = 154608.0
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.22486877 * 2560; err = 0.35937500 * 2560; time = 0.0166s; samplesPerSecond = 154598.7
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.17906189 * 2560; err = 0.35937500 * 2560; time = 0.0166s; samplesPerSecond = 154040.6
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.16787720 * 2560; err = 0.35468750 * 2560; time = 0.0167s; samplesPerSecond = 153165.0
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.17252502 * 2560; err = 0.35351562 * 2560; time = 0.0166s; samplesPerSecond = 154654.7
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.21054993 * 2560; err = 0.36328125 * 2560; time = 0.0166s; samplesPerSecond = 153975.7
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.19859924 * 2560; err = 0.36328125 * 2560; time = 0.0166s; samplesPerSecond = 154496.1
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.12578735 * 2560; err = 0.34765625 * 2560; time = 0.0166s; samplesPerSecond = 154031.3
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.13427124 * 2560; err = 0.33710937 * 2560; time = 0.0166s; samplesPerSecond = 154533.4
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.18345642 * 2560; err = 0.36015625 * 2560; time = 0.0166s; samplesPerSecond = 154486.8
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.10770874 * 2560; err = 0.32812500 * 2560; time = 0.0166s; samplesPerSecond = 154552.0
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.15658264 * 2560; err = 0.36757812 * 2560; time = 0.0166s; samplesPerSecond = 154170.4
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.12143250 * 2560; err = 0.33398438 * 2560; time = 0.0165s; samplesPerSecond = 154701.5
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.17078552 * 2560; err = 0.34648438 * 2560; time = 0.0166s; samplesPerSecond = 154626.7
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.19542542 * 2560; err = 0.35195312 * 2560; time = 0.0166s; samplesPerSecond = 154179.7
08/04/2016 13:54:18:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.16595154 * 2560; err = 0.34843750 * 2560; time = 0.0167s; samplesPerSecond = 153412.8
08/04/2016 13:54:18: Finished Epoch[ 1 of 4]: [Training] ce = 1.39677525 * 81920; err = 0.39904785 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.613052s
08/04/2016 13:54:18: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.1'

08/04/2016 13:54:18: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 13:54:18: Starting minibatch loop.
08/04/2016 13:54:18:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.26056890 * 5120; err = 0.37558594 * 5120; time = 0.0285s; samplesPerSecond = 179422.5
08/04/2016 13:54:18:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.55791731 * 5120; err = 0.41152344 * 5120; time = 0.0259s; samplesPerSecond = 198019.8
08/04/2016 13:54:18:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.31589680 * 5120; err = 0.38750000 * 5120; time = 0.0258s; samplesPerSecond = 198380.4
08/04/2016 13:54:18:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.14716721 * 5120; err = 0.34414062 * 5120; time = 0.0259s; samplesPerSecond = 197927.9
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.16320839 * 5120; err = 0.35917969 * 5120; time = 0.0260s; samplesPerSecond = 197279.7
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.11159515 * 5120; err = 0.33378906 * 5120; time = 0.0259s; samplesPerSecond = 198004.5
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.07575150 * 5120; err = 0.33632812 * 5120; time = 0.0260s; samplesPerSecond = 197044.3
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.11748962 * 5120; err = 0.33671875 * 5120; time = 0.0259s; samplesPerSecond = 198012.1
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09913940 * 5120; err = 0.33457031 * 5120; time = 0.0259s; samplesPerSecond = 198012.1
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.06679916 * 5120; err = 0.33437500 * 5120; time = 0.0259s; samplesPerSecond = 198042.8
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.10906219 * 5120; err = 0.34492187 * 5120; time = 0.0260s; samplesPerSecond = 197279.7
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.07883606 * 5120; err = 0.32675781 * 5120; time = 0.0258s; samplesPerSecond = 198142.4
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.11491699 * 5120; err = 0.33691406 * 5120; time = 0.0258s; samplesPerSecond = 198534.3
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.05659943 * 5120; err = 0.32968750 * 5120; time = 0.0258s; samplesPerSecond = 198280.5
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.07409515 * 5120; err = 0.32773438 * 5120; time = 0.0258s; samplesPerSecond = 198549.7
08/04/2016 13:54:19:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.07450562 * 5120; err = 0.33515625 * 5120; time = 0.0258s; samplesPerSecond = 198411.2
08/04/2016 13:54:19: Finished Epoch[ 2 of 4]: [Training] ce = 1.15147181 * 81920; err = 0.34718018 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.41976s
08/04/2016 13:54:19: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.2'

08/04/2016 13:54:19: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

08/04/2016 13:54:19: Starting minibatch loop.
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.16433182 * 5120; err = 0.36503906 * 5120; time = 0.0265s; samplesPerSecond = 192858.2
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.08577080 * 5120; err = 0.33691406 * 5120; time = 0.0259s; samplesPerSecond = 197897.3
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.05754356 * 5120; err = 0.32714844 * 5120; time = 0.0258s; samplesPerSecond = 198511.2
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.08211365 * 5120; err = 0.33613281 * 5120; time = 0.0258s; samplesPerSecond = 198196.1
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.11165009 * 5120; err = 0.33808594 * 5120; time = 0.0258s; samplesPerSecond = 198196.1
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.09777298 * 5120; err = 0.33652344 * 5120; time = 0.0260s; samplesPerSecond = 197105.0
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.05753860 * 5120; err = 0.33125000 * 5120; time = 0.0259s; samplesPerSecond = 197866.7
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.03217468 * 5120; err = 0.31640625 * 5120; time = 0.0258s; samplesPerSecond = 198150.1
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.06591949 * 5120; err = 0.32871094 * 5120; time = 0.0258s; samplesPerSecond = 198827.2
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.04894638 * 5120; err = 0.32656250 * 5120; time = 0.0258s; samplesPerSecond = 198395.8
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.08949585 * 5120; err = 0.33710937 * 5120; time = 0.0259s; samplesPerSecond = 197851.5
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.10956802 * 5120; err = 0.35019531 * 5120; time = 0.0258s; samplesPerSecond = 198226.8
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.07778168 * 5120; err = 0.33769531 * 5120; time = 0.0258s; samplesPerSecond = 198180.8
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02749634 * 5120; err = 0.32832031 * 5120; time = 0.0259s; samplesPerSecond = 197950.9
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05765533 * 5120; err = 0.32988281 * 5120; time = 0.0259s; samplesPerSecond = 197462.3
08/04/2016 13:54:19:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02716217 * 5120; err = 0.31386719 * 5120; time = 0.0259s; samplesPerSecond = 198065.8
08/04/2016 13:54:19: Finished Epoch[ 3 of 4]: [Training] ce = 1.07455759 * 81920; err = 0.33374023 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.417338s
08/04/2016 13:54:19: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.3'

08/04/2016 13:54:19: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

08/04/2016 13:54:19: Starting minibatch loop.
08/04/2016 13:54:19:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.04238539 * 5120; err = 0.32265625 * 5120; time = 0.0265s; samplesPerSecond = 192880.0
08/04/2016 13:54:19:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.00743415 * 4926; err = 0.30937881 * 4926; time = 0.0499s; samplesPerSecond = 98695.7
08/04/2016 13:54:19:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01025753 * 5120; err = 0.31347656 * 5120; time = 0.0259s; samplesPerSecond = 197523.2
08/04/2016 13:54:19:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.01178360 * 5120; err = 0.31679687 * 5120; time = 0.0260s; samplesPerSecond = 196786.8
08/04/2016 13:54:19:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.12052345 * 5120; err = 0.35410156 * 5120; time = 0.0259s; samplesPerSecond = 198012.1
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.01440353 * 5120; err = 0.31699219 * 5120; time = 0.0258s; samplesPerSecond = 198434.2
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.97455025 * 5120; err = 0.30410156 * 5120; time = 0.0259s; samplesPerSecond = 197790.3
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.00521698 * 5120; err = 0.32460937 * 5120; time = 0.0258s; samplesPerSecond = 198127.1
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.97339859 * 5120; err = 0.30136719 * 5120; time = 0.0259s; samplesPerSecond = 197889.7
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.96836319 * 5120; err = 0.30292969 * 5120; time = 0.0259s; samplesPerSecond = 197584.2
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.00247421 * 5120; err = 0.31113281 * 5120; time = 0.0259s; samplesPerSecond = 197569.0
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.00154114 * 5120; err = 0.31406250 * 5120; time = 0.0259s; samplesPerSecond = 197729.2
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.97331314 * 5120; err = 0.31054688 * 5120; time = 0.0260s; samplesPerSecond = 197150.6
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97797546 * 5120; err = 0.30605469 * 5120; time = 0.0260s; samplesPerSecond = 197188.5
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.99987946 * 5120; err = 0.30839844 * 5120; time = 0.0259s; samplesPerSecond = 197393.8
08/04/2016 13:54:20:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.98577881 * 5120; err = 0.30722656 * 5120; time = 0.0258s; samplesPerSecond = 198403.5
08/04/2016 13:54:20: Finished Epoch[ 4 of 4]: [Training] ce = 1.00435781 * 81920; err = 0.31411133 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.443713s
08/04/2016 13:54:20: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech'
08/04/2016 13:54:20: CNTKCommandTrainEnd: speechTrain

08/04/2016 13:54:20: Action "train" complete.


08/04/2016 13:54:20: ##############################################################################
08/04/2016 13:54:20: #                                                                            #
08/04/2016 13:54:20: # Action "edit"                                                              #
08/04/2016 13:54:20: #                                                                            #
08/04/2016 13:54:20: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *7]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *7]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *7], [363 x 1], [363 x 1] -> [363 x *7]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *7] -> [512 x *7]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *7], [512 x 1] -> [512 x 1 x *7]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *7] -> [512 x 1 x *7]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *7] -> [132 x 1 x *7]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *7], [132 x 1] -> [132 x 1 x *7]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *7], [132 x 1 x *7], [132 x 1 x *7] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *7], [132 x 1 x *7] -> [1]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 13:54:20: Action "edit" complete.


08/04/2016 13:54:20: ##############################################################################
08/04/2016 13:54:20: #                                                                            #
08/04/2016 13:54:20: # Action "train"                                                             #
08/04/2016 13:54:20: #                                                                            #
08/04/2016 13:54:20: ##############################################################################

08/04/2016 13:54:20: CNTKCommandTrainBegin: sequenceTrain
NDLBuilder Using GPU 0
simplesenonehmm: reading '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/model.overalltying', '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list', '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/state.list
htkmlfreader: reading MLF file /tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/TestData/CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames

08/04/2016 13:54:20: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0'.

Post-processing network...

3 roots:
	ce = SequenceWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *9]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *9]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *9], [363 x 1], [363 x 1] -> [363 x *9]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *9] -> [512 x *9]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *9], [512 x 1] -> [512 x 1 x *9]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *9] -> [512 x 1 x *9]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *9] -> [132 x 1 x *9]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *9], [132 x 1] -> [132 x 1 x *9]
Validating --> ce = SequenceWithSoftmax (labels, OL.z, scaledLogLikelihood) : [132 x *9], [132 x 1 x *9], [132 x 1 x *9] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *9], [132 x 1 x *9] -> [1]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 13:54:20: Loaded model with 29 nodes on GPU 0.

08/04/2016 13:54:20: Training criterion node(s):
08/04/2016 13:54:20: 	ce = SequenceWithSoftmax

08/04/2016 13:54:20: Evaluation criterion node(s):

08/04/2016 13:54:20: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *9]] [features Gradient[363 x *9]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *9]] [logPrior Gradient[132 x 1]] }
0x7fe7fc916248: {[features Value[363 x *9]] }
0x7fe7fc918518: {[err Value[1]] }
0x7fe7fc9776e8: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *9]] }
0x7fe7fc9a2948: {[OL.b Value[132 x 1]] }
0x7fe7fc9aa338: {[HL1.t Gradient[512 x *9]] [HL1.y Value[512 x 1 x *9]] }
0x7fe7fc9af298: {[HL1.W Value[512 x 363]] }
0x7fe7fc9b8758: {[labels Value[132 x *9]] }
0x7fe7fc9c2728: {[scaledLogLikelihood Value[132 x 1 x *9]] }
0x7fe7fc9d1158: {[HL1.t Value[512 x *9]] }
0x7fe7fc9d3828: {[OL.W Value[132 x 512]] }
0x7fe804ba00c8: {[HL2.b Value[512 x 1]] }
0x7fe804d0f1c8: {[HL1.b Value[512 x 1]] }
0x7fe804f25798: {[ce Gradient[1]] }
0x7fe804f25958: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *9]] [OL.z Gradient[132 x 1 x *9]] }
0x7fe804f25b18: {[OL.t Gradient[132 x 1 x *9]] [scaledLogLikelihood Gradient[132 x 1 x *9]] }
0x7fe804f25cd8: {[OL.b Gradient[132 x 1]] }
0x7fe804f29168: {[HL3.b Value[512 x 1]] }
0x7fe804f2b4b8: {[globalInvStd Value[363 x 1]] }
0x7fe804f2d738: {[featNorm Value[363 x *9]] }
0x7fe804f3f118: {[HL2.W Value[512 x 512]] }
0x7fe807366298: {[HL2.t Gradient[512 x 1 x *9]] [HL2.y Value[512 x 1 x *9]] }
0x7fe807366458: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *9]] [HL2.z Gradient[512 x 1 x *9]] [HL3.t Value[512 x 1 x *9]] }
0x7fe807366618: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *9]] }
0x7fe8073667d8: {[HL3.t Gradient[512 x 1 x *9]] [HL3.y Value[512 x 1 x *9]] }
0x7fe807366998: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *9]] [HL3.z Gradient[512 x 1 x *9]] [OL.t Value[132 x 1 x *9]] }
0x7fe807366b58: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *9]] }
0x7fe80b330e68: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *9]] }
0x7fe80b3732c8: {[globalPrior Value[132 x 1]] }
0x7fe80b38fbb8: {[HL3.W Value[512 x 512]] }
0x7fe80b3a2848: {[HL1.z Gradient[512 x 1 x *9]] [HL2.t Value[512 x 1 x *9]] }
0x7fe80b3a6908: {[logPrior Value[132 x 1]] }
0x7fe80b3cbb68: {[globalMean Value[363 x 1]] }
0x7fe80b3ce208: {[ce Value[1]] }

08/04/2016 13:54:20: No PreCompute nodes found, skipping PreCompute step.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-10
Setting SeqGammar-related parameters: amf=14.00, lmf=14.00, wp=0.00, bMMIFactor=0.00, usesMBR=false

08/04/2016 13:54:20: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 13:54:24: Starting minibatch loop.
dengamma value 1.039771
dengamma value 1.008860
dengamma value 1.002085
dengamma value 0.949738
dengamma value 0.992956
dengamma value 1.087056
dengamma value 1.064416
dengamma value 1.005506
dengamma value 0.921625
dengamma value 0.988952
dengamma value 1.061293
dengamma value 0.998910
dengamma value 0.989168
dengamma value 1.094359
dengamma value 1.038040
dengamma value 0.954046
dengamma value 1.017982
dengamma value 1.009348
dengamma value 1.067278
dengamma value 1.070337
dengamma value 0.958094
dengamma value 1.044646
dengamma value 1.031717
08/04/2016 13:54:26:  Epoch[ 1 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.06469340 * 6504; err = 0.39083641 * 6504; time = 1.4770s; samplesPerSecond = 4403.6
dengamma value 1.047024
dengamma value 1.063842
dengamma value 1.079810
dengamma value 1.055895
dengamma value 1.029205
dengamma value 1.043464
dengamma value 1.050164
dengamma value 1.014062
dengamma value 1.025529
dengamma value 0.999811
dengamma value 0.954664
dengamma value 0.980832
dengamma value 1.025532
dengamma value 1.054759
dengamma value 1.042974
dengamma value 1.058477
dengamma value 1.002889
dengamma value 0.987038
dengamma value 1.027580
dengamma value 1.044088
dengamma value 0.976494
dengamma value 1.061087
dengamma value 1.038997
dengamma value 0.994471
08/04/2016 13:54:26:  Epoch[ 1 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.07072530 * 5642; err = 0.38674229 * 5642; time = 0.4779s; samplesPerSecond = 11804.7
dengamma value 1.079603
dengamma value 1.040860
dengamma value 1.047533
dengamma value 1.043554
dengamma value 0.998665
dengamma value 0.979312
dengamma value 1.020268
dengamma value 1.017433
dengamma value 1.016191
dengamma value 1.075303
dengamma value 1.031240
dengamma value 0.992018
dengamma value 1.095154
dengamma value 1.042986
dengamma value 1.071314
dengamma value 1.032380
dengamma value 0.968015
dengamma value 0.970105
dengamma value 1.038717
dengamma value 0.999547
dengamma value 1.027746
dengamma value 1.005806
dengamma value 1.029988
dengamma value 1.047051
08/04/2016 13:54:27:  Epoch[ 1 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.07008593 * 6102; err = 0.37184530 * 6102; time = 0.5272s; samplesPerSecond = 11574.3
dengamma value 1.012969
dengamma value 1.051036
dengamma value 1.051214
dengamma value 1.008024
dengamma value 1.087781
dengamma value 1.083987
dengamma value 1.008060
dengamma value 1.022486
dengamma value 0.930406
dengamma value 1.065754
dengamma value 1.007726
dengamma value 0.973926
dengamma value 1.013584
dengamma value 1.063702
dengamma value 1.028542
dengamma value 0.997495
dengamma value 1.025576
dengamma value 1.020712
dengamma value 1.028654
dengamma value 1.031203
dengamma value 0.993454
dengamma value 1.040462
dengamma value 1.023351
dengamma value 0.991811
08/04/2016 13:54:28:  Epoch[ 1 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.07728385 * 7412; err = 0.36009174 * 7412; time = 0.6509s; samplesPerSecond = 11387.7
dengamma value 1.040723
dengamma value 1.054979
dengamma value 1.074546
dengamma value 1.013332
dengamma value 1.001707
dengamma value 1.047603
dengamma value 1.050264
dengamma value 1.069459
dengamma value 1.066440
dengamma value 1.012459
dengamma value 1.029881
dengamma value 1.062390
dengamma value 1.029718
dengamma value 0.999053
dengamma value 1.019313
dengamma value 1.002976
dengamma value 0.974984
dengamma value 1.023589
dengamma value 1.019069
dengamma value 0.959909
dengamma value 0.875768
08/04/2016 13:54:28:  Epoch[ 1 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.07445183 * 4878; err = 0.37658877 * 4878; time = 0.4069s; samplesPerSecond = 11988.1
dengamma value 1.046407
dengamma value 0.991080
dengamma value 1.024087
dengamma value 1.002174
dengamma value 1.039784
dengamma value 1.043419
dengamma value 1.098588
dengamma value 1.044616
dengamma value 1.058067
dengamma value 1.033657
dengamma value 1.011191
dengamma value 1.075042
dengamma value 1.003094
dengamma value 1.023017
dengamma value 1.057970
dengamma value 0.939363
dengamma value 0.993083
dengamma value 1.054491
dengamma value 1.067332
dengamma value 1.006874
dengamma value 1.018433
dengamma value 1.055201
dengamma value 1.041745
dengamma value 0.927560
dengamma value 1.008095
08/04/2016 13:54:29:  Epoch[ 1 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.07291757 * 7050; err = 0.35687943 * 7050; time = 0.5849s; samplesPerSecond = 12054.0
dengamma value 1.015860
dengamma value 1.018649
dengamma value 0.965674
dengamma value 0.974820
dengamma value 1.012597
dengamma value 1.105329
dengamma value 0.990132
dengamma value 1.033039
dengamma value 1.046967
dengamma value 0.949779
dengamma value 1.026569
dengamma value 0.920622
dengamma value 0.935995
dengamma value 0.980859
dengamma value 0.990260
dengamma value 0.986321
dengamma value 1.116884
dengamma value 1.102966
dengamma value 1.054944
dengamma value 1.055356
dengamma value 1.005845
dengamma value 1.068431
dengamma value 1.058301
dengamma value 1.028355
dengamma value 1.046318
08/04/2016 13:54:29:  Epoch[ 1 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07613070 * 5870; err = 0.37223169 * 5870; time = 0.4618s; samplesPerSecond = 12710.7
dengamma value 1.021894
dengamma value 1.025439
dengamma value 1.002320
dengamma value 1.094426
dengamma value 1.009210
dengamma value 1.019635
dengamma value 1.033418
dengamma value 1.019738
dengamma value 1.051134
dengamma value 0.997718
dengamma value 1.050877
dengamma value 0.928451
dengamma value 0.958983
dengamma value 1.032394
dengamma value 1.092752
dengamma value 0.987304
dengamma value 0.992877
dengamma value 1.021879
dengamma value 1.026897
dengamma value 1.056344
dengamma value 1.067380
dengamma value 1.030784
08/04/2016 13:54:30:  Epoch[ 1 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.07809831 * 5936; err = 0.34063342 * 5936; time = 0.5252s; samplesPerSecond = 11301.6
dengamma value 0.992179
dengamma value 1.032738
dengamma value 1.015661
dengamma value 0.982770
dengamma value 0.958191
dengamma value 1.087113
dengamma value 1.073352
dengamma value 1.025810
dengamma value 1.001832
dengamma value 1.054460
dengamma value 1.079779
dengamma value 1.019906
dengamma value 1.028527
dengamma value 1.019344
dengamma value 1.072422
dengamma value 1.020247
dengamma value 1.019066
dengamma value 1.031874
dengamma value 1.012291
dengamma value 1.044354
dengamma value 1.060817
dengamma value 1.003855
08/04/2016 13:54:30:  Epoch[ 1 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08151601 * 5116; err = 0.31489445 * 5116; time = 0.4532s; samplesPerSecond = 11289.8
dengamma value 1.009000
dengamma value 0.957195
dengamma value 1.002025
dengamma value 1.075853
dengamma value 1.048615
dengamma value 1.039516
dengamma value 0.975351
dengamma value 0.994097
dengamma value 1.087969
dengamma value 1.042295
dengamma value 0.993466
dengamma value 1.040734
dengamma value 0.967594
dengamma value 1.057782
dengamma value 1.013357
dengamma value 0.968645
dengamma value 1.055181
dengamma value 1.012347
dengamma value 0.945431
dengamma value 1.019795
dengamma value 1.052326
dengamma value 0.975220
dengamma value 1.042035
dengamma value 1.094060
08/04/2016 13:54:31:  Epoch[ 1 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.07468942 * 6182; err = 0.35538661 * 6182; time = 0.5151s; samplesPerSecond = 12001.4
dengamma value 1.049323
dengamma value 0.999397
dengamma value 1.024856
dengamma value 0.984013
dengamma value 0.969381
dengamma value 0.953721
dengamma value 1.007908
dengamma value 0.961619
dengamma value 1.036517
dengamma value 0.969941
dengamma value 1.039889
dengamma value 0.999214
dengamma value 1.006822
dengamma value 0.971873
dengamma value 1.009260
dengamma value 1.036282
dengamma value 1.039571
dengamma value 1.019917
dengamma value 1.068641
dengamma value 1.019767
dengamma value 1.039233
dengamma value 1.046051
dengamma value 0.952872
08/04/2016 13:54:31:  Epoch[ 1 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08374304 * 6614; err = 0.32400968 * 6614; time = 0.5560s; samplesPerSecond = 11896.0
dengamma value 0.976269
dengamma value 1.043678
dengamma value 1.021862
dengamma value 1.080781
dengamma value 1.019086
dengamma value 1.010714
dengamma value 0.975963
dengamma value 1.051607
dengamma value 0.997648
dengamma value 0.984958
dengamma value 1.027001
dengamma value 1.002849
dengamma value 1.028476
dengamma value 1.017829
dengamma value 0.946247
dengamma value 0.989007
dengamma value 1.009865
dengamma value 0.908882
dengamma value 0.966572
dengamma value 1.032903
dengamma value 1.010260
08/04/2016 13:54:31:  Epoch[ 1 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08453749 * 5008; err = 0.35942492 * 5008; time = 0.3926s; samplesPerSecond = 12757.0
dengamma value 1.045455
dengamma value 1.043195
dengamma value 0.958151
dengamma value 0.992466
dengamma value 1.016207
dengamma value 0.999867
dengamma value 1.044860
dengamma value 1.126963
dengamma value 1.073033
dengamma value 1.041269
dengamma value 1.037333
dengamma value 0.944177
dengamma value 1.004047
dengamma value 1.040049
dengamma value 1.008682
dengamma value 1.045216
dengamma value 1.038186
dengamma value 1.116788
dengamma value 1.050810
dengamma value 1.049018
dengamma value 1.037930
dengamma value 1.059146
dengamma value 1.034995
08/04/2016 13:54:32:  Epoch[ 1 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.07792706 * 6764; err = 0.31992904 * 6764; time = 0.5721s; samplesPerSecond = 11823.5
dengamma value 0.998926
dengamma value 0.989216
dengamma value 1.054536
dengamma value 1.018531
dengamma value 1.031053
dengamma value 0.960805
dengamma value 0.974687
dengamma value 1.001206
dengamma value 1.065157
dengamma value 0.998547
dengamma value 0.998547
dengamma value 0.982807
dengamma value 0.998592
dengamma value 0.998592
08/04/2016 13:54:32: Finished Epoch[ 1 of 3]: [Training] ce = 0.07627162 * 82650; err = 0.35553539 * 82650; totalSamplesSeen = 82650; learningRatePerSample = 2e-06; epochTime=12.1821s
08/04/2016 13:54:32: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.1'

08/04/2016 13:54:32: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 82176), data subset 0 of 1, with 1 datapasses

08/04/2016 13:54:32: Starting minibatch loop.
dengamma value 1.036684
dengamma value 1.009842
dengamma value 1.026739
dengamma value 0.912320
dengamma value 1.097263
dengamma value 0.995629
dengamma value 1.078168
dengamma value 1.001596
dengamma value 0.977730
dengamma value 0.962392
dengamma value 1.057881
dengamma value 1.047810
dengamma value 1.017029
dengamma value 1.074566
dengamma value 1.091998
dengamma value 0.991447
dengamma value 1.017280
dengamma value 0.982913
dengamma value 1.060633
dengamma value 1.058611
dengamma value 1.055063
dengamma value 1.065809
dengamma value 1.023396
dengamma value 1.003444
08/04/2016 13:54:33:  Epoch[ 2 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08156399 * 6272; err = 0.30978954 * 6272; time = 0.5355s; samplesPerSecond = 11711.7
dengamma value 0.991122
dengamma value 1.049612
dengamma value 0.963755
dengamma value 1.026547
dengamma value 1.006080
dengamma value 0.971548
dengamma value 1.056218
dengamma value 1.036323
dengamma value 1.047618
dengamma value 1.016893
dengamma value 0.988867
dengamma value 1.028221
dengamma value 1.055538
dengamma value 1.038413
dengamma value 0.995236
dengamma value 1.072303
dengamma value 1.009829
dengamma value 1.050626
dengamma value 1.043466
dengamma value 1.001944
dengamma value 1.027629
dengamma value 0.987460
dengamma value 0.963830
08/04/2016 13:54:33:  Epoch[ 2 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08143244 * 6264; err = 0.34434866 * 6264; time = 0.5148s; samplesPerSecond = 12168.1
dengamma value 0.929408
dengamma value 1.031480
dengamma value 1.019848
dengamma value 1.085069
dengamma value 1.019352
dengamma value 0.944314
dengamma value 1.028144
dengamma value 1.072228
dengamma value 1.045587
dengamma value 1.021673
dengamma value 1.033253
dengamma value 0.933313
dengamma value 0.980522
dengamma value 0.963293
dengamma value 0.999026
dengamma value 1.034523
dengamma value 0.999205
dengamma value 0.982600
dengamma value 1.008070
dengamma value 0.985843
dengamma value 1.056509
dengamma value 1.023122
08/04/2016 13:54:34:  Epoch[ 2 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08922094 * 6616; err = 0.34144498 * 6616; time = 0.5188s; samplesPerSecond = 12752.1
dengamma value 0.994430
dengamma value 1.094051
dengamma value 0.922474
dengamma value 1.018722
dengamma value 0.998303
dengamma value 1.060654
dengamma value 0.986623
dengamma value 1.041281
dengamma value 1.034348
dengamma value 1.071623
dengamma value 1.029659
dengamma value 1.007770
dengamma value 1.000930
dengamma value 1.017975
dengamma value 1.026661
dengamma value 1.050784
dengamma value 1.018837
dengamma value 0.988144
dengamma value 0.996864
dengamma value 0.933014
dengamma value 0.988900
dengamma value 1.071973
dengamma value 1.034660
dengamma value 1.018032
08/04/2016 13:54:35:  Epoch[ 2 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08042956 * 6732; err = 0.34685086 * 6732; time = 0.6028s; samplesPerSecond = 11168.2
dengamma value 1.005288
dengamma value 1.075452
dengamma value 0.987781
dengamma value 0.983235
dengamma value 0.997290
dengamma value 1.022472
dengamma value 1.065230
dengamma value 1.001937
dengamma value 1.019698
dengamma value 1.056590
dengamma value 1.040931
dengamma value 1.045537
dengamma value 0.900799
dengamma value 1.082395
dengamma value 0.989645
dengamma value 1.023341
dengamma value 1.011287
dengamma value 1.036037
dengamma value 0.944953
dengamma value 0.997007
dengamma value 1.070311
dengamma value 1.082398
dengamma value 1.046615
08/04/2016 13:54:35:  Epoch[ 2 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08169604 * 5914; err = 0.32363882 * 5914; time = 0.4910s; samplesPerSecond = 12044.3
dengamma value 0.951223
dengamma value 1.049390
dengamma value 0.996390
dengamma value 1.007866
dengamma value 1.051670
dengamma value 1.088676
dengamma value 0.999147
dengamma value 1.043790
dengamma value 1.037285
dengamma value 1.067294
dengamma value 0.957226
dengamma value 1.042345
dengamma value 1.101494
dengamma value 0.993688
dengamma value 1.072176
dengamma value 0.952132
dengamma value 1.053663
dengamma value 0.953022
dengamma value 1.022956
dengamma value 1.001466
dengamma value 1.065818
dengamma value 0.970890
dengamma value 1.019962
dengamma value 0.993931
dengamma value 0.954014
08/04/2016 13:54:36:  Epoch[ 2 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08243057 * 6460; err = 0.32383901 * 6460; time = 0.5468s; samplesPerSecond = 11813.9
dengamma value 1.076297
dengamma value 1.087974
dengamma value 0.999473
dengamma value 1.026446
dengamma value 0.995473
dengamma value 0.976292
dengamma value 0.993987
dengamma value 1.026644
dengamma value 0.977158
dengamma value 0.992908
dengamma value 1.068122
dengamma value 1.033753
dengamma value 0.962241
dengamma value 1.011447
dengamma value 1.034971
dengamma value 1.046527
dengamma value 0.989569
dengamma value 1.044148
dengamma value 1.016779
dengamma value 0.974637
08/04/2016 13:54:36:  Epoch[ 2 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07963895 * 6060; err = 0.33960396 * 6060; time = 0.4908s; samplesPerSecond = 12348.4
dengamma value 1.037764
dengamma value 1.010481
dengamma value 0.944881
dengamma value 0.972943
dengamma value 1.030752
dengamma value 1.005769
dengamma value 1.045666
dengamma value 1.001043
dengamma value 1.009148
dengamma value 1.068085
dengamma value 1.063954
dengamma value 1.009344
dengamma value 0.990921
dengamma value 1.087348
dengamma value 1.050772
dengamma value 1.058387
dengamma value 1.002505
dengamma value 0.999110
dengamma value 1.039413
dengamma value 1.029288
dengamma value 1.056919
dengamma value 1.020245
08/04/2016 13:54:37:  Epoch[ 2 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08824434 * 6996; err = 0.31046312 * 6996; time = 0.6001s; samplesPerSecond = 11658.3
dengamma value 1.011887
dengamma value 0.987695
dengamma value 1.034526
dengamma value 1.019890
dengamma value 0.927869
dengamma value 1.029231
dengamma value 1.050664
dengamma value 0.969061
dengamma value 1.037142
dengamma value 0.975268
dengamma value 1.075414
dengamma value 1.044737
dengamma value 1.008564
dengamma value 1.020024
dengamma value 0.970714
dengamma value 0.962018
dengamma value 0.970190
dengamma value 0.971826
dengamma value 0.971022
dengamma value 1.023691
dengamma value 1.002572
08/04/2016 13:54:37:  Epoch[ 2 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08193034 * 6668; err = 0.35947810 * 6668; time = 0.5410s; samplesPerSecond = 12324.6
dengamma value 1.014048
dengamma value 1.058976
dengamma value 1.010158
dengamma value 1.080150
dengamma value 1.018570
dengamma value 1.038481
dengamma value 1.041726
dengamma value 0.999272
dengamma value 1.033859
dengamma value 1.014322
dengamma value 1.029895
dengamma value 1.030509
dengamma value 1.036786
dengamma value 1.037183
dengamma value 0.961296
dengamma value 1.004140
dengamma value 1.001126
dengamma value 1.038268
dengamma value 1.081144
dengamma value 1.080584
dengamma value 0.976872
dengamma value 1.015145
dengamma value 0.983886
dengamma value 1.004897
dengamma value 0.986298
08/04/2016 13:54:38:  Epoch[ 2 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08194725 * 5830; err = 0.31938250 * 5830; time = 0.4919s; samplesPerSecond = 11851.4
dengamma value 0.944307
dengamma value 1.022248
dengamma value 0.972960
dengamma value 1.030391
dengamma value 1.032517
dengamma value 1.073886
dengamma value 0.999901
dengamma value 1.053174
dengamma value 1.034128
dengamma value 0.960718
dengamma value 1.029794
dengamma value 1.025264
dengamma value 0.988985
dengamma value 1.082752
dengamma value 1.026283
dengamma value 1.062488
dengamma value 1.126596
dengamma value 1.015529
dengamma value 0.979542
dengamma value 1.030129
dengamma value 1.070049
dengamma value 1.047260
dengamma value 1.110543
dengamma value 1.045509
08/04/2016 13:54:38:  Epoch[ 2 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08495780 * 5762; err = 0.32211038 * 5762; time = 0.4804s; samplesPerSecond = 11993.5
dengamma value 1.014098
dengamma value 0.993713
dengamma value 1.035517
dengamma value 0.918329
dengamma value 1.000139
dengamma value 1.049161
dengamma value 1.072710
dengamma value 1.036319
dengamma value 0.946821
dengamma value 0.991024
dengamma value 0.995384
dengamma value 1.019046
dengamma value 1.045420
dengamma value 1.076055
dengamma value 1.038598
dengamma value 0.983714
dengamma value 1.027145
dengamma value 0.985688
dengamma value 1.039965
dengamma value 1.072389
dengamma value 1.033930
dengamma value 1.017449
08/04/2016 13:54:39:  Epoch[ 2 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08209059 * 5776; err = 0.34210526 * 5776; time = 0.4724s; samplesPerSecond = 12226.2
dengamma value 1.026886
dengamma value 1.040809
dengamma value 1.007790
dengamma value 1.066827
dengamma value 1.052916
dengamma value 1.011955
dengamma value 1.026753
dengamma value 0.987055
dengamma value 1.017708
dengamma value 1.052085
dengamma value 0.996643
dengamma value 0.979976
dengamma value 1.063427
dengamma value 1.018625
dengamma value 0.975540
dengamma value 1.039149
dengamma value 1.059036
dengamma value 1.012622
dengamma value 1.009589
dengamma value 1.029966
dengamma value 0.982966
dengamma value 0.957563
dengamma value 0.975086
dengamma value 0.975086
08/04/2016 13:54:39:  Epoch[ 2 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08155027 * 6552; err = 0.35439560 * 6552; time = 0.5215s; samplesPerSecond = 12563.8
08/04/2016 13:54:39: Finished Epoch[ 2 of 3]: [Training] ce = 0.08291720 * 81902; err = 0.33385021 * 81902; totalSamplesSeen = 164552; learningRatePerSample = 2e-06; epochTime=6.8106s
08/04/2016 13:54:39: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.2'

08/04/2016 13:54:39: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163920), data subset 0 of 1, with 1 datapasses

08/04/2016 13:54:39: Starting minibatch loop.
dengamma value 1.033790
dengamma value 1.058838
dengamma value 0.983285
dengamma value 1.004307
dengamma value 1.003259
dengamma value 0.972868
dengamma value 1.044027
dengamma value 1.055795
dengamma value 1.027445
dengamma value 0.991712
dengamma value 0.984888
dengamma value 1.000839
dengamma value 0.998538
dengamma value 1.014974
dengamma value 1.049593
dengamma value 0.992588
dengamma value 1.044645
dengamma value 0.976084
dengamma value 1.049322
dengamma value 1.031955
dengamma value 1.055606
08/04/2016 13:54:40:  Epoch[ 3 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08120704 * 5488; err = 0.34876093 * 5488; time = 0.4521s; samplesPerSecond = 12138.1
dengamma value 1.017106
dengamma value 1.093662
dengamma value 0.978995
dengamma value 0.986668
dengamma value 1.036072
dengamma value 1.063224
dengamma value 1.039743
dengamma value 1.038468
dengamma value 1.004264
dengamma value 1.047054
dengamma value 1.098219
dengamma value 1.004887
dengamma value 1.056782
dengamma value 0.985638
dengamma value 1.011937
dengamma value 1.032978
dengamma value 0.992828
dengamma value 1.040075
dengamma value 1.050890
dengamma value 0.978007
dengamma value 1.018215
dengamma value 1.046342
dengamma value 1.032683
dengamma value 0.981305
08/04/2016 13:54:40:  Epoch[ 3 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08031380 * 6472; err = 0.31257726 * 6472; time = 0.5343s; samplesPerSecond = 12112.3
dengamma value 1.061669
dengamma value 0.986433
dengamma value 1.024128
dengamma value 1.068616
dengamma value 0.980619
dengamma value 1.068163
dengamma value 1.002150
dengamma value 1.015770
dengamma value 1.004058
dengamma value 1.042056
dengamma value 1.034684
dengamma value 1.038462
dengamma value 1.018862
dengamma value 1.061874
dengamma value 1.107896
dengamma value 1.091542
dengamma value 1.093607
dengamma value 1.070646
dengamma value 1.029326
dengamma value 1.026666
dengamma value 1.006278
08/04/2016 13:54:41:  Epoch[ 3 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08349640 * 5858; err = 0.27466712 * 5858; time = 0.5075s; samplesPerSecond = 11542.6
dengamma value 1.061643
dengamma value 1.009640
dengamma value 1.056302
dengamma value 1.050337
dengamma value 1.022621
dengamma value 1.028736
dengamma value 1.108396
dengamma value 0.993091
dengamma value 1.036169
dengamma value 1.042789
dengamma value 1.057740
dengamma value 1.080588
dengamma value 0.997437
dengamma value 1.032444
dengamma value 1.092876
dengamma value 1.002956
dengamma value 1.007864
dengamma value 0.917104
dengamma value 1.069846
dengamma value 1.068802
dengamma value 1.001328
dengamma value 1.030212
08/04/2016 13:54:41:  Epoch[ 3 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.07897599 * 6336; err = 0.30855429 * 6336; time = 0.5398s; samplesPerSecond = 11738.0
dengamma value 1.025045
dengamma value 0.982082
dengamma value 1.044821
dengamma value 0.974218
dengamma value 1.040537
dengamma value 1.046773
dengamma value 0.998308
dengamma value 0.978167
dengamma value 1.040567
dengamma value 1.108584
dengamma value 1.056041
dengamma value 0.975755
dengamma value 1.026941
dengamma value 0.980630
dengamma value 0.932152
dengamma value 1.016942
dengamma value 1.052292
dengamma value 1.010177
dengamma value 0.965717
dengamma value 0.992414
dengamma value 1.039188
08/04/2016 13:54:42:  Epoch[ 3 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08612748 * 6178; err = 0.32243444 * 6178; time = 0.4937s; samplesPerSecond = 12514.9
dengamma value 0.979342
dengamma value 1.072233
dengamma value 0.971029
dengamma value 1.072903
dengamma value 0.982876
dengamma value 0.998701
dengamma value 0.972198
dengamma value 1.023149
dengamma value 0.999761
dengamma value 1.017512
dengamma value 1.061328
dengamma value 1.061926
dengamma value 1.035242
dengamma value 1.051501
dengamma value 1.030696
dengamma value 1.042199
dengamma value 1.019915
dengamma value 0.922008
dengamma value 1.035822
dengamma value 1.023497
dengamma value 1.020997
dengamma value 0.971261
dengamma value 1.066127
08/04/2016 13:54:42:  Epoch[ 3 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08515951 * 5074; err = 0.34489555 * 5074; time = 0.4207s; samplesPerSecond = 12061.4
dengamma value 1.043463
dengamma value 1.097145
dengamma value 1.049169
dengamma value 0.983890
dengamma value 1.003426
dengamma value 0.984324
dengamma value 1.054304
dengamma value 1.086776
dengamma value 1.021471
dengamma value 1.048020
dengamma value 1.051759
dengamma value 1.015529
dengamma value 1.076996
dengamma value 0.952686
dengamma value 0.984274
dengamma value 1.061030
dengamma value 1.038014
dengamma value 1.040453
dengamma value 0.992475
dengamma value 1.004483
dengamma value 1.005049
dengamma value 1.047607
dengamma value 1.054446
dengamma value 0.990763
08/04/2016 13:54:43:  Epoch[ 3 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08047710 * 6222; err = 0.32176149 * 6222; time = 0.5274s; samplesPerSecond = 11797.9
dengamma value 0.984221
dengamma value 1.045665
dengamma value 1.047811
dengamma value 1.023892
dengamma value 1.006430
dengamma value 1.080667
dengamma value 1.039123
dengamma value 1.001006
dengamma value 1.061440
dengamma value 1.105940
dengamma value 1.072562
dengamma value 1.072595
dengamma value 0.982882
dengamma value 1.057767
dengamma value 0.972291
dengamma value 1.034094
dengamma value 1.054247
dengamma value 0.955296
dengamma value 1.004730
dengamma value 1.010153
dengamma value 1.001975
dengamma value 0.990743
dengamma value 1.027101
08/04/2016 13:54:43:  Epoch[ 3 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.09061576 * 6074; err = 0.31165624 * 6074; time = 0.4924s; samplesPerSecond = 12335.0
dengamma value 1.057833
dengamma value 1.052189
dengamma value 1.056383
dengamma value 1.040636
dengamma value 1.066776
dengamma value 1.018584
dengamma value 0.927616
dengamma value 1.022886
dengamma value 1.080715
dengamma value 1.046271
dengamma value 1.025684
dengamma value 0.950625
dengamma value 1.043502
dengamma value 1.041111
dengamma value 1.085643
dengamma value 0.992384
dengamma value 1.058979
dengamma value 0.987389
dengamma value 1.048694
dengamma value 1.064151
08/04/2016 13:54:44:  Epoch[ 3 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.07822802 * 6640; err = 0.31445783 * 6640; time = 0.5473s; samplesPerSecond = 12132.3
dengamma value 1.055242
dengamma value 1.044913
dengamma value 0.991851
dengamma value 1.062615
dengamma value 0.980770
dengamma value 1.028849
dengamma value 1.034709
dengamma value 0.988767
dengamma value 0.950344
dengamma value 1.034132
dengamma value 0.974192
dengamma value 1.079511
dengamma value 1.059883
dengamma value 1.038388
dengamma value 1.054800
dengamma value 1.006153
dengamma value 1.027013
dengamma value 1.064581
dengamma value 1.004798
dengamma value 1.032774
dengamma value 1.043434
dengamma value 1.049451
08/04/2016 13:54:44:  Epoch[ 3 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08341092 * 6266; err = 0.34663262 * 6266; time = 0.5442s; samplesPerSecond = 11514.5
dengamma value 1.022210
dengamma value 0.986740
dengamma value 0.871403
dengamma value 1.031950
dengamma value 0.974749
dengamma value 0.955691
dengamma value 1.030363
dengamma value 1.003471
dengamma value 1.093915
dengamma value 0.992246
dengamma value 1.048218
dengamma value 0.983352
dengamma value 1.023563
dengamma value 1.010574
dengamma value 1.074733
dengamma value 1.051278
dengamma value 0.983042
dengamma value 1.027488
dengamma value 1.083554
dengamma value 0.991610
dengamma value 1.040491
dengamma value 1.039416
dengamma value 1.020365
dengamma value 1.039064
dengamma value 1.011343
08/04/2016 13:54:45:  Epoch[ 3 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08779425 * 7250; err = 0.34262069 * 7250; time = 0.5944s; samplesPerSecond = 12196.7
dengamma value 1.043015
dengamma value 1.105014
dengamma value 1.083279
dengamma value 1.092601
dengamma value 0.991600
dengamma value 1.032533
dengamma value 0.974271
dengamma value 1.022984
dengamma value 1.009730
dengamma value 1.039301
dengamma value 0.994922
dengamma value 0.981159
dengamma value 0.946376
dengamma value 1.018087
dengamma value 1.008841
dengamma value 1.101816
dengamma value 1.106492
dengamma value 1.029815
dengamma value 1.033678
dengamma value 1.057870
dengamma value 1.040121
08/04/2016 13:54:45:  Epoch[ 3 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08327231 * 5708; err = 0.31482130 * 5708; time = 0.4972s; samplesPerSecond = 11481.3
dengamma value 1.027124
dengamma value 0.962268
dengamma value 0.990465
dengamma value 1.015494
dengamma value 1.026605
dengamma value 1.124163
dengamma value 1.020897
dengamma value 1.047579
dengamma value 0.993904
dengamma value 1.082866
dengamma value 1.014808
dengamma value 1.021442
dengamma value 1.090179
dengamma value 1.012838
dengamma value 1.028912
dengamma value 1.073258
dengamma value 1.038047
dengamma value 1.054584
dengamma value 1.009320
dengamma value 0.971175
dengamma value 1.002060
dengamma value 1.031018
dengamma value 1.050005
dengamma value 1.085809
dengamma value 0.998567
dengamma value 0.914428
dengamma value 1.071697
dengamma value 1.042771
dengamma value 1.028874
dengamma value 1.029837
08/04/2016 13:54:46:  Epoch[ 3 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.07934341 * 8500; err = 0.34294118 * 8500; time = 0.6719s; samplesPerSecond = 12650.9
08/04/2016 13:54:46: Finished Epoch[ 3 of 3]: [Training] ce = 0.08284338 * 82066; err = 0.32405625 * 82066; totalSamplesSeen = 246618; learningRatePerSample = 2e-06; epochTime=6.82569s
08/04/2016 13:54:46: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence'
08/04/2016 13:54:46: CNTKCommandTrainEnd: sequenceTrain

08/04/2016 13:54:46: Action "train" complete.

08/04/2016 13:54:46: __COMPLETED__