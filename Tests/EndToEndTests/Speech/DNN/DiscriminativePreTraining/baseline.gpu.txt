CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.cntk currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu DeviceId=0 timestamping=true
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
08/04/2016 13:52:12: -------------------------------------------------------------------
08/04/2016 13:52:12: Build info: 

08/04/2016 13:52:12: 		Built time: Aug  4 2016 13:05:36
08/04/2016 13:52:12: 		Last modified date: Thu Aug  4 12:33:33 2016
08/04/2016 13:52:12: 		Build type: release
08/04/2016 13:52:12: 		Build target: GPU
08/04/2016 13:52:12: 		With 1bit-SGD: no
08/04/2016 13:52:12: 		Math lib: mkl
08/04/2016 13:52:12: 		CUDA_PATH: /usr/local/cuda-7.5
08/04/2016 13:52:12: 		CUB_PATH: /usr/local/cub-1.4.1
08/04/2016 13:52:12: 		CUDNN_PATH: /usr/local/cudnn-4.0
08/04/2016 13:52:12: 		Build Branch: HEAD
08/04/2016 13:52:12: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
08/04/2016 13:52:12: 		Built by philly on 643085f7f8c2
08/04/2016 13:52:12: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
08/04/2016 13:52:12: -------------------------------------------------------------------
08/04/2016 13:52:13: -------------------------------------------------------------------
08/04/2016 13:52:13: GPU info:

08/04/2016 13:52:13: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/04/2016 13:52:13: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/04/2016 13:52:13: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/04/2016 13:52:13: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
08/04/2016 13:52:13: -------------------------------------------------------------------

08/04/2016 13:52:13: Running on localhost at 2016/08/04 13:52:13
08/04/2016 13:52:13: Command line: 
/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/cntk_dpt.cntk  currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu  DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining  OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu  DeviceId=0  timestamping=true



08/04/2016 13:52:13: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
08/04/2016 13:52:13: precision = "float"
deviceId = $DeviceId$
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "$ConfigDir$/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "$RunDir$/models/Pre1/cntkSpeech"
    newModel  = "$RunDir$/models/Pre2/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "$RunDir$/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "$RunDir$/models/Pre2/cntkSpeech"
    newModel  = "$RunDir$/models/cntkSpeech.0"
    editPath  = "$ConfigDir$/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "$RunDir$/models/cntkSpeech"
    deviceId = $DeviceId$
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "$ConfigDir$/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "$DataDir$/glob_0000.scp"
    ]
    labels = [
        mlfFile = "$DataDir$/glob_0000.mlf"
        labelMappingFile = "$DataDir$/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

08/04/2016 13:52:13: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<

08/04/2016 13:52:13: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
08/04/2016 13:52:13: precision = "float"
deviceId = 0
command = dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
ndlMacros = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/macros.txt"
globalMeanPath   = "GlobalStats/mean.363"
globalInvStdPath = "GlobalStats/var.363"
globalPriorPath  = "GlobalStats/prior.132"
traceLevel = 1
SGD = [
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]
dptPre1 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer2 = [    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
dptPre2 = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]
addLayer3 = [    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]
speechTrain = [
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]
reader = [
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]
currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu
DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu
DeviceId=0
timestamping=true

08/04/2016 13:52:13: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<

08/04/2016 13:52:13: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
configparameters: cntk_dpt.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:addLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/add_layer.mel"
]

configparameters: cntk_dpt.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain
configparameters: cntk_dpt.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining
configparameters: cntk_dpt.cntk:currentDirectory=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:DataDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data
configparameters: cntk_dpt.cntk:deviceId=0
configparameters: cntk_dpt.cntk:dptPre1=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:dptPre2=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_dpt.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_dpt.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_dpt.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_dpt.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/macros.txt
configparameters: cntk_dpt.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:precision=float
configparameters: cntk_dpt.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp"
    ]
    labels = [
        mlfFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf"
        labelMappingFile = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_dpt.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu
configparameters: cntk_dpt.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_dpt.cntk:speechTrain=[
    action = "train"
    modelPath = "/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech"
    deviceId = 0
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/DNN/DiscriminativePreTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_dpt.cntk:timestamping=true
configparameters: cntk_dpt.cntk:traceLevel=1
08/04/2016 13:52:13: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
08/04/2016 13:52:13: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain
08/04/2016 13:52:13: Precision = "float"
08/04/2016 13:52:13: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech
08/04/2016 13:52:13: CNTKCommandTrainInfo: dptPre1 : 2
08/04/2016 13:52:13: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech
08/04/2016 13:52:13: CNTKCommandTrainInfo: dptPre2 : 2
08/04/2016 13:52:13: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech
08/04/2016 13:52:13: CNTKCommandTrainInfo: speechTrain : 4
08/04/2016 13:52:13: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 8

08/04/2016 13:52:13: ##############################################################################
08/04/2016 13:52:13: #                                                                            #
08/04/2016 13:52:13: # Action "train"                                                             #
08/04/2016 13:52:13: #                                                                            #
08/04/2016 13:52:13: ##############################################################################

08/04/2016 13:52:13: CNTKCommandTrainBegin: dptPre1
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 13:52:13: Creating virgin network.
SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *], [363 x 1], [363 x 1] -> [363 x *]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *] -> [512 x *]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *], [512 x 1] -> [512 x 1 x *]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *] -> [512 x 1 x *]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *] -> [132 x 1 x *]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *], [132 x 1 x *] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *], [132 x 1] -> [132 x 1 x *]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 13:52:13: Created model with 19 nodes on GPU 0.

08/04/2016 13:52:13: Training criterion node(s):
08/04/2016 13:52:13: 	ce = CrossEntropyWithSoftmax

08/04/2016 13:52:13: Evaluation criterion node(s):

08/04/2016 13:52:13: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *]] [features Gradient[363 x *]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *]] }
0x7fcb5e825198: {[featNorm Value[363 x *]] }
0x7fcb5e825258: {[logPrior Value[132 x 1]] }
0x7fcb5e826b68: {[HL1.t Value[512 x *]] }
0x7fcb5e826f18: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *]] }
0x7fcb5e827078: {[HL1.t Gradient[512 x *]] [HL1.y Value[512 x 1 x *]] }
0x7fcb5e8271d8: {[HL1.z Gradient[512 x 1 x *]] [OL.t Value[132 x 1 x *]] }
0x7fcb5e827398: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *]] }
0x7fcb5e827e18: {[ce Gradient[1]] }
0x7fcb5e827fd8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *]] [OL.z Gradient[132 x 1 x *]] }
0x7fcb5e828198: {[OL.t Gradient[132 x 1 x *]] }
0x7fcb5e828358: {[OL.b Gradient[132 x 1]] }
0x7fcb60e3fb98: {[labels Value[132 x *]] }
0x7fcb60e40a98: {[globalMean Value[363 x 1]] }
0x7fcb60e41108: {[globalInvStd Value[363 x 1]] }
0x7fcb60e41e48: {[globalPrior Value[132 x 1]] }
0x7fcb60e42828: {[HL1.W Value[512 x 363]] }
0x7fcb60e43d28: {[HL1.b Value[512 x 1]] }
0x7fcb60e44a08: {[OL.W Value[132 x 512]] }
0x7fcb60e45828: {[OL.b Value[132 x 1]] }
0x7fcb64c07da8: {[features Value[363 x *]] }
0x7fcb64c090c8: {[err Value[1]] }
0x7fcb64c09288: {[scaledLogLikelihood Value[132 x 1 x *]] }
0x7fcb64c09448: {[ce Value[1]] }

08/04/2016 13:52:13: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 13:52:13: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 13:52:14: Starting minibatch loop.
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.77997551 * 2560; err = 0.81484375 * 2560; time = 0.1175s; samplesPerSecond = 21784.1
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.92382545 * 2560; err = 0.71328125 * 2560; time = 0.0086s; samplesPerSecond = 298229.3
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55630035 * 2560; err = 0.65859375 * 2560; time = 0.0086s; samplesPerSecond = 297951.6
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.30158844 * 2560; err = 0.61523438 * 2560; time = 0.0086s; samplesPerSecond = 296090.7
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 2.04897461 * 2560; err = 0.56328125 * 2560; time = 0.0085s; samplesPerSecond = 299730.7
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.85978241 * 2560; err = 0.52070313 * 2560; time = 0.0085s; samplesPerSecond = 300011.7
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.76588593 * 2560; err = 0.50312500 * 2560; time = 0.0085s; samplesPerSecond = 301389.2
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.74416962 * 2560; err = 0.49375000 * 2560; time = 0.0085s; samplesPerSecond = 301637.8
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.64636383 * 2560; err = 0.47539063 * 2560; time = 0.0085s; samplesPerSecond = 301318.3
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.60569763 * 2560; err = 0.46171875 * 2560; time = 0.0085s; samplesPerSecond = 302278.9
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.51603851 * 2560; err = 0.44648437 * 2560; time = 0.0084s; samplesPerSecond = 303317.5
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.51614990 * 2560; err = 0.44765625 * 2560; time = 0.0085s; samplesPerSecond = 301566.7
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.47455750 * 2560; err = 0.43398437 * 2560; time = 0.0085s; samplesPerSecond = 301708.9
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.46740112 * 2560; err = 0.42929688 * 2560; time = 0.0084s; samplesPerSecond = 304074.1
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.41627502 * 2560; err = 0.42734375 * 2560; time = 0.0085s; samplesPerSecond = 301993.6
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.42307129 * 2560; err = 0.41015625 * 2560; time = 0.0084s; samplesPerSecond = 303497.3
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.38768005 * 2560; err = 0.40781250 * 2560; time = 0.0089s; samplesPerSecond = 286097.5
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.38003235 * 2560; err = 0.40039062 * 2560; time = 0.0082s; samplesPerSecond = 313763.9
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33673401 * 2560; err = 0.39921875 * 2560; time = 0.0081s; samplesPerSecond = 316753.3
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.33659973 * 2560; err = 0.40117188 * 2560; time = 0.0081s; samplesPerSecond = 317736.1
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.33715210 * 2560; err = 0.40507813 * 2560; time = 0.0081s; samplesPerSecond = 316518.3
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.36712036 * 2560; err = 0.41406250 * 2560; time = 0.0080s; samplesPerSecond = 319920.0
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.36091919 * 2560; err = 0.42304687 * 2560; time = 0.0078s; samplesPerSecond = 329896.9
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.27734070 * 2560; err = 0.38320312 * 2560; time = 0.0079s; samplesPerSecond = 325658.3
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.27654419 * 2560; err = 0.38593750 * 2560; time = 0.0080s; samplesPerSecond = 321891.1
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27127075 * 2560; err = 0.38906250 * 2560; time = 0.0078s; samplesPerSecond = 328457.8
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.27654114 * 2560; err = 0.38671875 * 2560; time = 0.0079s; samplesPerSecond = 325451.3
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.29538574 * 2560; err = 0.40000000 * 2560; time = 0.0078s; samplesPerSecond = 328922.0
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.23519592 * 2560; err = 0.37226562 * 2560; time = 0.0079s; samplesPerSecond = 325741.2
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.27269897 * 2560; err = 0.37734375 * 2560; time = 0.0078s; samplesPerSecond = 327659.0
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.29835205 * 2560; err = 0.38554688 * 2560; time = 0.0078s; samplesPerSecond = 327240.2
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.27160950 * 2560; err = 0.38125000 * 2560; time = 0.0079s; samplesPerSecond = 325120.7
08/04/2016 13:52:14: Finished Epoch[ 1 of 2]: [Training] ce = 1.62585106 * 81920; err = 0.46021729 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.455266s
08/04/2016 13:52:14: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech.1'

08/04/2016 13:52:14: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 13:52:14: Starting minibatch loop.
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.21941137 * 2560; err = 0.38007812 * 2560; time = 0.0090s; samplesPerSecond = 285937.7
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.16789980 * 2560; err = 0.35898438 * 2560; time = 0.0078s; samplesPerSecond = 326822.4
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.22954521 * 2560; err = 0.37929687 * 2560; time = 0.0086s; samplesPerSecond = 296262.0
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.23093147 * 2560; err = 0.37421875 * 2560; time = 0.0078s; samplesPerSecond = 329599.6
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.23917122 * 2560; err = 0.36835937 * 2560; time = 0.0078s; samplesPerSecond = 326572.3
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.12390594 * 2560; err = 0.34140625 * 2560; time = 0.0077s; samplesPerSecond = 331477.4
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.17119217 * 2560; err = 0.35273437 * 2560; time = 0.0078s; samplesPerSecond = 329726.9
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.16904068 * 2560; err = 0.34492187 * 2560; time = 0.0077s; samplesPerSecond = 331520.3
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.14490128 * 2560; err = 0.34687500 * 2560; time = 0.0078s; samplesPerSecond = 326572.3
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.23116379 * 2560; err = 0.36484375 * 2560; time = 0.0078s; samplesPerSecond = 326239.3
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.18010635 * 2560; err = 0.36132812 * 2560; time = 0.0079s; samplesPerSecond = 324625.9
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.16575470 * 2560; err = 0.34765625 * 2560; time = 0.0079s; samplesPerSecond = 322255.8
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.15317993 * 2560; err = 0.36484375 * 2560; time = 0.0079s; samplesPerSecond = 323559.2
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.15339508 * 2560; err = 0.35664062 * 2560; time = 0.0080s; samplesPerSecond = 321769.7
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.21201782 * 2560; err = 0.35976562 * 2560; time = 0.0080s; samplesPerSecond = 320080.0
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.11846924 * 2560; err = 0.33750000 * 2560; time = 0.0078s; samplesPerSecond = 327826.9
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.15517273 * 2560; err = 0.35703125 * 2560; time = 0.0078s; samplesPerSecond = 329896.9
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.16442261 * 2560; err = 0.34648438 * 2560; time = 0.0077s; samplesPerSecond = 333507.0
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.10736237 * 2560; err = 0.34570312 * 2560; time = 0.0077s; samplesPerSecond = 331177.2
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.13406830 * 2560; err = 0.36210938 * 2560; time = 0.0078s; samplesPerSecond = 329642.0
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.09407654 * 2560; err = 0.34687500 * 2560; time = 0.0077s; samplesPerSecond = 331134.4
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.16549683 * 2560; err = 0.35195312 * 2560; time = 0.0078s; samplesPerSecond = 329218.1
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12700806 * 2560; err = 0.34687500 * 2560; time = 0.0077s; samplesPerSecond = 332251.8
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.04887695 * 2560; err = 0.32812500 * 2560; time = 0.0078s; samplesPerSecond = 328584.3
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.16085510 * 2560; err = 0.34843750 * 2560; time = 0.0077s; samplesPerSecond = 331434.5
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.17593079 * 2560; err = 0.34453125 * 2560; time = 0.0077s; samplesPerSecond = 330749.4
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.10052795 * 2560; err = 0.34570312 * 2560; time = 0.0078s; samplesPerSecond = 330024.5
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.09912109 * 2560; err = 0.33554688 * 2560; time = 0.0077s; samplesPerSecond = 331262.9
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.12586365 * 2560; err = 0.33828125 * 2560; time = 0.0078s; samplesPerSecond = 327198.4
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.10348206 * 2560; err = 0.34257813 * 2560; time = 0.0078s; samplesPerSecond = 329684.5
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.09067688 * 2560; err = 0.33242187 * 2560; time = 0.0078s; samplesPerSecond = 329769.4
08/04/2016 13:52:14:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.11611328 * 2560; err = 0.34609375 * 2560; time = 0.0078s; samplesPerSecond = 329896.9
08/04/2016 13:52:14: Finished Epoch[ 2 of 2]: [Training] ce = 1.15247316 * 81920; err = 0.35181885 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.254638s
08/04/2016 13:52:14: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre1/cntkSpeech'
08/04/2016 13:52:14: CNTKCommandTrainEnd: dptPre1

08/04/2016 13:52:14: Action "train" complete.


08/04/2016 13:52:14: ##############################################################################
08/04/2016 13:52:14: #                                                                            #
08/04/2016 13:52:14: # Action "edit"                                                              #
08/04/2016 13:52:14: #                                                                            #
08/04/2016 13:52:14: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 19 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL1.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 10 nodes to process in pass 2.


Validating network, final pass.



10 out of 19 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *1]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *1]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *1], [363 x 1], [363 x 1] -> [363 x *1]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *1] -> [512 x *1]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *1], [512 x 1] -> [512 x 1 x *1]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *1] -> [512 x 1 x *1]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *1] -> [132 x 1 x *1]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *1], [132 x 1 x *1] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *1], [132 x 1] -> [132 x 1 x *1]

Validating network. 12 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 13:52:14: Action "edit" complete.


08/04/2016 13:52:14: ##############################################################################
08/04/2016 13:52:14: #                                                                            #
08/04/2016 13:52:14: # Action "train"                                                             #
08/04/2016 13:52:14: #                                                                            #
08/04/2016 13:52:14: ##############################################################################

08/04/2016 13:52:14: CNTKCommandTrainBegin: dptPre2
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 13:52:14: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *3]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *3]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *3], [363 x 1], [363 x 1] -> [363 x *3]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *3] -> [512 x *3]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *3], [512 x 1] -> [512 x 1 x *3]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *3] -> [512 x 1 x *3]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *3] -> [132 x 1 x *3]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *3], [132 x 1 x *3] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *3], [132 x 1] -> [132 x 1 x *3]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 13:52:14: Loaded model with 24 nodes on GPU 0.

08/04/2016 13:52:14: Training criterion node(s):
08/04/2016 13:52:14: 	ce = CrossEntropyWithSoftmax

08/04/2016 13:52:14: Evaluation criterion node(s):

08/04/2016 13:52:14: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *3]] [features Gradient[363 x *3]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *3]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *3]] }
0x7fcb5623d1b8: {[globalInvStd Value[363 x 1]] }
0x7fcb56298508: {[OL.b Value[132 x 1]] }
0x7fcb562bc428: {[features Value[363 x *3]] }
0x7fcb562ce5a8: {[OL.W Value[132 x 512]] }
0x7fcb562d81a8: {[globalMean Value[363 x 1]] }
0x7fcb562dda38: {[HL2.t Gradient[512 x 1 x *3]] [HL2.y Value[512 x 1 x *3]] }
0x7fcb562ddbf8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *3]] [HL2.z Gradient[512 x 1 x *3]] [OL.t Value[132 x 1 x *3]] }
0x7fcb562dddb8: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *3]] }
0x7fcb562de6f8: {[OL.b Gradient[132 x 1]] }
0x7fcb562e2608: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *3]] }
0x7fcb562e27c8: {[HL1.t Gradient[512 x *3]] [HL1.y Value[512 x 1 x *3]] }
0x7fcb562e2988: {[HL1.z Gradient[512 x 1 x *3]] [HL2.t Value[512 x 1 x *3]] }
0x7fcb562e2b48: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *3]] }
0x7fcb562e3448: {[scaledLogLikelihood Value[132 x 1 x *3]] }
0x7fcb562e7e78: {[HL1.W Value[512 x 363]] }
0x7fcb5e83f018: {[err Value[1]] }
0x7fcb64c26ca8: {[HL1.b Value[512 x 1]] }
0x7fcb64c2b438: {[globalPrior Value[132 x 1]] }
0x7fcb64c51b28: {[labels Value[132 x *3]] }
0x7fcb64c84708: {[ce Value[1]] }
0x7fcb64c84b98: {[featNorm Value[363 x *3]] }
0x7fcb64c84c48: {[logPrior Value[132 x 1]] }
0x7fcb64c8ecd8: {[HL1.t Value[512 x *3]] }
0x7fcb64c92b18: {[HL2.W Value[512 x 512]] }
0x7fcb64cb07a8: {[HL2.b Value[512 x 1]] }
0x7fcb65e9b358: {[ce Gradient[1]] }
0x7fcb65e9b518: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *3]] [OL.z Gradient[132 x 1 x *3]] }
0x7fcb65e9b6d8: {[OL.t Gradient[132 x 1 x *3]] }

08/04/2016 13:52:14: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 13:52:14: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 13:52:14: Starting minibatch loop.
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.12%]: ce = 3.92264137 * 2560; err = 0.82890625 * 2560; time = 0.0149s; samplesPerSecond = 171443.9
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.62011375 * 2560; err = 0.66601562 * 2560; time = 0.0122s; samplesPerSecond = 210042.7
08/04/2016 13:52:14:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.13756485 * 2560; err = 0.58476562 * 2560; time = 0.0139s; samplesPerSecond = 183987.4
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.87948074 * 2560; err = 0.54023438 * 2560; time = 0.0122s; samplesPerSecond = 209372.7
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.72052612 * 2560; err = 0.48476562 * 2560; time = 0.0124s; samplesPerSecond = 206952.3
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.59273834 * 2560; err = 0.45820312 * 2560; time = 0.0123s; samplesPerSecond = 208282.5
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.48152161 * 2560; err = 0.43281250 * 2560; time = 0.0123s; samplesPerSecond = 208622.0
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.46213989 * 2560; err = 0.41953125 * 2560; time = 0.0122s; samplesPerSecond = 210266.9
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.40166626 * 2560; err = 0.41953125 * 2560; time = 0.0122s; samplesPerSecond = 209904.9
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.40336304 * 2560; err = 0.42382812 * 2560; time = 0.0122s; samplesPerSecond = 209853.3
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.34085541 * 2560; err = 0.40429688 * 2560; time = 0.0122s; samplesPerSecond = 210111.6
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.34828033 * 2560; err = 0.40390625 * 2560; time = 0.0122s; samplesPerSecond = 210491.7
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.32214508 * 2560; err = 0.38867188 * 2560; time = 0.0122s; samplesPerSecond = 210197.9
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.27623749 * 2560; err = 0.37304688 * 2560; time = 0.0122s; samplesPerSecond = 210146.1
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.27629395 * 2560; err = 0.38906250 * 2560; time = 0.0122s; samplesPerSecond = 209372.7
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.27472229 * 2560; err = 0.36562500 * 2560; time = 0.0122s; samplesPerSecond = 210077.1
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.27291870 * 2560; err = 0.37734375 * 2560; time = 0.0121s; samplesPerSecond = 210855.8
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.29111328 * 2560; err = 0.38164063 * 2560; time = 0.0121s; samplesPerSecond = 210734.3
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.24976807 * 2560; err = 0.37851563 * 2560; time = 0.0122s; samplesPerSecond = 210370.6
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.22633057 * 2560; err = 0.36015625 * 2560; time = 0.0122s; samplesPerSecond = 210595.6
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.23699646 * 2560; err = 0.38164063 * 2560; time = 0.0122s; samplesPerSecond = 210474.4
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.26840210 * 2560; err = 0.38125000 * 2560; time = 0.0122s; samplesPerSecond = 209116.2
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.24623718 * 2560; err = 0.38710937 * 2560; time = 0.0122s; samplesPerSecond = 209991.0
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.18269958 * 2560; err = 0.36210938 * 2560; time = 0.0122s; samplesPerSecond = 209304.2
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.18463745 * 2560; err = 0.35429688 * 2560; time = 0.0123s; samplesPerSecond = 208690.0
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.22700806 * 2560; err = 0.38125000 * 2560; time = 0.0122s; samplesPerSecond = 210370.6
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.15788269 * 2560; err = 0.34218750 * 2560; time = 0.0121s; samplesPerSecond = 211134.0
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.20556641 * 2560; err = 0.38398437 * 2560; time = 0.0121s; samplesPerSecond = 211325.7
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.17214050 * 2560; err = 0.35195312 * 2560; time = 0.0121s; samplesPerSecond = 210907.9
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.21809082 * 2560; err = 0.35898438 * 2560; time = 0.0121s; samplesPerSecond = 211535.3
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.22475891 * 2560; err = 0.35703125 * 2560; time = 0.0121s; samplesPerSecond = 210751.6
08/04/2016 13:52:15:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.20566711 * 2560; err = 0.35859375 * 2560; time = 0.0122s; samplesPerSecond = 210025.4
08/04/2016 13:52:15: Finished Epoch[ 1 of 2]: [Training] ce = 1.46970339 * 81920; err = 0.42128906 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.476208s
08/04/2016 13:52:15: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech.1'

08/04/2016 13:52:15: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 13:52:15: Starting minibatch loop.
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.12%]: ce = 1.15010700 * 2560; err = 0.36132812 * 2560; time = 0.0132s; samplesPerSecond = 193222.1
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.11401196 * 2560; err = 0.34570312 * 2560; time = 0.0122s; samplesPerSecond = 210682.2
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.16447182 * 2560; err = 0.35664062 * 2560; time = 0.0122s; samplesPerSecond = 210612.9
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.15266151 * 2560; err = 0.34218750 * 2560; time = 0.0121s; samplesPerSecond = 211029.6
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.62%]: ce = 1.20112534 * 2560; err = 0.35781250 * 2560; time = 0.0121s; samplesPerSecond = 210751.6
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.06493530 * 2560; err = 0.31796875 * 2560; time = 0.0122s; samplesPerSecond = 209784.5
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.15083237 * 2560; err = 0.34453125 * 2560; time = 0.0122s; samplesPerSecond = 210439.8
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.11611252 * 2560; err = 0.33320312 * 2560; time = 0.0123s; samplesPerSecond = 208147.0
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.12%]: ce = 1.11194077 * 2560; err = 0.33828125 * 2560; time = 0.0122s; samplesPerSecond = 209922.1
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.15132294 * 2560; err = 0.34609375 * 2560; time = 0.0121s; samplesPerSecond = 211186.3
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.11250153 * 2560; err = 0.33125000 * 2560; time = 0.0121s; samplesPerSecond = 210716.9
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.10859451 * 2560; err = 0.33164063 * 2560; time = 0.0122s; samplesPerSecond = 210578.3
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.62%]: ce = 1.10353088 * 2560; err = 0.34648438 * 2560; time = 0.0121s; samplesPerSecond = 210821.0
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.09240570 * 2560; err = 0.33671875 * 2560; time = 0.0121s; samplesPerSecond = 211308.3
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.17526245 * 2560; err = 0.34921875 * 2560; time = 0.0121s; samplesPerSecond = 210769.0
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.07992859 * 2560; err = 0.32539062 * 2560; time = 0.0121s; samplesPerSecond = 210716.9
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.12%]: ce = 1.09457855 * 2560; err = 0.33554688 * 2560; time = 0.0122s; samplesPerSecond = 210543.6
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11868744 * 2560; err = 0.34414062 * 2560; time = 0.0122s; samplesPerSecond = 209750.1
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.07582703 * 2560; err = 0.33671875 * 2560; time = 0.0121s; samplesPerSecond = 210786.3
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08641357 * 2560; err = 0.34726563 * 2560; time = 0.0122s; samplesPerSecond = 210422.5
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.62%]: ce = 1.05171814 * 2560; err = 0.33437500 * 2560; time = 0.0122s; samplesPerSecond = 210439.8
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.14046021 * 2560; err = 0.34414062 * 2560; time = 0.0122s; samplesPerSecond = 210682.2
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.10008392 * 2560; err = 0.34218750 * 2560; time = 0.0121s; samplesPerSecond = 211151.4
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.04127808 * 2560; err = 0.31953125 * 2560; time = 0.0122s; samplesPerSecond = 209647.0
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.12%]: ce = 1.13776855 * 2560; err = 0.34140625 * 2560; time = 0.0122s; samplesPerSecond = 209252.9
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.13193970 * 2560; err = 0.33789062 * 2560; time = 0.0123s; samplesPerSecond = 208741.0
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.07574158 * 2560; err = 0.33125000 * 2560; time = 0.0123s; samplesPerSecond = 208826.2
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.07451172 * 2560; err = 0.33710937 * 2560; time = 0.0123s; samplesPerSecond = 208945.5
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.62%]: ce = 1.09063721 * 2560; err = 0.32851562 * 2560; time = 0.0122s; samplesPerSecond = 210180.6
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.05981750 * 2560; err = 0.32617188 * 2560; time = 0.0122s; samplesPerSecond = 210439.8
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.06223145 * 2560; err = 0.32226562 * 2560; time = 0.0121s; samplesPerSecond = 210821.0
08/04/2016 13:52:15:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04121399 * 2560; err = 0.32851562 * 2560; time = 0.0122s; samplesPerSecond = 209664.2
08/04/2016 13:52:15: Finished Epoch[ 2 of 2]: [Training] ce = 1.10727043 * 81920; err = 0.33817139 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.394173s
08/04/2016 13:52:15: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/Pre2/cntkSpeech'
08/04/2016 13:52:15: CNTKCommandTrainEnd: dptPre2

08/04/2016 13:52:15: Action "train" complete.


08/04/2016 13:52:15: ##############################################################################
08/04/2016 13:52:15: #                                                                            #
08/04/2016 13:52:15: # Action "edit"                                                              #
08/04/2016 13:52:15: #                                                                            #
08/04/2016 13:52:15: ##############################################################################


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 24 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL2.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 13 nodes to process in pass 2.


Validating network, final pass.



12 out of 24 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *4]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *4]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *4], [363 x 1], [363 x 1] -> [363 x *4]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *4] -> [512 x *4]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *4], [512 x 1] -> [512 x 1 x *4]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *4] -> [512 x 1 x *4]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *4] -> [132 x 1 x *4]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *4], [132 x 1 x *4] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *4], [132 x 1] -> [132 x 1 x *4]

Validating network. 15 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.


08/04/2016 13:52:15: Action "edit" complete.


08/04/2016 13:52:15: ##############################################################################
08/04/2016 13:52:15: #                                                                            #
08/04/2016 13:52:15: # Action "train"                                                             #
08/04/2016 13:52:15: #                                                                            #
08/04/2016 13:52:15: ##############################################################################

08/04/2016 13:52:15: CNTKCommandTrainBegin: speechTrain
NDLBuilder Using GPU 0
reading script file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.scp ... 948 entries
total 132 state names in state list /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/state.list
htkmlfreader: reading MLF file /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Speech/Data/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames

08/04/2016 13:52:15: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.0'.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	err = ErrorPrediction()
	scaledLogLikelihood = Minus()

Validating network. 29 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [132 x *6]
Validating --> OL.W = LearnableParameter() :  -> [132 x 512]
Validating --> HL3.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL2.W = LearnableParameter() :  -> [512 x 512]
Validating --> HL1.W = LearnableParameter() :  -> [512 x 363]
Validating --> features = InputValue() :  -> [363 x *6]
Validating --> globalMean = LearnableParameter() :  -> [363 x 1]
Validating --> globalInvStd = LearnableParameter() :  -> [363 x 1]
Validating --> featNorm = PerDimMeanVarNormalization (features, globalMean, globalInvStd) : [363 x *6], [363 x 1], [363 x 1] -> [363 x *6]
Validating --> HL1.t = Times (HL1.W, featNorm) : [512 x 363], [363 x *6] -> [512 x *6]
Validating --> HL1.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL1.z = Plus (HL1.t, HL1.b) : [512 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL1.y = Sigmoid (HL1.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.t = Times (HL2.W, HL1.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL2.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL2.z = Plus (HL2.t, HL2.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL2.y = Sigmoid (HL2.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.t = Times (HL3.W, HL2.y) : [512 x 512], [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> HL3.b = LearnableParameter() :  -> [512 x 1]
Validating --> HL3.z = Plus (HL3.t, HL3.b) : [512 x 1 x *6], [512 x 1] -> [512 x 1 x *6]
Validating --> HL3.y = Sigmoid (HL3.z) : [512 x 1 x *6] -> [512 x 1 x *6]
Validating --> OL.t = Times (OL.W, HL3.y) : [132 x 512], [512 x 1 x *6] -> [132 x 1 x *6]
Validating --> OL.b = LearnableParameter() :  -> [132 x 1]
Validating --> OL.z = Plus (OL.t, OL.b) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]
Validating --> ce = CrossEntropyWithSoftmax (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> err = ErrorPrediction (labels, OL.z) : [132 x *6], [132 x 1 x *6] -> [1]
Validating --> globalPrior = LearnableParameter() :  -> [132 x 1]
Validating --> logPrior = Log (globalPrior) : [132 x 1] -> [132 x 1]
Validating --> scaledLogLikelihood = Minus (OL.z, logPrior) : [132 x 1 x *6], [132 x 1] -> [132 x 1 x *6]

Validating network. 16 nodes to process in pass 2.


Validating network, final pass.



14 out of 29 nodes do not share the minibatch layout with the input data.

Post-processing network complete.

08/04/2016 13:52:15: Loaded model with 29 nodes on GPU 0.

08/04/2016 13:52:15: Training criterion node(s):
08/04/2016 13:52:15: 	ce = CrossEntropyWithSoftmax

08/04/2016 13:52:15: Evaluation criterion node(s):

08/04/2016 13:52:15: 	err = ErrorPrediction


Allocating matrices for forward and/or backward propagation.

Memory Sharing Structure:

(nil): {[err Gradient[1]] [featNorm Gradient[363 x *6]] [features Gradient[363 x *6]] [globalInvStd Gradient[363 x 1]] [globalMean Gradient[363 x 1]] [globalPrior Gradient[132 x 1]] [labels Gradient[132 x *6]] [logPrior Gradient[132 x 1]] [scaledLogLikelihood Gradient[132 x 1 x *6]] }
0x7fcb56280fd8: {[HL3.b Value[512 x 1]] }
0x7fcb56286448: {[HL3.b Gradient[512 x 1]] [HL3.y Gradient[512 x 1 x *6]] [OL.z Gradient[132 x 1 x *6]] }
0x7fcb56286608: {[OL.t Gradient[132 x 1 x *6]] }
0x7fcb562867c8: {[OL.b Gradient[132 x 1]] }
0x7fcb56295208: {[globalPrior Value[132 x 1]] }
0x7fcb562a7068: {[globalMean Value[363 x 1]] }
0x7fcb562b4358: {[featNorm Value[363 x *6]] }
0x7fcb562b79c8: {[OL.W Value[132 x 512]] }
0x7fcb562b9008: {[err Value[1]] }
0x7fcb562e18e8: {[logPrior Value[132 x 1]] }
0x7fcb60e428e8: {[scaledLogLikelihood Value[132 x 1 x *6]] }
0x7fcb60e42aa8: {[ce Value[1]] }
0x7fcb60e42e78: {[HL1.t Value[512 x *6]] }
0x7fcb64c50638: {[globalInvStd Value[363 x 1]] }
0x7fcb64c50ad8: {[HL2.b Value[512 x 1]] }
0x7fcb64c55248: {[labels Value[132 x *6]] }
0x7fcb64c667e8: {[HL1.t Gradient[512 x *6]] [HL1.y Value[512 x 1 x *6]] }
0x7fcb64c669a8: {[HL1.z Gradient[512 x 1 x *6]] [HL2.t Value[512 x 1 x *6]] }
0x7fcb64c66b68: {[HL2.W Gradient[512 x 512]] [HL2.z Value[512 x 1 x *6]] }
0x7fcb64c66d28: {[HL2.t Gradient[512 x 1 x *6]] [HL2.y Value[512 x 1 x *6]] }
0x7fcb64c66ee8: {[HL1.b Gradient[512 x 1]] [HL1.y Gradient[512 x 1 x *6]] [HL2.z Gradient[512 x 1 x *6]] [HL3.t Value[512 x 1 x *6]] }
0x7fcb64c7caf8: {[HL1.W Gradient[512 x 363]] [HL1.z Value[512 x 1 x *6]] }
0x7fcb64c7d0c8: {[HL3.W Gradient[512 x 512]] [HL3.z Value[512 x 1 x *6]] }
0x7fcb64c7d288: {[HL3.t Gradient[512 x 1 x *6]] [HL3.y Value[512 x 1 x *6]] }
0x7fcb64c7d448: {[HL2.b Gradient[512 x 1]] [HL2.y Gradient[512 x 1 x *6]] [HL3.z Gradient[512 x 1 x *6]] [OL.t Value[132 x 1 x *6]] }
0x7fcb64c7d608: {[OL.W Gradient[132 x 512]] [OL.z Value[132 x 1 x *6]] }
0x7fcb64c88178: {[ce Gradient[1]] }
0x7fcb64c8d568: {[OL.b Value[132 x 1]] }
0x7fcb64c8fb68: {[HL1.b Value[512 x 1]] }
0x7fcb64ca6a08: {[HL1.W Value[512 x 363]] }
0x7fcb65e9b3d8: {[HL3.W Value[512 x 512]] }
0x7fcb65ebd128: {[features Value[363 x *6]] }
0x7fcb65eef708: {[HL2.W Value[512 x 512]] }

08/04/2016 13:52:15: No PreCompute nodes found, skipping PreCompute step.

08/04/2016 13:52:15: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

08/04/2016 13:52:16: Starting minibatch loop.
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.12%]: ce = 3.95473633 * 2560; err = 0.81445312 * 2560; time = 0.0195s; samplesPerSecond = 131248.4
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.51360703 * 2560; err = 0.62578125 * 2560; time = 0.0166s; samplesPerSecond = 154384.3
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 1.98941879 * 2560; err = 0.54570312 * 2560; time = 0.0167s; samplesPerSecond = 153716.8
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.74144974 * 2560; err = 0.49492188 * 2560; time = 0.0166s; samplesPerSecond = 153772.2
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.62%]: ce = 1.59180908 * 2560; err = 0.46054688 * 2560; time = 0.0166s; samplesPerSecond = 153799.9
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.46800308 * 2560; err = 0.43164062 * 2560; time = 0.0167s; samplesPerSecond = 153000.2
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.38572540 * 2560; err = 0.40312500 * 2560; time = 0.0166s; samplesPerSecond = 154281.9
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.36116180 * 2560; err = 0.39531250 * 2560; time = 0.0166s; samplesPerSecond = 154031.3
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.12%]: ce = 1.29876099 * 2560; err = 0.39218750 * 2560; time = 0.0165s; samplesPerSecond = 154701.5
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.31320801 * 2560; err = 0.39257812 * 2560; time = 0.0166s; samplesPerSecond = 154170.4
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.26076813 * 2560; err = 0.37382813 * 2560; time = 0.0166s; samplesPerSecond = 154319.1
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.26728516 * 2560; err = 0.37460938 * 2560; time = 0.0165s; samplesPerSecond = 154804.4
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.62%]: ce = 1.23206482 * 2560; err = 0.36406250 * 2560; time = 0.0166s; samplesPerSecond = 154561.4
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.19322510 * 2560; err = 0.35273437 * 2560; time = 0.0166s; samplesPerSecond = 154421.5
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.19844055 * 2560; err = 0.36445312 * 2560; time = 0.0166s; samplesPerSecond = 154682.8
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.20307465 * 2560; err = 0.34882812 * 2560; time = 0.0166s; samplesPerSecond = 154570.7
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.12%]: ce = 1.20918579 * 2560; err = 0.35976562 * 2560; time = 0.0166s; samplesPerSecond = 154031.3
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.22486877 * 2560; err = 0.35937500 * 2560; time = 0.0166s; samplesPerSecond = 153846.2
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.17906189 * 2560; err = 0.35937500 * 2560; time = 0.0166s; samplesPerSecond = 154133.3
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.16787720 * 2560; err = 0.35468750 * 2560; time = 0.0166s; samplesPerSecond = 154412.2
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.62%]: ce = 1.17252502 * 2560; err = 0.35351562 * 2560; time = 0.0165s; samplesPerSecond = 154748.2
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.21054993 * 2560; err = 0.36328125 * 2560; time = 0.0166s; samplesPerSecond = 154561.4
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.19859924 * 2560; err = 0.36328125 * 2560; time = 0.0166s; samplesPerSecond = 154375.0
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.12578735 * 2560; err = 0.34765625 * 2560; time = 0.0166s; samplesPerSecond = 154375.0
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.12%]: ce = 1.13427124 * 2560; err = 0.33710937 * 2560; time = 0.0165s; samplesPerSecond = 154944.9
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.18345642 * 2560; err = 0.36015625 * 2560; time = 0.0166s; samplesPerSecond = 154393.6
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.10770874 * 2560; err = 0.32812500 * 2560; time = 0.0165s; samplesPerSecond = 154757.6
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.15658264 * 2560; err = 0.36757812 * 2560; time = 0.0166s; samplesPerSecond = 154645.4
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.62%]: ce = 1.12143250 * 2560; err = 0.33398438 * 2560; time = 0.0167s; samplesPerSecond = 153477.2
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.17078552 * 2560; err = 0.34648438 * 2560; time = 0.0166s; samplesPerSecond = 154598.7
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.19542542 * 2560; err = 0.35195312 * 2560; time = 0.0166s; samplesPerSecond = 153772.2
08/04/2016 13:52:16:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.16595154 * 2560; err = 0.34843750 * 2560; time = 0.0166s; samplesPerSecond = 153947.9
08/04/2016 13:52:16: Finished Epoch[ 1 of 4]: [Training] ce = 1.39677525 * 81920; err = 0.39904785 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.61577s
08/04/2016 13:52:16: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.1'

08/04/2016 13:52:16: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

08/04/2016 13:52:16: Starting minibatch loop.
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.26056890 * 5120; err = 0.37558594 * 5120; time = 0.0288s; samplesPerSecond = 178056.0
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.55791731 * 5120; err = 0.41152344 * 5120; time = 0.0258s; samplesPerSecond = 198334.3
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.31589680 * 5120; err = 0.38750000 * 5120; time = 0.0258s; samplesPerSecond = 198372.7
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.14716721 * 5120; err = 0.34414062 * 5120; time = 0.0258s; samplesPerSecond = 198426.5
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.16320839 * 5120; err = 0.35917969 * 5120; time = 0.0258s; samplesPerSecond = 198104.1
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.11159515 * 5120; err = 0.33378906 * 5120; time = 0.0259s; samplesPerSecond = 197943.2
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.07575150 * 5120; err = 0.33632812 * 5120; time = 0.0259s; samplesPerSecond = 197431.8
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.11748962 * 5120; err = 0.33671875 * 5120; time = 0.0258s; samplesPerSecond = 198403.5
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09913940 * 5120; err = 0.33457031 * 5120; time = 0.0258s; samplesPerSecond = 198534.3
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.06679916 * 5120; err = 0.33437500 * 5120; time = 0.0259s; samplesPerSecond = 197813.2
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.10906219 * 5120; err = 0.34492187 * 5120; time = 0.0258s; samplesPerSecond = 198511.2
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.07883606 * 5120; err = 0.32675781 * 5120; time = 0.0258s; samplesPerSecond = 198119.4
08/04/2016 13:52:16:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.11491699 * 5120; err = 0.33691406 * 5120; time = 0.0259s; samplesPerSecond = 197721.6
08/04/2016 13:52:17:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.05659943 * 5120; err = 0.32968750 * 5120; time = 0.0258s; samplesPerSecond = 198234.5
08/04/2016 13:52:17:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.07409515 * 5120; err = 0.32773438 * 5120; time = 0.0259s; samplesPerSecond = 197652.9
08/04/2016 13:52:17:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.07450562 * 5120; err = 0.33515625 * 5120; time = 0.0259s; samplesPerSecond = 197561.4
08/04/2016 13:52:17: Finished Epoch[ 2 of 4]: [Training] ce = 1.15147181 * 81920; err = 0.34718018 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.419681s
08/04/2016 13:52:17: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.2'

08/04/2016 13:52:17: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

08/04/2016 13:52:17: Starting minibatch loop.
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.16433182 * 5120; err = 0.36503906 * 5120; time = 0.0266s; samplesPerSecond = 192589.8
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.08577080 * 5120; err = 0.33691406 * 5120; time = 0.0258s; samplesPerSecond = 198142.4
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.05754356 * 5120; err = 0.32714844 * 5120; time = 0.0258s; samplesPerSecond = 198165.4
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.08211365 * 5120; err = 0.33613281 * 5120; time = 0.0259s; samplesPerSecond = 197736.8
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.11165009 * 5120; err = 0.33808594 * 5120; time = 0.0259s; samplesPerSecond = 197553.7
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.09777298 * 5120; err = 0.33652344 * 5120; time = 0.0260s; samplesPerSecond = 196983.7
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.05753860 * 5120; err = 0.33125000 * 5120; time = 0.0259s; samplesPerSecond = 197996.8
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.03217468 * 5120; err = 0.31640625 * 5120; time = 0.0259s; samplesPerSecond = 197645.2
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.06591949 * 5120; err = 0.32871094 * 5120; time = 0.0259s; samplesPerSecond = 197813.2
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.04894638 * 5120; err = 0.32656250 * 5120; time = 0.0258s; samplesPerSecond = 198349.7
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.08949585 * 5120; err = 0.33710937 * 5120; time = 0.0258s; samplesPerSecond = 198157.8
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.10956802 * 5120; err = 0.35019531 * 5120; time = 0.0259s; samplesPerSecond = 197996.8
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.07778168 * 5120; err = 0.33769531 * 5120; time = 0.0259s; samplesPerSecond = 197386.2
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.02749634 * 5120; err = 0.32832031 * 5120; time = 0.0259s; samplesPerSecond = 197576.6
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05765533 * 5120; err = 0.32988281 * 5120; time = 0.0258s; samplesPerSecond = 198434.2
08/04/2016 13:52:17:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.02716217 * 5120; err = 0.31386719 * 5120; time = 0.0259s; samplesPerSecond = 197874.4
08/04/2016 13:52:17: Finished Epoch[ 3 of 4]: [Training] ce = 1.07455759 * 81920; err = 0.33374023 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.417682s
08/04/2016 13:52:17: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech.3'

08/04/2016 13:52:17: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

08/04/2016 13:52:17: Starting minibatch loop.
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.04238539 * 5120; err = 0.32265625 * 5120; time = 0.0267s; samplesPerSecond = 191940.0
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.00743415 * 4926; err = 0.30937881 * 4926; time = 0.0513s; samplesPerSecond = 96079.6
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01025753 * 5120; err = 0.31347656 * 5120; time = 0.0259s; samplesPerSecond = 197310.1
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.01178360 * 5120; err = 0.31679687 * 5120; time = 0.0259s; samplesPerSecond = 197736.8
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.12052345 * 5120; err = 0.35410156 * 5120; time = 0.0258s; samplesPerSecond = 198403.5
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.01440353 * 5120; err = 0.31699219 * 5120; time = 0.0258s; samplesPerSecond = 198565.1
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.97455025 * 5120; err = 0.30410156 * 5120; time = 0.0259s; samplesPerSecond = 197553.7
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.00521698 * 5120; err = 0.32460937 * 5120; time = 0.0259s; samplesPerSecond = 197736.8
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.97339859 * 5120; err = 0.30136719 * 5120; time = 0.0259s; samplesPerSecond = 197996.8
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.96836319 * 5120; err = 0.30292969 * 5120; time = 0.0259s; samplesPerSecond = 197614.7
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.00247421 * 5120; err = 0.31113281 * 5120; time = 0.0260s; samplesPerSecond = 197150.6
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.00154114 * 5120; err = 0.31406250 * 5120; time = 0.0260s; samplesPerSecond = 196998.8
08/04/2016 13:52:17:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 0.97331314 * 5120; err = 0.31054688 * 5120; time = 0.0259s; samplesPerSecond = 197775.0
08/04/2016 13:52:18:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97797546 * 5120; err = 0.30605469 * 5120; time = 0.0258s; samplesPerSecond = 198065.8
08/04/2016 13:52:18:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.99987946 * 5120; err = 0.30839844 * 5120; time = 0.0259s; samplesPerSecond = 197706.3
08/04/2016 13:52:18:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.98577881 * 5120; err = 0.30722656 * 5120; time = 0.0259s; samplesPerSecond = 198042.8
08/04/2016 13:52:18: Finished Epoch[ 4 of 4]: [Training] ce = 1.00435781 * 81920; err = 0.31411133 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.444948s
08/04/2016 13:52:18: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Speech/DNN_DiscriminativePreTraining@release_gpu/models/cntkSpeech'
08/04/2016 13:52:18: CNTKCommandTrainEnd: speechTrain

08/04/2016 13:52:18: Action "train" complete.

08/04/2016 13:52:18: __COMPLETED__