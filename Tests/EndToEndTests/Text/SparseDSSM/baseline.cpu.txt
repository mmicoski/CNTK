CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 24
    Total Memory: 264172964 kB
-------------------------------------------------------------------
=== Running mpiexec -n 4 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=6 stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
Changed current directory to /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
08/04/2016 13:57:09: Redirecting stderr to file /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr_train.logrank0
08/04/2016 13:57:10: Redirecting stderr to file /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr_train.logrank1
08/04/2016 13:57:10: Redirecting stderr to file /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr_train.logrank2
08/04/2016 13:57:11: Redirecting stderr to file /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 0: 08/04/2016 13:57:09: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 13:57:09: Build info: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:09: 		Built time: Aug  4 2016 13:05:36
MPI Rank 0: 08/04/2016 13:57:09: 		Last modified date: Thu Aug  4 12:33:33 2016
MPI Rank 0: 08/04/2016 13:57:09: 		Build type: release
MPI Rank 0: 08/04/2016 13:57:09: 		Build target: GPU
MPI Rank 0: 08/04/2016 13:57:09: 		With 1bit-SGD: no
MPI Rank 0: 08/04/2016 13:57:09: 		Math lib: mkl
MPI Rank 0: 08/04/2016 13:57:09: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 08/04/2016 13:57:09: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 08/04/2016 13:57:09: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 08/04/2016 13:57:09: 		Build Branch: HEAD
MPI Rank 0: 08/04/2016 13:57:09: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 0: 08/04/2016 13:57:09: 		Built by philly on 643085f7f8c2
MPI Rank 0: 08/04/2016 13:57:09: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 08/04/2016 13:57:09: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 13:57:11: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 13:57:11: GPU info:
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 08/04/2016 13:57:11: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 08/04/2016 13:57:11: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 08/04/2016 13:57:11: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 08/04/2016 13:57:11: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: Running on localhost at 2016/08/04 13:57:11
MPI Rank 0: 08/04/2016 13:57:11: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 13:57:11: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 13:57:11: modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 08/04/2016 13:57:11: Commands: train
MPI Rank 0: 08/04/2016 13:57:11: Precision = "float"
MPI Rank 0: 08/04/2016 13:57:11: Using 6 CPU threads.
MPI Rank 0: 08/04/2016 13:57:11: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 08/04/2016 13:57:11: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 08/04/2016 13:57:11: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: ##############################################################################
MPI Rank 0: 08/04/2016 13:57:11: #                                                                            #
MPI Rank 0: 08/04/2016 13:57:11: # Action "train"                                                             #
MPI Rank 0: 08/04/2016 13:57:11: #                                                                            #
MPI Rank 0: 08/04/2016 13:57:11: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:11: Creating virgin network.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:12: Created model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:12: Training criterion node(s):
MPI Rank 0: 08/04/2016 13:57:12: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 0x7f3bb310d1f8: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0x7f3bb310db28: {[Query Value[49292 x *]] }
MPI Rank 0: 0x7f3bb310e7f8: {[Keyword Value[49292 x *]] }
MPI Rank 0: 0x7f3bb310e958: {[ce Value[1]] }
MPI Rank 0: 0x7f3bb310ee78: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0x7f3bb310f2a8: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0x7f3bb310fd48: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0x7f3bb3161ca8: {[S Value[1 x 1]] }
MPI Rank 0: 0x7f3bb3162078: {[N Value[1 x 1]] }
MPI Rank 0: 0x7f3bb31624e8: {[G Value[1 x 1]] }
MPI Rank 0: 0x7f3bb31629f8: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 0x7f3bb3165b38: {[SIM Value[51 x *]] }
MPI Rank 0: 0x7f3bb3167058: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 0x7f3bb3167398: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 0x7f3bb3167b38: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 0x7f3bb3167c98: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 0x7f3bb3167e58: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 0x7f3bb3168018: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 0x7f3bb31681d8: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 0x7f3bb3168398: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 0x7f3bb3168c58: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 0x7f3bb3169198: {[ce Gradient[1]] }
MPI Rank 0: 0x7f3bb3169358: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 0x7f3bb3169518: {[SIM Gradient[51 x *]] }
MPI Rank 0: 0x7f3bb3169898: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 0x7f3bb3169c48: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0x7f3bb3169ce8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 08/04/2016 13:57:12: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:14: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:14: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:57:28:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 3.44191360 * 10240; time = 13.7106s; samplesPerSecond = 746.9
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:57:42:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.68707848 * 10240; time = 14.0897s; samplesPerSecond = 726.8
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:57:49: Finished Epoch[ 1 of 3]: [Training] ce = 2.91918920 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=34.3716s
MPI Rank 0: 08/04/2016 13:57:51: Final Results: Minibatch[1-26]: ce = 2.13785050 * 102399; perplexity = 8.48118773
MPI Rank 0: 08/04/2016 13:57:51: Finished Epoch[ 1 of 3]: [Validate] ce = 2.13785050 * 102399
MPI Rank 0: 08/04/2016 13:57:52: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:53: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:57:53: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:58:07:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.99931793 * 10240; time = 13.1983s; samplesPerSecond = 775.9
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:58:21:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89505386 * 10240; time = 13.9372s; samplesPerSecond = 734.7
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:58:28: Finished Epoch[ 2 of 3]: [Training] ce = 1.94379515 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=34.217s
MPI Rank 0: 08/04/2016 13:58:30: Final Results: Minibatch[1-26]: ce = 1.81478808 * 102399; perplexity = 6.13977488
MPI Rank 0: 08/04/2016 13:58:30: Finished Epoch[ 2 of 3]: [Validate] ce = 1.81478808 * 102399
MPI Rank 0: 08/04/2016 13:58:31: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:58:32: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:58:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.03 seconds
MPI Rank 0: 08/04/2016 13:58:45:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.75707302 * 10240; time = 13.2220s; samplesPerSecond = 774.5
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.03 seconds
MPI Rank 0: 08/04/2016 13:58:59:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.74772434 * 10240; time = 13.2979s; samplesPerSecond = 770.0
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:59:06: Finished Epoch[ 3 of 3]: [Training] ce = 1.75639887 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=33.3122s
MPI Rank 0: 08/04/2016 13:59:08: Final Results: Minibatch[1-26]: ce = 1.69809268 * 102399; perplexity = 5.46351680
MPI Rank 0: 08/04/2016 13:59:08: Finished Epoch[ 3 of 3]: [Validate] ce = 1.69809268 * 102399
MPI Rank 0: 08/04/2016 13:59:09: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 08/04/2016 13:59:10: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:10: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:10: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 08/04/2016 13:57:10: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 13:57:10: Build info: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:10: 		Built time: Aug  4 2016 13:05:36
MPI Rank 1: 08/04/2016 13:57:10: 		Last modified date: Thu Aug  4 12:33:33 2016
MPI Rank 1: 08/04/2016 13:57:10: 		Build type: release
MPI Rank 1: 08/04/2016 13:57:10: 		Build target: GPU
MPI Rank 1: 08/04/2016 13:57:10: 		With 1bit-SGD: no
MPI Rank 1: 08/04/2016 13:57:10: 		Math lib: mkl
MPI Rank 1: 08/04/2016 13:57:10: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 08/04/2016 13:57:10: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 08/04/2016 13:57:10: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 08/04/2016 13:57:10: 		Build Branch: HEAD
MPI Rank 1: 08/04/2016 13:57:10: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 1: 08/04/2016 13:57:10: 		Built by philly on 643085f7f8c2
MPI Rank 1: 08/04/2016 13:57:10: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 08/04/2016 13:57:10: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 13:57:11: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 13:57:11: GPU info:
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 08/04/2016 13:57:11: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 08/04/2016 13:57:11: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 08/04/2016 13:57:11: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 08/04/2016 13:57:11: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: Running on localhost at 2016/08/04 13:57:11
MPI Rank 1: 08/04/2016 13:57:11: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 13:57:11: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 13:57:11: modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 08/04/2016 13:57:11: Commands: train
MPI Rank 1: 08/04/2016 13:57:11: Precision = "float"
MPI Rank 1: 08/04/2016 13:57:11: Using 6 CPU threads.
MPI Rank 1: 08/04/2016 13:57:11: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 08/04/2016 13:57:11: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 08/04/2016 13:57:11: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: ##############################################################################
MPI Rank 1: 08/04/2016 13:57:11: #                                                                            #
MPI Rank 1: 08/04/2016 13:57:11: # Action "train"                                                             #
MPI Rank 1: 08/04/2016 13:57:11: #                                                                            #
MPI Rank 1: 08/04/2016 13:57:11: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: Creating virgin network.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: Created model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:11: Training criterion node(s):
MPI Rank 1: 08/04/2016 13:57:11: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 0x7f09b7aa61a8: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0x7f09b7aaaeb8: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0x7f09ceb9f1f8: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 0x7f09ceb9f478: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 0x7f09ceb9f9e8: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 0x7f09ceb9fb48: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 0x7f09ceb9fd08: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 0x7f09ceb9fec8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 0x7f09ceba0088: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 0x7f09ceba0248: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 0x7f09ceba0b08: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 0x7f09ceba1048: {[ce Gradient[1]] }
MPI Rank 1: 0x7f09ceba1208: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 0x7f09ceba13c8: {[SIM Gradient[51 x *]] }
MPI Rank 1: 0x7f09ceba1748: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 0x7f09ceba1b28: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0x7f09ceba1bc8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0x7f09cebca7e8: {[Keyword Value[49292 x *]] }
MPI Rank 1: 0x7f09cebcb808: {[S Value[1 x 1]] }
MPI Rank 1: 0x7f09cebcbc38: {[N Value[1 x 1]] }
MPI Rank 1: 0x7f09cebcc068: {[G Value[1 x 1]] }
MPI Rank 1: 0x7f09cebcc5d8: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 0x7f09cebfd2a8: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0x7f09cebfd3d8: {[SIM Value[51 x *]] }
MPI Rank 1: 0x7f09cebfd598: {[ce Value[1]] }
MPI Rank 1: 0x7f09cebfd9b8: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0x7f09cebfe0c8: {[Query Value[49292 x *]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 08/04/2016 13:57:11: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:14: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:14: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.02 seconds
MPI Rank 1: 08/04/2016 13:57:28:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 3.47101059 * 10240; time = 13.7107s; samplesPerSecond = 746.9
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.02 seconds
MPI Rank 1: 08/04/2016 13:57:42:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.69970703 * 10240; time = 14.0897s; samplesPerSecond = 726.8
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.02 seconds
MPI Rank 1: 08/04/2016 13:57:49: Finished Epoch[ 1 of 3]: [Training] ce = 2.91918920 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=34.3716s
MPI Rank 1: 08/04/2016 13:57:51: Final Results: Minibatch[1-26]: ce = 2.13785050 * 102399; perplexity = 8.48118773
MPI Rank 1: 08/04/2016 13:57:51: Finished Epoch[ 1 of 3]: [Validate] ce = 2.13785050 * 102399
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:53: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:57:53: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 13:58:07:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.02775860 * 10240; time = 13.1981s; samplesPerSecond = 775.9
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 13:58:21:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90990696 * 10240; time = 13.9372s; samplesPerSecond = 734.7
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 13:58:28: Finished Epoch[ 2 of 3]: [Training] ce = 1.94379515 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=34.217s
MPI Rank 1: 08/04/2016 13:58:30: Final Results: Minibatch[1-26]: ce = 1.81478808 * 102399; perplexity = 6.13977488
MPI Rank 1: 08/04/2016 13:58:30: Finished Epoch[ 2 of 3]: [Validate] ce = 1.81478808 * 102399
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:58:32: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:58:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 13:58:45:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.77812557 * 10240; time = 13.2218s; samplesPerSecond = 774.5
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 13:58:59:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.74922504 * 10240; time = 13.2979s; samplesPerSecond = 770.0
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 13:59:06: Finished Epoch[ 3 of 3]: [Training] ce = 1.75639887 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=33.3122s
MPI Rank 1: 08/04/2016 13:59:08: Final Results: Minibatch[1-26]: ce = 1.69809268 * 102399; perplexity = 5.46351680
MPI Rank 1: 08/04/2016 13:59:08: Finished Epoch[ 3 of 3]: [Validate] ce = 1.69809268 * 102399
MPI Rank 1: 08/04/2016 13:59:10: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:10: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:10: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 08/04/2016 13:57:10: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 13:57:10: Build info: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:10: 		Built time: Aug  4 2016 13:05:36
MPI Rank 2: 08/04/2016 13:57:10: 		Last modified date: Thu Aug  4 12:33:33 2016
MPI Rank 2: 08/04/2016 13:57:10: 		Build type: release
MPI Rank 2: 08/04/2016 13:57:10: 		Build target: GPU
MPI Rank 2: 08/04/2016 13:57:10: 		With 1bit-SGD: no
MPI Rank 2: 08/04/2016 13:57:10: 		Math lib: mkl
MPI Rank 2: 08/04/2016 13:57:10: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 08/04/2016 13:57:10: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 08/04/2016 13:57:10: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 08/04/2016 13:57:10: 		Build Branch: HEAD
MPI Rank 2: 08/04/2016 13:57:10: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 2: 08/04/2016 13:57:10: 		Built by philly on 643085f7f8c2
MPI Rank 2: 08/04/2016 13:57:10: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 08/04/2016 13:57:10: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 13:57:12: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 13:57:12: GPU info:
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 08/04/2016 13:57:12: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 08/04/2016 13:57:12: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 08/04/2016 13:57:12: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 08/04/2016 13:57:12: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: Running on localhost at 2016/08/04 13:57:12
MPI Rank 2: 08/04/2016 13:57:12: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 13:57:12: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 13:57:12: modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 08/04/2016 13:57:12: Commands: train
MPI Rank 2: 08/04/2016 13:57:12: Precision = "float"
MPI Rank 2: 08/04/2016 13:57:12: Using 6 CPU threads.
MPI Rank 2: 08/04/2016 13:57:12: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 08/04/2016 13:57:12: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 08/04/2016 13:57:12: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: ##############################################################################
MPI Rank 2: 08/04/2016 13:57:12: #                                                                            #
MPI Rank 2: 08/04/2016 13:57:12: # Action "train"                                                             #
MPI Rank 2: 08/04/2016 13:57:12: #                                                                            #
MPI Rank 2: 08/04/2016 13:57:12: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:12: Creating virgin network.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:13: Created model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:13: Training criterion node(s):
MPI Rank 2: 08/04/2016 13:57:13: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 0x7fe26079b918: {[S Value[1 x 1]] }
MPI Rank 2: 0x7fe26079bad8: {[N Value[1 x 1]] }
MPI Rank 2: 0x7fe2607b8408: {[G Value[1 x 1]] }
MPI Rank 2: 0x7fe2607b8958: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 0x7fe2607c78d8: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0x7fe2607c8a38: {[Query Value[49292 x *]] }
MPI Rank 2: 0x7fe2607c8f98: {[SIM Value[51 x *]] }
MPI Rank 2: 0x7fe2607c9158: {[ce Value[1]] }
MPI Rank 2: 0x7fe2607c9578: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0x7fe2607c99d8: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0x7fe2607c9b38: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0x7fe2607d99c8: {[Keyword Value[49292 x *]] }
MPI Rank 2: 0x7fe2607f6848: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 0x7fe2607f7d18: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 0x7fe2607f8058: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 0x7fe2607f8568: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 0x7fe2607f8728: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 0x7fe2607f88e8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 0x7fe2607f8aa8: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 0x7fe2607f8c68: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 0x7fe2607f9528: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 0x7fe2607f9a98: {[ce Gradient[1]] }
MPI Rank 2: 0x7fe2607f9c58: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 0x7fe2607f9e18: {[SIM Gradient[51 x *]] }
MPI Rank 2: 0x7fe2607fa198: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 0x7fe2607fa578: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0x7fe2607fa618: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 08/04/2016 13:57:13: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:14: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:14: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 13:57:28:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 3.44945602 * 10240; time = 13.7099s; samplesPerSecond = 746.9
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 13:57:42:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.72469749 * 10240; time = 14.0897s; samplesPerSecond = 726.8
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 13:57:49: Finished Epoch[ 1 of 3]: [Training] ce = 2.91918920 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=34.3716s
MPI Rank 2: 08/04/2016 13:57:51: Final Results: Minibatch[1-26]: ce = 2.13785050 * 102399; perplexity = 8.48118773
MPI Rank 2: 08/04/2016 13:57:51: Finished Epoch[ 1 of 3]: [Validate] ce = 2.13785050 * 102399
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:53: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:57:53: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 13:58:07:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.05766125 * 10240; time = 13.1981s; samplesPerSecond = 775.9
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 13:58:21:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.92501583 * 10240; time = 13.9372s; samplesPerSecond = 734.7
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 13:58:28: Finished Epoch[ 2 of 3]: [Training] ce = 1.94379515 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=34.217s
MPI Rank 2: 08/04/2016 13:58:30: Final Results: Minibatch[1-26]: ce = 1.81478808 * 102399; perplexity = 6.13977488
MPI Rank 2: 08/04/2016 13:58:30: Finished Epoch[ 2 of 3]: [Validate] ce = 1.81478808 * 102399
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:58:32: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:58:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 2: 08/04/2016 13:58:45:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.82333508 * 10240; time = 13.2217s; samplesPerSecond = 774.5
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 13:58:59:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.76784000 * 10240; time = 13.2979s; samplesPerSecond = 770.0
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 13:59:06: Finished Epoch[ 3 of 3]: [Training] ce = 1.75639887 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=33.3122s
MPI Rank 2: 08/04/2016 13:59:08: Final Results: Minibatch[1-26]: ce = 1.69809268 * 102399; perplexity = 5.46351680
MPI Rank 2: 08/04/2016 13:59:08: Finished Epoch[ 3 of 3]: [Validate] ce = 1.69809268 * 102399
MPI Rank 2: 08/04/2016 13:59:10: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:10: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:10: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 08/04/2016 13:57:11: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 13:57:11: Build info: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:11: 		Built time: Aug  4 2016 13:05:36
MPI Rank 3: 08/04/2016 13:57:11: 		Last modified date: Thu Aug  4 12:33:33 2016
MPI Rank 3: 08/04/2016 13:57:11: 		Build type: release
MPI Rank 3: 08/04/2016 13:57:11: 		Build target: GPU
MPI Rank 3: 08/04/2016 13:57:11: 		With 1bit-SGD: no
MPI Rank 3: 08/04/2016 13:57:11: 		Math lib: mkl
MPI Rank 3: 08/04/2016 13:57:11: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 3: 08/04/2016 13:57:11: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 3: 08/04/2016 13:57:11: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 3: 08/04/2016 13:57:11: 		Build Branch: HEAD
MPI Rank 3: 08/04/2016 13:57:11: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 3: 08/04/2016 13:57:11: 		Built by philly on 643085f7f8c2
MPI Rank 3: 08/04/2016 13:57:11: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 3: 08/04/2016 13:57:11: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 13:57:12: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 13:57:12: GPU info:
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 08/04/2016 13:57:12: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 08/04/2016 13:57:12: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 08/04/2016 13:57:12: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 08/04/2016 13:57:12: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: Running on localhost at 2016/08/04 13:57:12
MPI Rank 3: 08/04/2016 13:57:12: Command line: 
MPI Rank 3: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 13:57:12: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 13:57:12: modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 08/04/2016 13:57:12: Commands: train
MPI Rank 3: 08/04/2016 13:57:12: Precision = "float"
MPI Rank 3: 08/04/2016 13:57:12: Using 6 CPU threads.
MPI Rank 3: 08/04/2016 13:57:12: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 08/04/2016 13:57:12: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 08/04/2016 13:57:12: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: ##############################################################################
MPI Rank 3: 08/04/2016 13:57:12: #                                                                            #
MPI Rank 3: 08/04/2016 13:57:12: # Action "train"                                                             #
MPI Rank 3: 08/04/2016 13:57:12: #                                                                            #
MPI Rank 3: 08/04/2016 13:57:12: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: Creating virgin network.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: Created model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:12: Training criterion node(s):
MPI Rank 3: 08/04/2016 13:57:12: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: (nil): {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 0x24b3338: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0x2630108: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 0x2630678: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 0x2630a28: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 0x2630be8: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 0x2630da8: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 0x2630f68: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 0x2631828: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 0x2631d98: {[ce Gradient[1]] }
MPI Rank 3: 0x2631f58: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 0x2632118: {[SIM Gradient[51 x *]] }
MPI Rank 3: 0x2632498: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 0x2632878: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0x2632918: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 0x2c9fc78: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0x2ca2388: {[Keyword Value[49292 x *]] }
MPI Rank 3: 0x2ca2568: {[S Value[1 x 1]] }
MPI Rank 3: 0x2ca2608: {[N Value[1 x 1]] }
MPI Rank 3: 0x2ca8168: {[G Value[1 x 1]] }
MPI Rank 3: 0x2ca8658: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 0x2cab9e8: {[SIM Value[51 x *]] }
MPI Rank 3: 0x2cabae8: {[ce Value[1]] }
MPI Rank 3: 0x2cabc78: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 0x2cad188: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 0x2cbac18: {[Query Value[49292 x *]] }
MPI Rank 3: 0x2cbb468: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0x2cbb918: {[WD1 Value[64 x 288]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 08/04/2016 13:57:12: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:14: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:14: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.04 seconds
MPI Rank 3: 08/04/2016 13:57:28:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 3.44398956 * 10240; time = 13.7360s; samplesPerSecond = 745.5
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.03 seconds
MPI Rank 3: 08/04/2016 13:57:42:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.71102791 * 10240; time = 14.0897s; samplesPerSecond = 726.8
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.73 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.03 seconds
MPI Rank 3: 08/04/2016 13:57:49: Finished Epoch[ 1 of 3]: [Training] ce = 2.91918920 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-05; epochTime=34.3716s
MPI Rank 3: 08/04/2016 13:57:51: Final Results: Minibatch[1-26]: ce = 2.13785050 * 102399; perplexity = 8.48118773
MPI Rank 3: 08/04/2016 13:57:51: Finished Epoch[ 1 of 3]: [Validate] ce = 2.13785050 * 102399
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:53: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:57:53: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 13:58:07:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.00262775 * 10240; time = 13.1983s; samplesPerSecond = 775.9
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 13:58:21:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90484028 * 10240; time = 13.9372s; samplesPerSecond = 734.7
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 13:58:28: Finished Epoch[ 2 of 3]: [Training] ce = 1.94379515 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-05; epochTime=34.217s
MPI Rank 3: 08/04/2016 13:58:30: Final Results: Minibatch[1-26]: ce = 1.81478808 * 102399; perplexity = 6.13977488
MPI Rank 3: 08/04/2016 13:58:30: Finished Epoch[ 2 of 3]: [Validate] ce = 1.81478808 * 102399
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:58:32: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:58:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 3: 08/04/2016 13:58:45:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.75186119 * 10240; time = 13.2219s; samplesPerSecond = 774.5
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.02 seconds
MPI Rank 3: 08/04/2016 13:58:59:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.73868027 * 10240; time = 13.2979s; samplesPerSecond = 770.0
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 13:59:06: Finished Epoch[ 3 of 3]: [Training] ce = 1.75639887 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=33.3122s
MPI Rank 3: 08/04/2016 13:59:08: Final Results: Minibatch[1-26]: ce = 1.69809268 * 102399; perplexity = 5.46351680
MPI Rank 3: 08/04/2016 13:59:08: Finished Epoch[ 3 of 3]: [Validate] ce = 1.69809268 * 102399
MPI Rank 3: 08/04/2016 13:59:10: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:10: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:10: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running mpiexec -n 4 /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/ OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=6 stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
Changed current directory to /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 13:05:36
		Last modified date: Thu Aug  4 12:33:33 2016
		Build type: release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-7.5
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-4.0
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by philly on 643085f7f8c2
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
-------------------------------------------------------------------
Changed current directory to /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
08/04/2016 13:59:11: Redirecting stderr to file /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr_train.logrank0
08/04/2016 13:59:11: Redirecting stderr to file /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr_train.logrank1
08/04/2016 13:59:12: Redirecting stderr to file /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr_train.logrank2
08/04/2016 13:59:12: Redirecting stderr to file /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 0: 08/04/2016 13:59:11: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 13:59:11: Build info: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:11: 		Built time: Aug  4 2016 13:05:36
MPI Rank 0: 08/04/2016 13:59:11: 		Last modified date: Thu Aug  4 12:33:33 2016
MPI Rank 0: 08/04/2016 13:59:11: 		Build type: release
MPI Rank 0: 08/04/2016 13:59:11: 		Build target: GPU
MPI Rank 0: 08/04/2016 13:59:11: 		With 1bit-SGD: no
MPI Rank 0: 08/04/2016 13:59:11: 		Math lib: mkl
MPI Rank 0: 08/04/2016 13:59:11: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 0: 08/04/2016 13:59:11: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 0: 08/04/2016 13:59:11: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 0: 08/04/2016 13:59:11: 		Build Branch: HEAD
MPI Rank 0: 08/04/2016 13:59:11: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 0: 08/04/2016 13:59:11: 		Built by philly on 643085f7f8c2
MPI Rank 0: 08/04/2016 13:59:11: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 0: 08/04/2016 13:59:11: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 13:59:12: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 13:59:12: GPU info:
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 08/04/2016 13:59:12: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 08/04/2016 13:59:12: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 08/04/2016 13:59:12: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 0: 08/04/2016 13:59:12: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: Running on localhost at 2016/08/04 13:59:12
MPI Rank 0: 08/04/2016 13:59:12: Command line: 
MPI Rank 0: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 13:59:12: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 13:59:12: modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=6
MPI Rank 0: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 0: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 08/04/2016 13:59:12: Commands: train
MPI Rank 0: 08/04/2016 13:59:12: Precision = "float"
MPI Rank 0: 08/04/2016 13:59:12: Using 6 CPU threads.
MPI Rank 0: 08/04/2016 13:59:12: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 08/04/2016 13:59:12: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 08/04/2016 13:59:12: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: ##############################################################################
MPI Rank 0: 08/04/2016 13:59:12: #                                                                            #
MPI Rank 0: 08/04/2016 13:59:12: # Action "train"                                                             #
MPI Rank 0: 08/04/2016 13:59:12: #                                                                            #
MPI Rank 0: 08/04/2016 13:59:12: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:12: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:14: Loaded model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:14: Training criterion node(s):
MPI Rank 0: 08/04/2016 13:59:14: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 0x1262ca8: {[G Value[1 x 1]] }
MPI Rank 0: 0x129e448: {[SIM Value[51 x *1]] }
MPI Rank 0: 0x129e568: {[ce Value[1]] }
MPI Rank 0: 0x129ff38: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 0x12a0278: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 0x12a0668: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 0x12a0828: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 0x12a09e8: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 0x12a0ba8: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 0x12a0d68: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 0x12a0f28: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 0x12a17e8: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 0x12a1d58: {[ce Gradient[1]] }
MPI Rank 0: 0x12a2b08: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 0x1485778: {[N Value[1 x 1]] }
MPI Rank 0: 0x1485c58: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0x14860a8: {[S Value[1 x 1]] }
MPI Rank 0: 0x14875d8: {[WD1 Value[64 x 288]] }
MPI Rank 0: 0x14880e8: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0x1488b78: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0x148e968: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 0x148ede8: {[Query Value[49292 x *1]] }
MPI Rank 0: 0x1498da8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 0x1498f68: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 0x14992e8: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 0x1499678: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0x1499718: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 08/04/2016 13:59:14: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:17: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:17: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 13:59:31:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.73774796 * 10240; time = 13.4322s; samplesPerSecond = 762.3
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:59:44:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.68121185 * 10240; time = 13.6378s; samplesPerSecond = 750.9
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.02 seconds
MPI Rank 0: 08/04/2016 13:59:52: Finished Epoch[ 3 of 3]: [Training] ce = 1.76047450 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=34.2081s
MPI Rank 0: 08/04/2016 13:59:54: Final Results: Minibatch[1-26]: ce = 1.70737101 * 102399; perplexity = 5.51444497
MPI Rank 0: 08/04/2016 13:59:54: Finished Epoch[ 3 of 3]: [Validate] ce = 1.70737101 * 102399
MPI Rank 0: 08/04/2016 13:59:55: SGD: Saving checkpoint model '/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 08/04/2016 13:59:56: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:56: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 13:59:56: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 08/04/2016 13:59:11: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 13:59:11: Build info: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:11: 		Built time: Aug  4 2016 13:05:36
MPI Rank 1: 08/04/2016 13:59:11: 		Last modified date: Thu Aug  4 12:33:33 2016
MPI Rank 1: 08/04/2016 13:59:11: 		Build type: release
MPI Rank 1: 08/04/2016 13:59:11: 		Build target: GPU
MPI Rank 1: 08/04/2016 13:59:11: 		With 1bit-SGD: no
MPI Rank 1: 08/04/2016 13:59:11: 		Math lib: mkl
MPI Rank 1: 08/04/2016 13:59:11: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 1: 08/04/2016 13:59:11: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 1: 08/04/2016 13:59:11: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 1: 08/04/2016 13:59:11: 		Build Branch: HEAD
MPI Rank 1: 08/04/2016 13:59:11: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 1: 08/04/2016 13:59:11: 		Built by philly on 643085f7f8c2
MPI Rank 1: 08/04/2016 13:59:11: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 1: 08/04/2016 13:59:11: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 13:59:12: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 13:59:12: GPU info:
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 08/04/2016 13:59:12: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 08/04/2016 13:59:12: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 08/04/2016 13:59:12: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 1: 08/04/2016 13:59:12: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: Running on localhost at 2016/08/04 13:59:12
MPI Rank 1: 08/04/2016 13:59:12: Command line: 
MPI Rank 1: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 13:59:12: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 13:59:12: modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=6
MPI Rank 1: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 1: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 08/04/2016 13:59:12: Commands: train
MPI Rank 1: 08/04/2016 13:59:12: Precision = "float"
MPI Rank 1: 08/04/2016 13:59:12: Using 6 CPU threads.
MPI Rank 1: 08/04/2016 13:59:12: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 08/04/2016 13:59:12: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 08/04/2016 13:59:12: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: ##############################################################################
MPI Rank 1: 08/04/2016 13:59:12: #                                                                            #
MPI Rank 1: 08/04/2016 13:59:12: # Action "train"                                                             #
MPI Rank 1: 08/04/2016 13:59:12: #                                                                            #
MPI Rank 1: 08/04/2016 13:59:12: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:12: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:14: Loaded model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:14: Training criterion node(s):
MPI Rank 1: 08/04/2016 13:59:14: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 0x20fdf78: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 0x20ffa18: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 0x2100078: {[Query Value[49292 x *1]] }
MPI Rank 1: 0x21004c8: {[N Value[1 x 1]] }
MPI Rank 1: 0x2100e58: {[S Value[1 x 1]] }
MPI Rank 1: 0x21018d8: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 0x2102368: {[WD1 Value[64 x 288]] }
MPI Rank 1: 0x2102528: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 0x227b138: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 0x227b478: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 0x227b988: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 0x227bb48: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 0x227bd08: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 0x227bec8: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 0x227c088: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 0x227c248: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 0x227cb08: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0x227d048: {[ce Gradient[1]] }
MPI Rank 1: 0x227d208: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 0x227d3c8: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 0x227d748: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 0x227db28: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0x227dbc8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0x2468b28: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0x246b588: {[SIM Value[51 x *1]] }
MPI Rank 1: 0x246b7b8: {[ce Value[1]] }
MPI Rank 1: 0x24aa5d8: {[G Value[1 x 1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 08/04/2016 13:59:14: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:17: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:17: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.02 seconds
MPI Rank 1: 08/04/2016 13:59:31:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.78650436 * 10240; time = 13.4330s; samplesPerSecond = 762.3
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.02 seconds
MPI Rank 1: 08/04/2016 13:59:44:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.77671204 * 10240; time = 13.6378s; samplesPerSecond = 750.9
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.02 seconds
MPI Rank 1: 08/04/2016 13:59:52: Finished Epoch[ 3 of 3]: [Training] ce = 1.76047450 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=34.2081s
MPI Rank 1: 08/04/2016 13:59:54: Final Results: Minibatch[1-26]: ce = 1.70737101 * 102399; perplexity = 5.51444497
MPI Rank 1: 08/04/2016 13:59:54: Finished Epoch[ 3 of 3]: [Validate] ce = 1.70737101 * 102399
MPI Rank 1: 08/04/2016 13:59:56: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:56: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 13:59:56: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 08/04/2016 13:59:12: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 13:59:12: Build info: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:12: 		Built time: Aug  4 2016 13:05:36
MPI Rank 2: 08/04/2016 13:59:12: 		Last modified date: Thu Aug  4 12:33:33 2016
MPI Rank 2: 08/04/2016 13:59:12: 		Build type: release
MPI Rank 2: 08/04/2016 13:59:12: 		Build target: GPU
MPI Rank 2: 08/04/2016 13:59:12: 		With 1bit-SGD: no
MPI Rank 2: 08/04/2016 13:59:12: 		Math lib: mkl
MPI Rank 2: 08/04/2016 13:59:12: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 2: 08/04/2016 13:59:12: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 2: 08/04/2016 13:59:12: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 2: 08/04/2016 13:59:12: 		Build Branch: HEAD
MPI Rank 2: 08/04/2016 13:59:12: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 2: 08/04/2016 13:59:12: 		Built by philly on 643085f7f8c2
MPI Rank 2: 08/04/2016 13:59:12: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 2: 08/04/2016 13:59:12: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 13:59:14: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 13:59:14: GPU info:
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 08/04/2016 13:59:14: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 08/04/2016 13:59:14: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 08/04/2016 13:59:14: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 2: 08/04/2016 13:59:14: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: Running on localhost at 2016/08/04 13:59:14
MPI Rank 2: 08/04/2016 13:59:14: Command line: 
MPI Rank 2: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 13:59:14: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 13:59:14: modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=6
MPI Rank 2: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 2: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 08/04/2016 13:59:14: Commands: train
MPI Rank 2: 08/04/2016 13:59:14: Precision = "float"
MPI Rank 2: 08/04/2016 13:59:14: Using 6 CPU threads.
MPI Rank 2: 08/04/2016 13:59:14: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 08/04/2016 13:59:14: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 08/04/2016 13:59:14: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: ##############################################################################
MPI Rank 2: 08/04/2016 13:59:14: #                                                                            #
MPI Rank 2: 08/04/2016 13:59:14: # Action "train"                                                             #
MPI Rank 2: 08/04/2016 13:59:14: #                                                                            #
MPI Rank 2: 08/04/2016 13:59:14: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:14: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:16: Loaded model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:16: Training criterion node(s):
MPI Rank 2: 08/04/2016 13:59:16: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 0x7fac7b00ad58: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0x7fac7b00d608: {[SIM Value[51 x *1]] }
MPI Rank 2: 0x7fac7b00d928: {[ce Value[1]] }
MPI Rank 2: 0x7fac7b00f298: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 0x7fac7b00f518: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 0x7fac7b00fa88: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 0x7fac7b00fbe8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 0x7fac7b00fda8: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 0x7fac7b00ff68: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 0x7fac7b010128: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 0x7fac7b034428: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 0x7fac7b034ce8: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0x7fac7b035258: {[ce Gradient[1]] }
MPI Rank 2: 0x7fac7b035418: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 0x7fac7b0355d8: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 0x7fac7b035958: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 0x7fac7b035d38: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0x7fac7b035dd8: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 0x7fac7b0d75f8: {[S Value[1 x 1]] }
MPI Rank 2: 0x7fac7b0d8108: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0x7fac7b0d8bf8: {[WD1 Value[64 x 288]] }
MPI Rank 2: 0x7fac7b0d9118: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0x7fac7f3f4618: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 0x7fac7f3f91f8: {[G Value[1 x 1]] }
MPI Rank 2: 0x7fac7f3fe478: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 0x7fac7f3ffa38: {[N Value[1 x 1]] }
MPI Rank 2: 0x7fac7f3fff48: {[Query Value[49292 x *1]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 08/04/2016 13:59:16: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:17: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:17: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.00 seconds
MPI Rank 2: 08/04/2016 13:59:31:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.83119640 * 10240; time = 13.4176s; samplesPerSecond = 763.2
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 08/04/2016 13:59:44:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79677334 * 10240; time = 13.6378s; samplesPerSecond = 750.9
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.00 seconds
MPI Rank 2: 08/04/2016 13:59:52: Finished Epoch[ 3 of 3]: [Training] ce = 1.76047450 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=34.2081s
MPI Rank 2: 08/04/2016 13:59:54: Final Results: Minibatch[1-26]: ce = 1.70737101 * 102399; perplexity = 5.51444497
MPI Rank 2: 08/04/2016 13:59:54: Finished Epoch[ 3 of 3]: [Validate] ce = 1.70737101 * 102399
MPI Rank 2: 08/04/2016 13:59:56: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:56: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 13:59:56: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 08/04/2016 13:59:12: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 13:59:12: Build info: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:12: 		Built time: Aug  4 2016 13:05:36
MPI Rank 3: 08/04/2016 13:59:12: 		Last modified date: Thu Aug  4 12:33:33 2016
MPI Rank 3: 08/04/2016 13:59:12: 		Build type: release
MPI Rank 3: 08/04/2016 13:59:12: 		Build target: GPU
MPI Rank 3: 08/04/2016 13:59:12: 		With 1bit-SGD: no
MPI Rank 3: 08/04/2016 13:59:12: 		Math lib: mkl
MPI Rank 3: 08/04/2016 13:59:12: 		CUDA_PATH: /usr/local/cuda-7.5
MPI Rank 3: 08/04/2016 13:59:12: 		CUB_PATH: /usr/local/cub-1.4.1
MPI Rank 3: 08/04/2016 13:59:12: 		CUDNN_PATH: /usr/local/cudnn-4.0
MPI Rank 3: 08/04/2016 13:59:12: 		Build Branch: HEAD
MPI Rank 3: 08/04/2016 13:59:12: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 3: 08/04/2016 13:59:12: 		Built by philly on 643085f7f8c2
MPI Rank 3: 08/04/2016 13:59:12: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
MPI Rank 3: 08/04/2016 13:59:12: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 13:59:13: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 13:59:13: GPU info:
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 08/04/2016 13:59:13: 		Device[1]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 08/04/2016 13:59:13: 		Device[2]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 08/04/2016 13:59:13: 		Device[3]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; memory = 3071 MB
MPI Rank 3: 08/04/2016 13:59:13: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: Running on localhost at 2016/08/04 13:59:13
MPI Rank 3: 08/04/2016 13:59:13: Command line: 
MPI Rank 3: /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.cntk  currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/  OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=6  stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 13:59:13: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 13:59:13: modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=6
MPI Rank 3: stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM/
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Text/SparseDSSM//dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=6
MPI Rank 3: configparameters: dssm.cntk:OutputDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 08/04/2016 13:59:13: Commands: train
MPI Rank 3: 08/04/2016 13:59:13: Precision = "float"
MPI Rank 3: 08/04/2016 13:59:13: Using 6 CPU threads.
MPI Rank 3: 08/04/2016 13:59:13: CNTKModelPath: /tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 08/04/2016 13:59:13: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 08/04/2016 13:59:13: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: ##############################################################################
MPI Rank 3: 08/04/2016 13:59:13: #                                                                            #
MPI Rank 3: 08/04/2016 13:59:13: # Action "train"                                                             #
MPI Rank 3: 08/04/2016 13:59:13: #                                                                            #
MPI Rank 3: 08/04/2016 13:59:13: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:13: Starting from checkpoint. Loading network from '/tmp/cntk-test-20160804135211.433559/Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:14: Loaded model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:14: Training criterion node(s):
MPI Rank 3: 08/04/2016 13:59:14: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: (nil): {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 0x2c9a6a8: {[N Value[1 x 1]] }
MPI Rank 3: 0x2cd5628: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 0x2cd6118: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 0x2cd8958: {[SIM Value[51 x *1]] }
MPI Rank 3: 0x2cd8c78: {[ce Value[1]] }
MPI Rank 3: 0x337a388: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 0x337a608: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 0x337ab78: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 0x337acd8: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 0x337ae98: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 0x337b058: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 0x337b218: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 0x337b3d8: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 0x337bc98: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 0x337c208: {[ce Gradient[1]] }
MPI Rank 3: 0x337c3c8: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 0x337c588: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 0x337c908: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 0x337cce8: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 0x337cd88: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 0x3381eb8: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 0x3383e28: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 0x33842b8: {[G Value[1 x 1]] }
MPI Rank 3: 0x346a5e8: {[Query Value[49292 x *1]] }
MPI Rank 3: 0x346ab38: {[S Value[1 x 1]] }
MPI Rank 3: 0x346b628: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 0x346bc48: {[WD1 Value[64 x 288]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 08/04/2016 13:59:15: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:17: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:17: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.05 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.03 seconds
MPI Rank 3: 08/04/2016 13:59:31:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.75967293 * 10240; time = 13.4437s; samplesPerSecond = 761.7
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 08/04/2016 13:59:44:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.76906281 * 10240; time = 13.6378s; samplesPerSecond = 750.9
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.02 seconds
MPI Rank 3: 08/04/2016 13:59:52: Finished Epoch[ 3 of 3]: [Training] ce = 1.76047450 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-05; epochTime=34.2081s
MPI Rank 3: 08/04/2016 13:59:54: Final Results: Minibatch[1-26]: ce = 1.70737101 * 102399; perplexity = 5.51444497
MPI Rank 3: 08/04/2016 13:59:54: Finished Epoch[ 3 of 3]: [Validate] ce = 1.70737101 * 102399
MPI Rank 3: 08/04/2016 13:59:56: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:56: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 13:59:56: __COMPLETED__
MPI Rank 3: ~MPIWrapper