CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3550 @ 3.07GHz
    Hardware threads: 4
    Total Memory: 12580388 kB
-------------------------------------------------------------------
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu DeviceId=0 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 08/04/2016 09:27:43: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr_train.logrank0
MPI Rank 0: 08/04/2016 09:27:43: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:27:43: Build info: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:43: 		Built time: Aug  4 2016 06:18:04
MPI Rank 0: 08/04/2016 09:27:43: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 0: 08/04/2016 09:27:43: 		Build type: Release
MPI Rank 0: 08/04/2016 09:27:43: 		Build target: GPU
MPI Rank 0: 08/04/2016 09:27:43: 		With 1bit-SGD: no
MPI Rank 0: 08/04/2016 09:27:43: 		Math lib: mkl
MPI Rank 0: 08/04/2016 09:27:43: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 08/04/2016 09:27:43: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 08/04/2016 09:27:43: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 08/04/2016 09:27:43: 		Build Branch: HEAD
MPI Rank 0: 08/04/2016 09:27:43: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 0: 08/04/2016 09:27:43: 		Built by svcphil on dphaim-26-new
MPI Rank 0: 08/04/2016 09:27:43: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 0: 08/04/2016 09:27:43: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:27:44: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:27:44: GPU info:
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 08/04/2016 09:27:44: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: Running on cntk-muc02 at 2016/08/04 09:27:44
MPI Rank 0: 08/04/2016 09:27:44: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 09:27:44: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 09:27:44: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 08/04/2016 09:27:44: Commands: train
MPI Rank 0: 08/04/2016 09:27:44: Precision = "float"
MPI Rank 0: 08/04/2016 09:27:44: Using 1 CPU threads.
MPI Rank 0: 08/04/2016 09:27:44: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: 08/04/2016 09:27:44: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 08/04/2016 09:27:44: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: ##############################################################################
MPI Rank 0: 08/04/2016 09:27:44: #                                                                            #
MPI Rank 0: 08/04/2016 09:27:44: # Action "train"                                                             #
MPI Rank 0: 08/04/2016 09:27:44: #                                                                            #
MPI Rank 0: 08/04/2016 09:27:44: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: Creating virgin network.
MPI Rank 0: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: Created model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:44: Training criterion node(s):
MPI Rank 0: 08/04/2016 09:27:44: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 000000C415D7C460: {[WD1 Value[64 x 288]] }
MPI Rank 0: 000000C424247C20: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 000000C424247CC0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 000000C424247D60: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 000000C424247E00: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 000000C424247FE0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 000000C4242481C0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 000000C4242486C0: {[SIM Value[51 x *]] }
MPI Rank 0: 000000C424248760: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 000000C424248C60: {[ce Value[1]] }
MPI Rank 0: 000000C424248DA0: {[G Value[1 x 1]] }
MPI Rank 0: 000000C424248E40: {[Query Value[49292 x *]] }
MPI Rank 0: 000000C424249340: {[N Value[1 x 1]] }
MPI Rank 0: 000000C424249660: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 000000C424249700: {[Keyword Value[49292 x *]] }
MPI Rank 0: 000000C4242497A0: {[S Value[1 x 1]] }
MPI Rank 0: 000000C4242498E0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 000000C42A4232D0: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 000000C42A423370: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 000000C42A423FF0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 000000C42A772890: {[SIM Gradient[51 x *]] }
MPI Rank 0: 000000C42A772A70: {[ce Gradient[1]] }
MPI Rank 0: 000000C42A773790: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 000000C42A773830: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 000000C42A773970: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 000000C42A773A10: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 000000C42A7740F0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 08/04/2016 09:27:44: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:47: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:47: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:27:53:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.34696808 * 10240; time = 5.4481s; samplesPerSecond = 1879.6
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:27:58:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.34277420 * 10240; time = 5.0914s; samplesPerSecond = 2011.3
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:00: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601714 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=13.2486s
MPI Rank 0: 08/04/2016 09:28:01: Final Results: Minibatch[1-26]: ce = 2.49916007 * 102399; perplexity = 12.17226578
MPI Rank 0: 08/04/2016 09:28:01: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916007 * 102399
MPI Rank 0: 08/04/2016 09:28:02: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:04: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:04: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:09:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.30270958 * 10240; time = 5.0602s; samplesPerSecond = 2023.6
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:14:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.09883842 * 10240; time = 5.0815s; samplesPerSecond = 2015.2
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:16: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=12.6339s
MPI Rank 0: 08/04/2016 09:28:17: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107636
MPI Rank 0: 08/04/2016 09:28:17: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 0: 08/04/2016 09:28:18: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:20: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:20: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:25:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.89778175 * 10240; time = 5.1286s; samplesPerSecond = 1996.6
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:30:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86335983 * 10240; time = 5.0205s; samplesPerSecond = 2039.7
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:32: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563949 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=12.6499s
MPI Rank 0: 08/04/2016 09:28:33: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525581
MPI Rank 0: 08/04/2016 09:28:33: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 0: 08/04/2016 09:28:34: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net'
MPI Rank 0: 08/04/2016 09:28:36: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:36: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:36: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 08/04/2016 09:27:44: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr_train.logrank1
MPI Rank 1: 08/04/2016 09:27:44: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:27:44: Build info: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: 		Built time: Aug  4 2016 06:18:04
MPI Rank 1: 08/04/2016 09:27:44: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 1: 08/04/2016 09:27:44: 		Build type: Release
MPI Rank 1: 08/04/2016 09:27:44: 		Build target: GPU
MPI Rank 1: 08/04/2016 09:27:44: 		With 1bit-SGD: no
MPI Rank 1: 08/04/2016 09:27:44: 		Math lib: mkl
MPI Rank 1: 08/04/2016 09:27:44: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 08/04/2016 09:27:44: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 08/04/2016 09:27:44: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 08/04/2016 09:27:44: 		Build Branch: HEAD
MPI Rank 1: 08/04/2016 09:27:44: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 1: 08/04/2016 09:27:44: 		Built by svcphil on dphaim-26-new
MPI Rank 1: 08/04/2016 09:27:44: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 1: 08/04/2016 09:27:44: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:27:44: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:27:44: GPU info:
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 08/04/2016 09:27:44: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: Running on cntk-muc02 at 2016/08/04 09:27:44
MPI Rank 1: 08/04/2016 09:27:44: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 09:27:44: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 09:27:44: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 08/04/2016 09:27:44: Commands: train
MPI Rank 1: 08/04/2016 09:27:44: Precision = "float"
MPI Rank 1: 08/04/2016 09:27:44: Using 1 CPU threads.
MPI Rank 1: 08/04/2016 09:27:44: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: 08/04/2016 09:27:44: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 08/04/2016 09:27:44: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: ##############################################################################
MPI Rank 1: 08/04/2016 09:27:44: #                                                                            #
MPI Rank 1: 08/04/2016 09:27:44: # Action "train"                                                             #
MPI Rank 1: 08/04/2016 09:27:44: #                                                                            #
MPI Rank 1: 08/04/2016 09:27:44: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: Creating virgin network.
MPI Rank 1: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: Created model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:44: Training criterion node(s):
MPI Rank 1: 08/04/2016 09:27:44: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 0000008658797160: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 000000865AAB6010: {[Query Value[49292 x *]] }
MPI Rank 1: 000000865AAB6970: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 000000865AAB6AB0: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 000000865AAB6BF0: {[WD1 Value[64 x 288]] }
MPI Rank 1: 000000865AAB6FB0: {[Keyword Value[49292 x *]] }
MPI Rank 1: 000000866CD77860: {[SIM Value[51 x *]] }
MPI Rank 1: 000000866CD77900: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 000000866CD77C20: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 000000866CD77E00: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 000000866CD77FE0: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 000000866CD78080: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 000000866CD781C0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 000000866CD783A0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 000000866CD784E0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 000000866CD78580: {[SIM Gradient[51 x *]] }
MPI Rank 1: 000000866CD78760: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 000000866CD78800: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 000000866CD78940: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 000000866CD78B20: {[G Value[1 x 1]] }
MPI Rank 1: 000000866CD78C60: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 000000866CD78D00: {[ce Value[1]] }
MPI Rank 1: 000000866CD78EE0: {[S Value[1 x 1]] }
MPI Rank 1: 000000866CD790C0: {[ce Gradient[1]] }
MPI Rank 1: 000000866CD79160: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 000000866CD793E0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 000000866CD795C0: {[N Value[1 x 1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 08/04/2016 09:27:44: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:47: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:47: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:27:53:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32159615 * 10240; time = 5.4883s; samplesPerSecond = 1865.8
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:27:58:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.33525505 * 10240; time = 5.0963s; samplesPerSecond = 2009.3
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:00: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601714 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=13.2751s
MPI Rank 1: 08/04/2016 09:28:01: Final Results: Minibatch[1-26]: ce = 2.49916007 * 102399; perplexity = 12.17226578
MPI Rank 1: 08/04/2016 09:28:01: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916007 * 102399
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:04: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:04: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:09:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32732925 * 10240; time = 5.0596s; samplesPerSecond = 2023.9
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:14:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11035995 * 10240; time = 5.1107s; samplesPerSecond = 2003.6
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:16: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=12.6339s
MPI Rank 1: 08/04/2016 09:28:17: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107636
MPI Rank 1: 08/04/2016 09:28:17: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:20: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:20: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:25:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.92909813 * 10240; time = 5.0834s; samplesPerSecond = 2014.4
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:30:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86598778 * 10240; time = 5.0200s; samplesPerSecond = 2039.9
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:32: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563949 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=12.6494s
MPI Rank 1: 08/04/2016 09:28:33: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525581
MPI Rank 1: 08/04/2016 09:28:33: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 1: 08/04/2016 09:28:36: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:36: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:36: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 08/04/2016 09:27:44: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr_train.logrank2
MPI Rank 2: 08/04/2016 09:27:44: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:27:44: Build info: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:44: 		Built time: Aug  4 2016 06:18:04
MPI Rank 2: 08/04/2016 09:27:44: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 2: 08/04/2016 09:27:44: 		Build type: Release
MPI Rank 2: 08/04/2016 09:27:44: 		Build target: GPU
MPI Rank 2: 08/04/2016 09:27:44: 		With 1bit-SGD: no
MPI Rank 2: 08/04/2016 09:27:44: 		Math lib: mkl
MPI Rank 2: 08/04/2016 09:27:44: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 08/04/2016 09:27:44: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 08/04/2016 09:27:44: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 08/04/2016 09:27:44: 		Build Branch: HEAD
MPI Rank 2: 08/04/2016 09:27:44: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 2: 08/04/2016 09:27:44: 		Built by svcphil on dphaim-26-new
MPI Rank 2: 08/04/2016 09:27:44: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 2: 08/04/2016 09:27:44: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:27:45: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:27:45: GPU info:
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 08/04/2016 09:27:45: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: Running on cntk-muc02 at 2016/08/04 09:27:45
MPI Rank 2: 08/04/2016 09:27:45: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 09:27:45: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 09:27:45: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 08/04/2016 09:27:45: Commands: train
MPI Rank 2: 08/04/2016 09:27:45: Precision = "float"
MPI Rank 2: 08/04/2016 09:27:45: Using 1 CPU threads.
MPI Rank 2: 08/04/2016 09:27:45: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: 08/04/2016 09:27:45: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 08/04/2016 09:27:45: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: ##############################################################################
MPI Rank 2: 08/04/2016 09:27:45: #                                                                            #
MPI Rank 2: 08/04/2016 09:27:45: # Action "train"                                                             #
MPI Rank 2: 08/04/2016 09:27:45: #                                                                            #
MPI Rank 2: 08/04/2016 09:27:45: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: Creating virgin network.
MPI Rank 2: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: Created model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:45: Training criterion node(s):
MPI Rank 2: 08/04/2016 09:27:45: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 000000CD051173C0: {[WD1 Value[64 x 288]] }
MPI Rank 2: 000000CD05117460: {[Query Value[49292 x *]] }
MPI Rank 2: 000000CD05117820: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 000000CD05117DC0: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 000000CD05118860: {[Keyword Value[49292 x *]] }
MPI Rank 2: 000000CD1435CB90: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 000000CD1435CC30: {[SIM Gradient[51 x *]] }
MPI Rank 2: 000000CD1435CCD0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 000000CD1435CFF0: {[S Value[1 x 1]] }
MPI Rank 2: 000000CD1435D1D0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 000000CD1435D3B0: {[N Value[1 x 1]] }
MPI Rank 2: 000000CD1435D590: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 000000CD1435D630: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 000000CD1435D770: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 000000CD1435DA90: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 000000CD1435DD10: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 000000CD1435DDB0: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 000000CD1435DEF0: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 000000CD1435DF90: {[G Value[1 x 1]] }
MPI Rank 2: 000000CD1435E170: {[SIM Value[51 x *]] }
MPI Rank 2: 000000CD1435E3F0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 000000CD1435E5D0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 000000CD1435E670: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 000000CD1435E710: {[ce Gradient[1]] }
MPI Rank 2: 000000CD1435E7B0: {[ce Value[1]] }
MPI Rank 2: 000000CD1435E850: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 000000CD7FA27200: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 08/04/2016 09:27:45: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:47: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:47: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:27:53:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32837563 * 10240; time = 5.5096s; samplesPerSecond = 1858.6
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:27:58:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35655479 * 10240; time = 5.0770s; samplesPerSecond = 2016.9
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:00: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601714 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=13.2483s
MPI Rank 2: 08/04/2016 09:28:01: Final Results: Minibatch[1-26]: ce = 2.49916007 * 102399; perplexity = 12.17226578
MPI Rank 2: 08/04/2016 09:28:01: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916007 * 102399
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:04: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:04: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:09:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.32893581 * 10240; time = 5.0595s; samplesPerSecond = 2023.9
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:14:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11646938 * 10240; time = 5.0817s; samplesPerSecond = 2015.1
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:16: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=12.6322s
MPI Rank 2: 08/04/2016 09:28:17: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107636
MPI Rank 2: 08/04/2016 09:28:17: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:20: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:20: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:25:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95308418 * 10240; time = 5.1074s; samplesPerSecond = 2004.9
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:30:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87902641 * 10240; time = 4.9964s; samplesPerSecond = 2049.5
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:32: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563949 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=12.6481s
MPI Rank 2: 08/04/2016 09:28:33: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525581
MPI Rank 2: 08/04/2016 09:28:33: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 2: 08/04/2016 09:28:36: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:36: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:36: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 08/04/2016 09:27:45: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr_train.logrank3
MPI Rank 3: 08/04/2016 09:27:45: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:27:45: Build info: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: 		Built time: Aug  4 2016 06:18:04
MPI Rank 3: 08/04/2016 09:27:45: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 3: 08/04/2016 09:27:45: 		Build type: Release
MPI Rank 3: 08/04/2016 09:27:45: 		Build target: GPU
MPI Rank 3: 08/04/2016 09:27:45: 		With 1bit-SGD: no
MPI Rank 3: 08/04/2016 09:27:45: 		Math lib: mkl
MPI Rank 3: 08/04/2016 09:27:45: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 08/04/2016 09:27:45: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 08/04/2016 09:27:45: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 08/04/2016 09:27:45: 		Build Branch: HEAD
MPI Rank 3: 08/04/2016 09:27:45: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 3: 08/04/2016 09:27:45: 		Built by svcphil on dphaim-26-new
MPI Rank 3: 08/04/2016 09:27:45: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 3: 08/04/2016 09:27:45: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:27:45: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:27:45: GPU info:
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 08/04/2016 09:27:45: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: Running on cntk-muc02 at 2016/08/04 09:27:45
MPI Rank 3: 08/04/2016 09:27:45: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 09:27:45: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 09:27:45: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 08/04/2016 09:27:45: Commands: train
MPI Rank 3: 08/04/2016 09:27:45: Precision = "float"
MPI Rank 3: 08/04/2016 09:27:45: Using 1 CPU threads.
MPI Rank 3: 08/04/2016 09:27:45: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: 08/04/2016 09:27:45: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 08/04/2016 09:27:45: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: ##############################################################################
MPI Rank 3: 08/04/2016 09:27:45: #                                                                            #
MPI Rank 3: 08/04/2016 09:27:45: # Action "train"                                                             #
MPI Rank 3: 08/04/2016 09:27:45: #                                                                            #
MPI Rank 3: 08/04/2016 09:27:45: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: Creating virgin network.
MPI Rank 3: Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: Created model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:45: Training criterion node(s):
MPI Rank 3: 08/04/2016 09:27:45: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000D6B27A6250: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000D6C003B9C0: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000D6C003BB00: {[G Value[1 x 1]] }
MPI Rank 3: 000000D6C003BBA0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 000000D6C003BC40: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 000000D6C003BCE0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 000000D6C003BEC0: {[SIM Gradient[51 x *]] }
MPI Rank 3: 000000D6C003C000: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 000000D6C003C0A0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000D6C003C140: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000D6C003C320: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 000000D6C003C3C0: {[ce Gradient[1]] }
MPI Rank 3: 000000D6C003C5A0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000D6C003C6E0: {[N Value[1 x 1]] }
MPI Rank 3: 000000D6C003C780: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 000000D6C003CF00: {[SIM Value[51 x *]] }
MPI Rank 3: 000000D6C003CFA0: {[ce Value[1]] }
MPI Rank 3: 000000D6C003D0E0: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 000000D6C003D220: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 000000D6C003D2C0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 000000D6C003D400: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 000000D6C003D5E0: {[S Value[1 x 1]] }
MPI Rank 3: 000000D6C0D1AE30: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000D6C0D1B1F0: {[Keyword Value[49292 x *]] }
MPI Rank 3: 000000D6C0D1BA10: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000D6C0D1C0F0: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000D6C0D1C4B0: {[Query Value[49292 x *]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 08/04/2016 09:27:45: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:47: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:47: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.02 seconds
MPI Rank 3: 08/04/2016 09:27:53:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.32287788 * 10240; time = 5.5047s; samplesPerSecond = 1860.2
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:27:58:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.35470390 * 10240; time = 5.0962s; samplesPerSecond = 2009.3
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:28:00: Finished Epoch[ 1 of 3]: [Training] ce = 3.61601714 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=13.25s
MPI Rank 3: 08/04/2016 09:28:01: Final Results: Minibatch[1-26]: ce = 2.49916007 * 102399; perplexity = 12.17226578
MPI Rank 3: 08/04/2016 09:28:01: Finished Epoch[ 1 of 3]: [Validate] ce = 2.49916007 * 102399
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:04: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:04: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.00 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:28:09:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29653873 * 10240; time = 5.0906s; samplesPerSecond = 2011.6
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:28:14:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11679478 * 10240; time = 5.0552s; samplesPerSecond = 2025.6
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:28:16: Finished Epoch[ 2 of 3]: [Training] ce = 2.17577530 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=12.6324s
MPI Rank 3: 08/04/2016 09:28:17: Final Results: Minibatch[1-26]: ce = 1.97005576 * 102399; perplexity = 7.17107636
MPI Rank 3: 08/04/2016 09:28:17: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97005576 * 102399
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:20: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:20: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:28:25:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90347176 * 10240; time = 5.1287s; samplesPerSecond = 1996.6
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:28:30:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88304176 * 10240; time = 5.0201s; samplesPerSecond = 2039.8
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:28:32: Finished Epoch[ 3 of 3]: [Training] ce = 1.88563949 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=12.6474s
MPI Rank 3: 08/04/2016 09:28:33: Final Results: Minibatch[1-26]: ce = 1.80751073 * 102399; perplexity = 6.09525581
MPI Rank 3: 08/04/2016 09:28:33: Finished Epoch[ 3 of 3]: [Validate] ce = 1.80751073 * 102399
MPI Rank 3: 08/04/2016 09:28:36: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:36: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:36: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu DeviceId=0 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 08/04/2016 09:28:39: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr_train.logrank0
MPI Rank 0: 08/04/2016 09:28:39: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:28:39: Build info: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: 		Built time: Aug  4 2016 06:18:04
MPI Rank 0: 08/04/2016 09:28:39: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 0: 08/04/2016 09:28:39: 		Build type: Release
MPI Rank 0: 08/04/2016 09:28:39: 		Build target: GPU
MPI Rank 0: 08/04/2016 09:28:39: 		With 1bit-SGD: no
MPI Rank 0: 08/04/2016 09:28:39: 		Math lib: mkl
MPI Rank 0: 08/04/2016 09:28:39: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 08/04/2016 09:28:39: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 08/04/2016 09:28:39: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 08/04/2016 09:28:39: 		Build Branch: HEAD
MPI Rank 0: 08/04/2016 09:28:39: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 0: 08/04/2016 09:28:39: 		Built by svcphil on dphaim-26-new
MPI Rank 0: 08/04/2016 09:28:39: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 0: 08/04/2016 09:28:39: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:28:39: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:28:39: GPU info:
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 08/04/2016 09:28:39: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: Running on cntk-muc02 at 2016/08/04 09:28:39
MPI Rank 0: 08/04/2016 09:28:39: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 09:28:39: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 09:28:39: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: DeviceId=0
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=0
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=0
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 08/04/2016 09:28:39: Commands: train
MPI Rank 0: 08/04/2016 09:28:39: Precision = "float"
MPI Rank 0: 08/04/2016 09:28:39: Using 1 CPU threads.
MPI Rank 0: 08/04/2016 09:28:39: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 0: 08/04/2016 09:28:39: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 08/04/2016 09:28:39: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: ##############################################################################
MPI Rank 0: 08/04/2016 09:28:39: #                                                                            #
MPI Rank 0: 08/04/2016 09:28:39: # Action "train"                                                             #
MPI Rank 0: 08/04/2016 09:28:39: #                                                                            #
MPI Rank 0: 08/04/2016 09:28:39: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using GPU 0
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:39: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:41: Loaded model with 21 nodes on GPU 0.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:41: Training criterion node(s):
MPI Rank 0: 08/04/2016 09:28:41: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 000000F66D3E0FE0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 000000F679774D60: {[G Value[1 x 1]] }
MPI Rank 0: 000000F679775260: {[WD1 Value[64 x 288]] }
MPI Rank 0: 000000F679775940: {[Query Value[49292 x *1]] }
MPI Rank 0: 000000F679775B20: {[N Value[1 x 1]] }
MPI Rank 0: 000000F679775D00: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 000000F679775EE0: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 000000F6797760C0: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 000000F679776340: {[S Value[1 x 1]] }
MPI Rank 0: 000000F679776660: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 000000F67986FC50: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 000000F67986FCF0: {[ce Value[1]] }
MPI Rank 0: 000000F67986FE30: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 000000F67986FED0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 000000F6798701F0: {[ce Gradient[1]] }
MPI Rank 0: 000000F679870330: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 000000F6798703D0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 000000F679870510: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 000000F6798708D0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 000000F679870970: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 000000F679870F10: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 000000F679870FB0: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 000000F679871550: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 000000F6798715F0: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 000000F679871690: {[SIM Value[51 x *1]] }
MPI Rank 0: 000000F679871730: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 000000F6798717D0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 08/04/2016 09:28:41: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:46: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:28:46: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:51:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87577038 * 10240; time = 5.5300s; samplesPerSecond = 1851.7
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:56:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79361134 * 10240; time = 5.0646s; samplesPerSecond = 2021.9
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.01 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.01 seconds
MPI Rank 0: 08/04/2016 09:28:59: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=13.3216s
MPI Rank 0: 08/04/2016 09:28:59: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241645
MPI Rank 0: 08/04/2016 09:28:59: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 0: 08/04/2016 09:29:01: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net'
MPI Rank 0: 08/04/2016 09:29:02: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:29:02: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:29:02: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 08/04/2016 09:28:39: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr_train.logrank1
MPI Rank 1: 08/04/2016 09:28:39: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:28:39: Build info: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: 		Built time: Aug  4 2016 06:18:04
MPI Rank 1: 08/04/2016 09:28:39: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 1: 08/04/2016 09:28:39: 		Build type: Release
MPI Rank 1: 08/04/2016 09:28:39: 		Build target: GPU
MPI Rank 1: 08/04/2016 09:28:39: 		With 1bit-SGD: no
MPI Rank 1: 08/04/2016 09:28:39: 		Math lib: mkl
MPI Rank 1: 08/04/2016 09:28:39: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 08/04/2016 09:28:39: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 08/04/2016 09:28:39: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 08/04/2016 09:28:39: 		Build Branch: HEAD
MPI Rank 1: 08/04/2016 09:28:39: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 1: 08/04/2016 09:28:39: 		Built by svcphil on dphaim-26-new
MPI Rank 1: 08/04/2016 09:28:39: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 1: 08/04/2016 09:28:39: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:28:39: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:28:39: GPU info:
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 08/04/2016 09:28:39: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: Running on cntk-muc02 at 2016/08/04 09:28:39
MPI Rank 1: 08/04/2016 09:28:39: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 09:28:39: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 09:28:39: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: DeviceId=0
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=0
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=0
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 08/04/2016 09:28:39: Commands: train
MPI Rank 1: 08/04/2016 09:28:39: Precision = "float"
MPI Rank 1: 08/04/2016 09:28:39: Using 1 CPU threads.
MPI Rank 1: 08/04/2016 09:28:39: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 1: 08/04/2016 09:28:39: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 08/04/2016 09:28:39: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: ##############################################################################
MPI Rank 1: 08/04/2016 09:28:39: #                                                                            #
MPI Rank 1: 08/04/2016 09:28:39: # Action "train"                                                             #
MPI Rank 1: 08/04/2016 09:28:39: #                                                                            #
MPI Rank 1: 08/04/2016 09:28:39: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using GPU 0
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:39: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:41: Loaded model with 21 nodes on GPU 0.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:41: Training criterion node(s):
MPI Rank 1: 08/04/2016 09:28:41: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 000000729B1FE760: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 000000729B1FE940: {[ce Value[1]] }
MPI Rank 1: 000000729B1FE9E0: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 000000729B1FEBC0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 000000729B1FEC60: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 000000729B1FEEE0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 000000729B1FF020: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 000000729B1FF0C0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 000000729B1FF2A0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 000000729B1FF660: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 000000729B1FFA20: {[SIM Value[51 x *1]] }
MPI Rank 1: 000000729B1FFAC0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 000000729B1FFC00: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 000000729B1FFDE0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 000000729B1FFE80: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 000000729B1FFF20: {[ce Gradient[1]] }
MPI Rank 1: 000000729B200100: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 000000729B83EDE0: {[N Value[1 x 1]] }
MPI Rank 1: 000000729B83F240: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 000000729B83F420: {[S Value[1 x 1]] }
MPI Rank 1: 000000729B83F880: {[G Value[1 x 1]] }
MPI Rank 1: 000000729B83F920: {[Query Value[49292 x *1]] }
MPI Rank 1: 000000729B83FBA0: {[WD1 Value[64 x 288]] }
MPI Rank 1: 000000729B83FE20: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 000000729B8400A0: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 000000729B8401E0: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 00000072F3EEBC70: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 08/04/2016 09:28:41: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:46: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:28:46: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:51:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93745022 * 10240; time = 5.5060s; samplesPerSecond = 1859.8
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:56:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89571209 * 10240; time = 5.0658s; samplesPerSecond = 2021.4
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.01 seconds
MPI Rank 1: 08/04/2016 09:28:59: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=13.322s
MPI Rank 1: 08/04/2016 09:28:59: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241645
MPI Rank 1: 08/04/2016 09:28:59: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 1: 08/04/2016 09:29:02: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:29:02: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:29:02: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 08/04/2016 09:28:40: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr_train.logrank2
MPI Rank 2: 08/04/2016 09:28:40: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:28:40: Build info: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: 		Built time: Aug  4 2016 06:18:04
MPI Rank 2: 08/04/2016 09:28:40: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 2: 08/04/2016 09:28:40: 		Build type: Release
MPI Rank 2: 08/04/2016 09:28:40: 		Build target: GPU
MPI Rank 2: 08/04/2016 09:28:40: 		With 1bit-SGD: no
MPI Rank 2: 08/04/2016 09:28:40: 		Math lib: mkl
MPI Rank 2: 08/04/2016 09:28:40: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 08/04/2016 09:28:40: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 08/04/2016 09:28:40: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 08/04/2016 09:28:40: 		Build Branch: HEAD
MPI Rank 2: 08/04/2016 09:28:40: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 2: 08/04/2016 09:28:40: 		Built by svcphil on dphaim-26-new
MPI Rank 2: 08/04/2016 09:28:40: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 2: 08/04/2016 09:28:40: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:28:40: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:28:40: GPU info:
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 08/04/2016 09:28:40: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: Running on cntk-muc02 at 2016/08/04 09:28:40
MPI Rank 2: 08/04/2016 09:28:40: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 09:28:40: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 09:28:40: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: DeviceId=0
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=0
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=0
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 08/04/2016 09:28:40: Commands: train
MPI Rank 2: 08/04/2016 09:28:40: Precision = "float"
MPI Rank 2: 08/04/2016 09:28:40: Using 1 CPU threads.
MPI Rank 2: 08/04/2016 09:28:40: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 2: 08/04/2016 09:28:40: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 08/04/2016 09:28:40: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: ##############################################################################
MPI Rank 2: 08/04/2016 09:28:40: #                                                                            #
MPI Rank 2: 08/04/2016 09:28:40: # Action "train"                                                             #
MPI Rank 2: 08/04/2016 09:28:40: #                                                                            #
MPI Rank 2: 08/04/2016 09:28:40: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using GPU 0
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:40: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:42: Loaded model with 21 nodes on GPU 0.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:42: Training criterion node(s):
MPI Rank 2: 08/04/2016 09:28:42: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 000000BFA6CD1D60: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 000000BFB571F420: {[WD1 Value[64 x 288]] }
MPI Rank 2: 000000BFB571F560: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 000000BFB571F740: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 000000BFB571FA60: {[G Value[1 x 1]] }
MPI Rank 2: 000000BFB5720280: {[Query Value[49292 x *1]] }
MPI Rank 2: 000000BFB57205A0: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 000000BFB5720820: {[N Value[1 x 1]] }
MPI Rank 2: 000000BFB5720BE0: {[S Value[1 x 1]] }
MPI Rank 2: 000000BFB5721040: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 000000BFBF1D7AD0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 000000BFBF1D7B70: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 000000BFBF1D7CB0: {[ce Gradient[1]] }
MPI Rank 2: 000000BFBF1D7E90: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 000000BFBF1D7F30: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 000000BFBF1D8250: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 000000BFBF1D82F0: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 000000BFBF1D8430: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 000000BFBF1D84D0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 000000BFBF1D8A70: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 000000BFBF1D8BB0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 000000BFBF1D8E30: {[SIM Value[51 x *1]] }
MPI Rank 2: 000000BFBF1D8ED0: {[ce Value[1]] }
MPI Rank 2: 000000BFBF1D8F70: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 000000BFBF1D90B0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 000000BFBF1D91F0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 000000BFBF1D9970: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 08/04/2016 09:28:42: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:46: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:28:46: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:51:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96188030 * 10240; time = 5.5469s; samplesPerSecond = 1846.1
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:56:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90950069 * 10240; time = 5.0663s; samplesPerSecond = 2021.2
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.01 seconds
MPI Rank 2: 08/04/2016 09:28:59: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=13.3214s
MPI Rank 2: 08/04/2016 09:28:59: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241645
MPI Rank 2: 08/04/2016 09:28:59: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 2: 08/04/2016 09:29:02: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:29:02: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:29:02: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 08/04/2016 09:28:40: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr_train.logrank3
MPI Rank 3: 08/04/2016 09:28:40: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:28:40: Build info: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:40: 		Built time: Aug  4 2016 06:18:04
MPI Rank 3: 08/04/2016 09:28:40: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 3: 08/04/2016 09:28:40: 		Build type: Release
MPI Rank 3: 08/04/2016 09:28:40: 		Build target: GPU
MPI Rank 3: 08/04/2016 09:28:40: 		With 1bit-SGD: no
MPI Rank 3: 08/04/2016 09:28:40: 		Math lib: mkl
MPI Rank 3: 08/04/2016 09:28:40: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 08/04/2016 09:28:40: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 08/04/2016 09:28:40: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 08/04/2016 09:28:40: 		Build Branch: HEAD
MPI Rank 3: 08/04/2016 09:28:40: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 3: 08/04/2016 09:28:40: 		Built by svcphil on dphaim-26-new
MPI Rank 3: 08/04/2016 09:28:40: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 3: 08/04/2016 09:28:40: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:28:41: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:28:41: GPU info:
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 08/04/2016 09:28:41: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: Running on cntk-muc02 at 2016/08/04 09:28:41
MPI Rank 3: 08/04/2016 09:28:41: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu  DeviceId=0  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 09:28:41: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 09:28:41: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: DeviceId=0
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=0
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=0
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 08/04/2016 09:28:41: Commands: train
MPI Rank 3: 08/04/2016 09:28:41: Precision = "float"
MPI Rank 3: 08/04/2016 09:28:41: Using 1 CPU threads.
MPI Rank 3: 08/04/2016 09:28:41: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net
MPI Rank 3: 08/04/2016 09:28:41: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 08/04/2016 09:28:41: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: ##############################################################################
MPI Rank 3: 08/04/2016 09:28:41: #                                                                            #
MPI Rank 3: 08/04/2016 09:28:41: # Action "train"                                                             #
MPI Rank 3: 08/04/2016 09:28:41: #                                                                            #
MPI Rank 3: 08/04/2016 09:28:41: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using GPU 0
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:41: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_gpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:44: Loaded model with 21 nodes on GPU 0.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:44: Training criterion node(s):
MPI Rank 3: 08/04/2016 09:28:44: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000B71D04FB50: {[Query Value[49292 x *1]] }
MPI Rank 3: 000000B71D04FE70: {[S Value[1 x 1]] }
MPI Rank 3: 000000B71D050230: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000B71D0502D0: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 000000B71D050410: {[N Value[1 x 1]] }
MPI Rank 3: 000000B71D050870: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000B71D050910: {[G Value[1 x 1]] }
MPI Rank 3: 000000B71D050FF0: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000B71D051310: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000B71D240C70: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 000000B71D240DB0: {[SIM Value[51 x *1]] }
MPI Rank 3: 000000B71D240E50: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 000000B71D241210: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 000000B71D2413F0: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 000000B71D241490: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 000000B71D241670: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000B71D241850: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 000000B71D241AD0: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 000000B71D242570: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 000000B71D242610: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000B71D2426B0: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 000000B71D242750: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 000000B71D2427F0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 000000B71D242890: {[ce Gradient[1]] }
MPI Rank 3: 000000B71D242930: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 000000B71D242B10: {[ce Value[1]] }
MPI Rank 3: 000000B7723C6C60: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 08/04/2016 09:28:44: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:46: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:28:46: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.03 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.02 seconds
MPI Rank 3: 08/04/2016 09:28:51:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91174278 * 10240; time = 5.5558s; samplesPerSecond = 1843.1
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.02 seconds
MPI Rank 3: 08/04/2016 09:28:56:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91370468 * 10240; time = 5.0656s; samplesPerSecond = 2021.5
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.01 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.01 seconds
MPI Rank 3: 08/04/2016 09:28:59: Finished Epoch[ 3 of 3]: [Training] ce = 1.88974288 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=13.3222s
MPI Rank 3: 08/04/2016 09:28:59: Final Results: Minibatch[1-26]: ce = 1.81846898 * 102399; perplexity = 6.16241645
MPI Rank 3: 08/04/2016 09:28:59: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81846898 * 102399
MPI Rank 3: 08/04/2016 09:29:02: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:29:02: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:29:02: __COMPLETED__
MPI Rank 3: ~MPIWrapper