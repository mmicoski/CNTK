CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU W3550 @ 3.07GHz
    Hardware threads: 4
    Total Memory: 12580388 kB
-------------------------------------------------------------------
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 0 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
mpihelper: we are cog 1 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 08/04/2016 09:25:27: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr_train.logrank0
MPI Rank 0: 08/04/2016 09:25:27: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:25:27: Build info: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:27: 		Built time: Aug  4 2016 06:18:04
MPI Rank 0: 08/04/2016 09:25:27: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 0: 08/04/2016 09:25:27: 		Build type: Release
MPI Rank 0: 08/04/2016 09:25:27: 		Build target: GPU
MPI Rank 0: 08/04/2016 09:25:27: 		With 1bit-SGD: no
MPI Rank 0: 08/04/2016 09:25:27: 		Math lib: mkl
MPI Rank 0: 08/04/2016 09:25:27: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 08/04/2016 09:25:27: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 08/04/2016 09:25:27: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 08/04/2016 09:25:27: 		Build Branch: HEAD
MPI Rank 0: 08/04/2016 09:25:27: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 0: 08/04/2016 09:25:27: 		Built by svcphil on dphaim-26-new
MPI Rank 0: 08/04/2016 09:25:27: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 0: 08/04/2016 09:25:27: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:25:28: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:25:28: GPU info:
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 08/04/2016 09:25:28: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: Running on cntk-muc02 at 2016/08/04 09:25:28
MPI Rank 0: 08/04/2016 09:25:28: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 09:25:28: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 09:25:28: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 08/04/2016 09:25:28: Commands: train
MPI Rank 0: 08/04/2016 09:25:28: Precision = "float"
MPI Rank 0: 08/04/2016 09:25:28: Using 1 CPU threads.
MPI Rank 0: 08/04/2016 09:25:28: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 08/04/2016 09:25:28: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 08/04/2016 09:25:28: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: ##############################################################################
MPI Rank 0: 08/04/2016 09:25:28: #                                                                            #
MPI Rank 0: 08/04/2016 09:25:28: # Action "train"                                                             #
MPI Rank 0: 08/04/2016 09:25:28: #                                                                            #
MPI Rank 0: 08/04/2016 09:25:28: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: Creating virgin network.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: Created model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:28: Training criterion node(s):
MPI Rank 0: 08/04/2016 09:25:28: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 0: 0000006D2D49ADA0: {[G Value[1 x 1]] }
MPI Rank 0: 0000006D2D49AEE0: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 0: 0000006D2D49AF80: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 0: 0000006D2D49B2A0: {[WD1_D Gradient[64 x *]] }
MPI Rank 0: 0000006D2D49B480: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 0: 0000006D2D49B520: {[ce Value[1]] }
MPI Rank 0: 0000006D2D49B660: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 0000006D2D49B840: {[SIM Value[51 x *]] }
MPI Rank 0: 0000006D2D49B8E0: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 0: 0000006D2D49B980: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 0: 0000006D2D49BA20: {[WQ0_Q Value[288 x *]] }
MPI Rank 0: 0000006D2D49BB60: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 0: 0000006D2D49BCA0: {[N Value[1 x 1]] }
MPI Rank 0: 0000006D2D49BD40: {[ce Gradient[1]] }
MPI Rank 0: 0000006D2D49BE80: {[S Value[1 x 1]] }
MPI Rank 0: 0000006D2D49BFC0: {[Keyword Value[49292 x *]] }
MPI Rank 0: 0000006D2D49C2E0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 0: 0000006D2D49C420: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 0: 0000006D2D49C600: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 0: 0000006D2D49C740: {[SIM Gradient[51 x *]] }
MPI Rank 0: 0000006D2D49C7E0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 0000006D2D49C880: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 0: 0000006D4A7862A0: {[Query Value[49292 x *]] }
MPI Rank 0: 0000006D4A786520: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 0000006D4A786660: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 0000006D4A786C00: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 0000006D4A786DE0: {[WD1 Value[64 x 288]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 08/04/2016 09:25:28: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:32: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:25:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.03 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.07 seconds
MPI Rank 0: 08/04/2016 09:25:42:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.41944122 * 10240; time = 10.0384s; samplesPerSecond = 1020.1
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.06 seconds
MPI Rank 0: 08/04/2016 09:25:52:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38406754 * 10240; time = 9.8041s; samplesPerSecond = 1044.5
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 0.05 seconds
MPI Rank 0: 08/04/2016 09:25:57: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.8886s
MPI Rank 0: 08/04/2016 09:25:59: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 0: 08/04/2016 09:25:59: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 0: 08/04/2016 09:26:00: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net.1'
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:26:02: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:26:02: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.11 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.07 seconds
MPI Rank 0: 08/04/2016 09:26:12:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29922523 * 10240; time = 9.7327s; samplesPerSecond = 1052.1
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 0.07 seconds
MPI Rank 0: 08/04/2016 09:26:22:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.08742409 * 10240; time = 9.7691s; samplesPerSecond = 1048.2
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 0.06 seconds
MPI Rank 0: 08/04/2016 09:26:26: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=24.1974s
MPI Rank 0: 08/04/2016 09:26:28: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 0: 08/04/2016 09:26:28: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 0: 08/04/2016 09:26:30: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net.2'
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:26:31: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:26:31: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.08 seconds
MPI Rank 0: 08/04/2016 09:26:41:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90042439 * 10240; time = 10.1254s; samplesPerSecond = 1011.3
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.08 seconds
MPI Rank 0: 08/04/2016 09:26:51:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.85719700 * 10240; time = 9.9582s; samplesPerSecond = 1028.3
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.85 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.98 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.98 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 0.08 seconds
MPI Rank 0: 08/04/2016 09:26:56: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.059s
MPI Rank 0: 08/04/2016 09:26:58: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 0: 08/04/2016 09:26:58: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 0: 08/04/2016 09:27:00: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 08/04/2016 09:27:01: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:01: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:01: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 08/04/2016 09:25:28: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr_train.logrank1
MPI Rank 1: 08/04/2016 09:25:28: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:25:28: Build info: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: 		Built time: Aug  4 2016 06:18:04
MPI Rank 1: 08/04/2016 09:25:28: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 1: 08/04/2016 09:25:28: 		Build type: Release
MPI Rank 1: 08/04/2016 09:25:28: 		Build target: GPU
MPI Rank 1: 08/04/2016 09:25:28: 		With 1bit-SGD: no
MPI Rank 1: 08/04/2016 09:25:28: 		Math lib: mkl
MPI Rank 1: 08/04/2016 09:25:28: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 08/04/2016 09:25:28: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 08/04/2016 09:25:28: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 08/04/2016 09:25:28: 		Build Branch: HEAD
MPI Rank 1: 08/04/2016 09:25:28: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 1: 08/04/2016 09:25:28: 		Built by svcphil on dphaim-26-new
MPI Rank 1: 08/04/2016 09:25:28: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 1: 08/04/2016 09:25:28: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:25:28: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:25:28: GPU info:
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 08/04/2016 09:25:28: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: Running on cntk-muc02 at 2016/08/04 09:25:28
MPI Rank 1: 08/04/2016 09:25:28: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 09:25:28: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 09:25:28: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 08/04/2016 09:25:28: Commands: train
MPI Rank 1: 08/04/2016 09:25:28: Precision = "float"
MPI Rank 1: 08/04/2016 09:25:28: Using 1 CPU threads.
MPI Rank 1: 08/04/2016 09:25:28: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 08/04/2016 09:25:28: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 08/04/2016 09:25:28: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: ##############################################################################
MPI Rank 1: 08/04/2016 09:25:28: #                                                                            #
MPI Rank 1: 08/04/2016 09:25:28: # Action "train"                                                             #
MPI Rank 1: 08/04/2016 09:25:28: #                                                                            #
MPI Rank 1: 08/04/2016 09:25:28: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:28: Creating virgin network.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:29: Created model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:29: Training criterion node(s):
MPI Rank 1: 08/04/2016 09:25:29: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 1: 000000051582A210: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 000000051582A490: {[WD1 Value[64 x 288]] }
MPI Rank 1: 000000051582A5D0: {[Query Value[49292 x *]] }
MPI Rank 1: 000000051582A710: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 000000051582A850: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 0000000518A4BB60: {[S Value[1 x 1]] }
MPI Rank 1: 0000000518A4BC00: {[N Value[1 x 1]] }
MPI Rank 1: 0000000518A4BCA0: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 1: 0000000518A4BF20: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 1: 0000000518A4C060: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 1: 0000000518A4C100: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 1: 0000000518A4C1A0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 1: 0000000518A4C6A0: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 1: 0000000518A4C740: {[SIM Gradient[51 x *]] }
MPI Rank 1: 0000000518A4C7E0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 1: 0000000518A4C9C0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 0000000518A4CA60: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 1: 0000000518A4CCE0: {[WD1_D Gradient[64 x *]] }
MPI Rank 1: 0000000518A4CE20: {[Keyword Value[49292 x *]] }
MPI Rank 1: 0000000518A4D000: {[WQ0_Q Value[288 x *]] }
MPI Rank 1: 0000000518A4D0A0: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 0000000518A4D280: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 1: 0000000518A4D500: {[SIM Value[51 x *]] }
MPI Rank 1: 0000000518A4D5A0: {[G Value[1 x 1]] }
MPI Rank 1: 0000000518A4D820: {[ce Gradient[1]] }
MPI Rank 1: 0000000518A4D8C0: {[ce Value[1]] }
MPI Rank 1: 0000000518A4D960: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 08/04/2016 09:25:29: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:32: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:25:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.13 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.14 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.12 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.10 seconds
MPI Rank 1: 08/04/2016 09:25:42:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.42829742 * 10240; time = 9.9728s; samplesPerSecond = 1026.8
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 0.11 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.10 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.08 seconds
MPI Rank 1: 08/04/2016 09:25:52:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38771057 * 10240; time = 9.8590s; samplesPerSecond = 1038.6
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.07 seconds
MPI Rank 1: 08/04/2016 09:25:57: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.8886s
MPI Rank 1: 08/04/2016 09:25:59: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 1: 08/04/2016 09:25:59: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:26:02: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:26:02: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.07 seconds
MPI Rank 1: 08/04/2016 09:26:12:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.34065857 * 10240; time = 9.7039s; samplesPerSecond = 1055.2
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.79 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.79 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 0.06 seconds
MPI Rank 1: 08/04/2016 09:26:22:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11009483 * 10240; time = 9.7123s; samplesPerSecond = 1054.3
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 0.05 seconds
MPI Rank 1: 08/04/2016 09:26:26: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=24.1971s
MPI Rank 1: 08/04/2016 09:26:28: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 1: 08/04/2016 09:26:28: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:26:31: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:26:31: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.02 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.03 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.06 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.66 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.09 seconds
MPI Rank 1: 08/04/2016 09:26:41:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.93978710 * 10240; time = 10.1205s; samplesPerSecond = 1011.8
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 0.08 seconds
MPI Rank 1: 08/04/2016 09:26:51:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.86772938 * 10240; time = 9.9581s; samplesPerSecond = 1028.3
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.95 seconds , average latency = 0.08 seconds
MPI Rank 1: 08/04/2016 09:26:56: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.0587s
MPI Rank 1: 08/04/2016 09:26:58: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 1: 08/04/2016 09:26:58: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 1: 08/04/2016 09:27:01: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:01: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:01: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 08/04/2016 09:25:28: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr_train.logrank2
MPI Rank 2: 08/04/2016 09:25:28: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:25:28: Build info: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:28: 		Built time: Aug  4 2016 06:18:04
MPI Rank 2: 08/04/2016 09:25:28: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 2: 08/04/2016 09:25:28: 		Build type: Release
MPI Rank 2: 08/04/2016 09:25:28: 		Build target: GPU
MPI Rank 2: 08/04/2016 09:25:28: 		With 1bit-SGD: no
MPI Rank 2: 08/04/2016 09:25:28: 		Math lib: mkl
MPI Rank 2: 08/04/2016 09:25:28: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 08/04/2016 09:25:28: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 08/04/2016 09:25:28: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 08/04/2016 09:25:28: 		Build Branch: HEAD
MPI Rank 2: 08/04/2016 09:25:28: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 2: 08/04/2016 09:25:28: 		Built by svcphil on dphaim-26-new
MPI Rank 2: 08/04/2016 09:25:28: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 2: 08/04/2016 09:25:28: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:25:29: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:25:29: GPU info:
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 08/04/2016 09:25:29: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: Running on cntk-muc02 at 2016/08/04 09:25:29
MPI Rank 2: 08/04/2016 09:25:29: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 09:25:29: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 09:25:29: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 08/04/2016 09:25:29: Commands: train
MPI Rank 2: 08/04/2016 09:25:29: Precision = "float"
MPI Rank 2: 08/04/2016 09:25:29: Using 1 CPU threads.
MPI Rank 2: 08/04/2016 09:25:29: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 08/04/2016 09:25:29: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 08/04/2016 09:25:29: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: ##############################################################################
MPI Rank 2: 08/04/2016 09:25:29: #                                                                            #
MPI Rank 2: 08/04/2016 09:25:29: # Action "train"                                                             #
MPI Rank 2: 08/04/2016 09:25:29: #                                                                            #
MPI Rank 2: 08/04/2016 09:25:29: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: Creating virgin network.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: Created model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:29: Training criterion node(s):
MPI Rank 2: 08/04/2016 09:25:29: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 2: 0000002F5E45A5C0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 2: 0000002F5E45A660: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 2: 0000002F5E45A700: {[N Value[1 x 1]] }
MPI Rank 2: 0000002F5E45A840: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 0000002F5E45A8E0: {[ce Gradient[1]] }
MPI Rank 2: 0000002F5E45AB60: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 2: 0000002F5E45ACA0: {[SIM Gradient[51 x *]] }
MPI Rank 2: 0000002F5E45AD40: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 2: 0000002F5E45B100: {[S Value[1 x 1]] }
MPI Rank 2: 0000002F5E45B380: {[WQ0_Q Value[288 x *]] }
MPI Rank 2: 0000002F5E45B420: {[Keyword Value[49292 x *]] }
MPI Rank 2: 0000002F5E45B4C0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 2: 0000002F5E45B560: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 2: 0000002F5E45B920: {[G Value[1 x 1]] }
MPI Rank 2: 0000002F5E45BB00: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 2: 0000002F5E45BBA0: {[ce Value[1]] }
MPI Rank 2: 0000002F5E45BC40: {[WD1_D Gradient[64 x *]] }
MPI Rank 2: 0000002F5E45BE20: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 2: 0000002F5E45BEC0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 2: 0000002F5E45BF60: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 0000002F5E45C1E0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 2: 0000002F5E45C3C0: {[SIM Value[51 x *]] }
MPI Rank 2: 0000002F7B7AC0D0: {[Query Value[49292 x *]] }
MPI Rank 2: 0000002F7B7AC710: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 0000002F7B7AC8F0: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 0000002F7B7AC990: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 0000002F7B7ACF30: {[WD1 Value[64 x 288]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 08/04/2016 09:25:29: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:32: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:25:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.06 seconds
MPI Rank 2: 08/04/2016 09:25:42:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.44101372 * 10240; time = 9.9830s; samplesPerSecond = 1025.7
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.96 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 0.06 seconds
MPI Rank 2: 08/04/2016 09:25:52:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.39723701 * 10240; time = 9.8867s; samplesPerSecond = 1035.7
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 0.07 seconds
MPI Rank 2: 08/04/2016 09:25:57: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.8881s
MPI Rank 2: 08/04/2016 09:25:59: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 2: 08/04/2016 09:25:59: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:26:02: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:26:02: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.07 seconds
MPI Rank 2: 08/04/2016 09:26:12:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.34435501 * 10240; time = 9.7019s; samplesPerSecond = 1055.5
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.07 seconds
MPI Rank 2: 08/04/2016 09:26:22:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.13005295 * 10240; time = 9.7692s; samplesPerSecond = 1048.2
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 0.07 seconds
MPI Rank 2: 08/04/2016 09:26:26: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=24.1958s
MPI Rank 2: 08/04/2016 09:26:28: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 2: 08/04/2016 09:26:28: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:26:31: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:26:31: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.02 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.03 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.63 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.80 seconds , average latency = 0.08 seconds
MPI Rank 2: 08/04/2016 09:26:41:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.95727177 * 10240; time = 10.0930s; samplesPerSecond = 1014.6
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 0.09 seconds
MPI Rank 2: 08/04/2016 09:26:51:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.88702602 * 10240; time = 9.9013s; samplesPerSecond = 1034.2
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.95 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.11 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 2.23 seconds , average latency = 0.10 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.23 seconds , average latency = 0.09 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.23 seconds , average latency = 0.09 seconds
MPI Rank 2: 08/04/2016 09:26:56: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.0578s
MPI Rank 2: 08/04/2016 09:26:58: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 2: 08/04/2016 09:26:58: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 2: 08/04/2016 09:27:01: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:01: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:01: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 08/04/2016 09:25:29: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 3: 08/04/2016 09:25:29: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:25:29: Build info: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: 		Built time: Aug  4 2016 06:18:04
MPI Rank 3: 08/04/2016 09:25:29: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 3: 08/04/2016 09:25:29: 		Build type: Release
MPI Rank 3: 08/04/2016 09:25:29: 		Build target: GPU
MPI Rank 3: 08/04/2016 09:25:29: 		With 1bit-SGD: no
MPI Rank 3: 08/04/2016 09:25:29: 		Math lib: mkl
MPI Rank 3: 08/04/2016 09:25:29: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 08/04/2016 09:25:29: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 08/04/2016 09:25:29: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 08/04/2016 09:25:29: 		Build Branch: HEAD
MPI Rank 3: 08/04/2016 09:25:29: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 3: 08/04/2016 09:25:29: 		Built by svcphil on dphaim-26-new
MPI Rank 3: 08/04/2016 09:25:29: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 3: 08/04/2016 09:25:29: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:25:29: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:25:29: GPU info:
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 08/04/2016 09:25:29: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: Running on cntk-muc02 at 2016/08/04 09:25:29
MPI Rank 3: 08/04/2016 09:25:29: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 09:25:29: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 09:25:29: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 08/04/2016 09:25:29: Commands: train
MPI Rank 3: 08/04/2016 09:25:29: Precision = "float"
MPI Rank 3: 08/04/2016 09:25:29: Using 1 CPU threads.
MPI Rank 3: 08/04/2016 09:25:29: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 08/04/2016 09:25:29: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 08/04/2016 09:25:29: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: ##############################################################################
MPI Rank 3: 08/04/2016 09:25:29: #                                                                            #
MPI Rank 3: 08/04/2016 09:25:29: # Action "train"                                                             #
MPI Rank 3: 08/04/2016 09:25:29: #                                                                            #
MPI Rank 3: 08/04/2016 09:25:29: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:29: Creating virgin network.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *] -> [288 x *]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *] -> [288 x *]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *] -> [64 x *]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *] -> [64 x *]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *], [64 x *], [1 x 1], [1 x 1] -> [51 x *]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *] -> [51 x 1 x *]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *], [51 x 1 x *] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:30: Created model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:30: Training criterion node(s):
MPI Rank 3: 08/04/2016 09:25:30: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *]] [N Gradient[1 x 1]] [Query Gradient[49292 x *]] [S Gradient[1 x 1]] }
MPI Rank 3: 000000C94D765F80: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 000000C94D766660: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 000000C94D7668E0: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 000000C94D766980: {[WD1 Value[64 x 288]] }
MPI Rank 3: 000000C94D766AC0: {[Query Value[49292 x *]] }
MPI Rank 3: 000000C94E0C7A90: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *]] }
MPI Rank 3: 000000C94E0C7B30: {[SIM_Scale Value[51 x 1 x *]] [WQ0_Q_Tanh Gradient[288 x *]] [WQ1_Q_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000C94E0C7EF0: {[G Value[1 x 1]] }
MPI Rank 3: 000000C94E0C7F90: {[WQ0_Q Value[288 x *]] }
MPI Rank 3: 000000C94E0C8030: {[SIM Gradient[51 x *]] }
MPI Rank 3: 000000C94E0C85D0: {[WQ0_Q Gradient[288 x *]] [WQ1_Q Value[64 x *]] }
MPI Rank 3: 000000C94E0C87B0: {[WD0_D_Tanh Value[288 x *]] }
MPI Rank 3: 000000C94E0C8850: {[ce Gradient[1]] }
MPI Rank 3: 000000C94E0C88F0: {[DSSMLabel Value[51 x 1 x *]] }
MPI Rank 3: 000000C94E0C8990: {[Keyword Value[49292 x *]] }
MPI Rank 3: 000000C94E0C8A30: {[WD1_D Gradient[64 x *]] }
MPI Rank 3: 000000C94E0C8AD0: {[WD0_D Gradient[288 x *]] [WD1_D Value[64 x *]] }
MPI Rank 3: 000000C94E0C8B70: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 000000C94E0C8D50: {[N Value[1 x 1]] }
MPI Rank 3: 000000C94E0C8DF0: {[ce Value[1]] }
MPI Rank 3: 000000C94E0C8E90: {[SIM Value[51 x *]] }
MPI Rank 3: 000000C94E0C91B0: {[S Value[1 x 1]] }
MPI Rank 3: 000000C94E0C9390: {[WQ0_Q_Tanh Value[288 x *]] }
MPI Rank 3: 000000C94E0C9570: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *]] }
MPI Rank 3: 000000C94E0C9610: {[WD0_D Value[288 x *]] [WQ1_Q Gradient[64 x *]] }
MPI Rank 3: 000000C94E0C9750: {[SIM_Scale Gradient[51 x 1 x *]] [WD0_D_Tanh Gradient[288 x *]] [WD1_D_Tanh Gradient[64 x *]] }
MPI Rank 3: 000000C94E0C97F0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 08/04/2016 09:25:30: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:32: Starting Epoch 1: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:25:32: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.13 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.53 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.53 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.06 seconds
MPI Rank 3: 08/04/2016 09:25:42:  Epoch[ 1 of 3]-Minibatch[   1-  10, 40.00%]: ce = 4.43899651 * 10240; time = 9.9957s; samplesPerSecond = 1024.4
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 0.07 seconds
MPI Rank 3: 08/04/2016 09:25:52:  Epoch[ 1 of 3]-Minibatch[  11-  20, 80.00%]: ce = 3.38633995 * 10240; time = 9.8590s; samplesPerSecond = 1038.6
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 0.07 seconds
MPI Rank 3: 08/04/2016 09:25:57: Finished Epoch[ 1 of 3]: [Training] ce = 3.67886901 * 102399; totalSamplesSeen = 102399; learningRatePerSample = 9.9999997e-005; epochTime=24.8883s
MPI Rank 3: 08/04/2016 09:25:59: Final Results: Minibatch[1-26]: ce = 2.50976880 * 102399; perplexity = 12.30208550
MPI Rank 3: 08/04/2016 09:25:59: Finished Epoch[ 1 of 3]: [Validate] ce = 2.50976880 * 102399
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:26:02: Starting Epoch 2: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:26:02: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.08 seconds
MPI Rank 3: 08/04/2016 09:26:12:  Epoch[ 2 of 3]-Minibatch[   1-  10, 40.00%]: ce = 2.29999523 * 10240; time = 9.7327s; samplesPerSecond = 1052.1
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 0.08 seconds
MPI Rank 3: 08/04/2016 09:26:22:  Epoch[ 2 of 3]-Minibatch[  11-  20, 80.00%]: ce = 2.11324863 * 10240; time = 9.7692s; samplesPerSecond = 1048.2
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 0.07 seconds
MPI Rank 3: 08/04/2016 09:26:26: Finished Epoch[ 2 of 3]: [Training] ce = 2.18057679 * 102399; totalSamplesSeen = 204798; learningRatePerSample = 9.9999997e-005; epochTime=24.194s
MPI Rank 3: 08/04/2016 09:26:28: Final Results: Minibatch[1-26]: ce = 1.97361739 * 102399; perplexity = 7.19666260
MPI Rank 3: 08/04/2016 09:26:28: Finished Epoch[ 2 of 3]: [Validate] ce = 1.97361739 * 102399
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:26:31: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:26:31: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.89 seconds , average latency = 0.09 seconds
MPI Rank 3: 08/04/2016 09:26:41:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.90144348 * 10240; time = 10.1254s; samplesPerSecond = 1011.3
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.07 seconds
MPI Rank 3: 08/04/2016 09:26:51:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.87299423 * 10240; time = 9.9582s; samplesPerSecond = 1028.3
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 0.06 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 0.06 seconds
MPI Rank 3: 08/04/2016 09:26:56: Finished Epoch[ 3 of 3]: [Training] ce = 1.88833944 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=25.0571s
MPI Rank 3: 08/04/2016 09:26:58: Final Results: Minibatch[1-26]: ce = 1.81044051 * 102399; perplexity = 6.11313976
MPI Rank 3: 08/04/2016 09:26:58: Finished Epoch[ 3 of 3]: [Validate] ce = 1.81044051 * 102399
MPI Rank 3: 08/04/2016 09:27:01: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:01: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:01: __COMPLETED__
MPI Rank 3: ~MPIWrapper
=== Deleting last epoch data
==== Re-running from checkpoint
=== Running C:\Program Files\Microsoft MPI\Bin\/mpiexec.exe -n 4 C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu DeviceId=-1 timestamping=true numCPUThreads=1 stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
-------------------------------------------------------------------
Build info: 

		Built time: Aug  4 2016 06:18:04
		Last modified date: Thu Aug  4 03:39:14 2016
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		Math lib: mkl
		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
		CUB_PATH: C:\src\cub-1.4.1
		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
		Build Branch: HEAD
		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
		Built by svcphil on dphaim-26-new
		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
-------------------------------------------------------------------
Changed current directory to C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPIWrapper: initializing MPI
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
ping [requestnodes (before change)]: all 4 nodes responded
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (0) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (1) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (2) are in (participating)
requestnodes [MPIWrapper]: using 4 out of 4 MPI nodes (4 requested); we (3) are in (participating)
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
ping [requestnodes (after change)]: all 4 nodes responded
mpihelper: we are cog 1 in a gearbox of 4
mpihelper: we are cog 0 in a gearbox of 4
mpihelper: we are cog 2 in a gearbox of 4
mpihelper: we are cog 3 in a gearbox of 4
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: 4 nodes pinging each other
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
ping [mpihelper]: all 4 nodes responded
MPI Rank 0: 08/04/2016 09:27:03: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr_train.logrank0
MPI Rank 0: 08/04/2016 09:27:03: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:27:03: Build info: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:03: 		Built time: Aug  4 2016 06:18:04
MPI Rank 0: 08/04/2016 09:27:03: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 0: 08/04/2016 09:27:03: 		Build type: Release
MPI Rank 0: 08/04/2016 09:27:03: 		Build target: GPU
MPI Rank 0: 08/04/2016 09:27:03: 		With 1bit-SGD: no
MPI Rank 0: 08/04/2016 09:27:03: 		Math lib: mkl
MPI Rank 0: 08/04/2016 09:27:03: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 0: 08/04/2016 09:27:03: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 0: 08/04/2016 09:27:03: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 0: 08/04/2016 09:27:03: 		Build Branch: HEAD
MPI Rank 0: 08/04/2016 09:27:03: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 0: 08/04/2016 09:27:03: 		Built by svcphil on dphaim-26-new
MPI Rank 0: 08/04/2016 09:27:03: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 0: 08/04/2016 09:27:03: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:27:04: -------------------------------------------------------------------
MPI Rank 0: 08/04/2016 09:27:04: GPU info:
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 0: 08/04/2016 09:27:04: -------------------------------------------------------------------
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: Running on cntk-muc02 at 2016/08/04 09:27:04
MPI Rank 0: 08/04/2016 09:27:04: Command line: 
MPI Rank 0: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 09:27:04: modelPath=$RunDir$/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=$DeviceId$
MPI Rank 0:     minibatchSize = $MBSize$
MPI Rank 0:     modelPath = $modelPath$
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = $LRate$
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = $DataDir$/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: 08/04/2016 09:27:04: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: MBSize=4096
MPI Rank 0: LRate=0.0001
MPI Rank 0: DeviceId=-1
MPI Rank 0: parallelTrain=true
MPI Rank 0: command = train
MPI Rank 0: precision = float
MPI Rank 0: traceGPUMemoryAllocations=0
MPI Rank 0: train = [
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: NDLNetworkBuilder = [
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: reader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: cvReader = [
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: DeviceId=-1
MPI Rank 0: timestamping=true
MPI Rank 0: numCPUThreads=1
MPI Rank 0: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 0: configparameters: dssm.cntk:command=train
MPI Rank 0: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 0: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:cvReader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 0: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 0: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 0: configparameters: dssm.cntk:MBSize=4096
MPI Rank 0: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 0:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 0: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 0: configparameters: dssm.cntk:precision=float
MPI Rank 0: configparameters: dssm.cntk:reader=[
MPI Rank 0:     readerType = LibSVMBinaryReader
MPI Rank 0:     miniBatchMode = Partial
MPI Rank 0:     randomize = 0
MPI Rank 0:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 0: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 0: configparameters: dssm.cntk:timestamping=true
MPI Rank 0: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 0: configparameters: dssm.cntk:train=[
MPI Rank 0:     action = train
MPI Rank 0:     numMBsToShowResult=10
MPI Rank 0:     deviceId=-1
MPI Rank 0:     minibatchSize = 4096
MPI Rank 0:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0:     traceLevel = 1
MPI Rank 0:     SGD = [
MPI Rank 0:         epochSize=102399
MPI Rank 0:         learningRatesPerSample = 0.0001
MPI Rank 0:         momentumPerMB = 0.9
MPI Rank 0:         maxEpochs=3
MPI Rank 0:         ParallelTrain=[
MPI Rank 0:             parallelizationStartEpoch=1
MPI Rank 0:             parallelizationMethod=ModelAveragingSGD
MPI Rank 0:             distributedMBReading=true
MPI Rank 0:             ModelAveragingSGD=[
MPI Rank 0:                 SyncFrequencyInFrames=1024
MPI Rank 0:             ]
MPI Rank 0:         ]
MPI Rank 0: 		gradUpdateType=none
MPI Rank 0: 		gradientClippingWithTruncation=true
MPI Rank 0: 		clippingThresholdPerSample=1#INF
MPI Rank 0: 		keepCheckPointFiles = true
MPI Rank 0:     ]
MPI Rank 0: ]
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 0: 08/04/2016 09:27:04: Commands: train
MPI Rank 0: 08/04/2016 09:27:04: Precision = "float"
MPI Rank 0: 08/04/2016 09:27:04: Using 1 CPU threads.
MPI Rank 0: 08/04/2016 09:27:04: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 0: 08/04/2016 09:27:04: CNTKCommandTrainInfo: train : 3
MPI Rank 0: 08/04/2016 09:27:04: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: ##############################################################################
MPI Rank 0: 08/04/2016 09:27:04: #                                                                            #
MPI Rank 0: 08/04/2016 09:27:04: # Action "train"                                                             #
MPI Rank 0: 08/04/2016 09:27:04: #                                                                            #
MPI Rank 0: 08/04/2016 09:27:04: ##############################################################################
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: CNTKCommandTrainBegin: train
MPI Rank 0: NDLBuilder Using CPU
MPI Rank 0: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:04: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 0: 
MPI Rank 0: Post-processing network...
MPI Rank 0: 
MPI Rank 0: 2 roots:
MPI Rank 0: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 0: 	ce = CrossEntropyWithSoftmax()
MPI Rank 0: 
MPI Rank 0: Validating network. 21 nodes to process in pass 1.
MPI Rank 0: 
MPI Rank 0: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 0: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 0: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 0: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 0: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 0: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 0: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 0: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 0: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 0: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 0: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 0: 
MPI Rank 0: Validating network. 11 nodes to process in pass 2.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Validating network, final pass.
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 0: 
MPI Rank 0: Post-processing network complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:06: Loaded model with 21 nodes on CPU.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:06: Training criterion node(s):
MPI Rank 0: 08/04/2016 09:27:06: 	ce = CrossEntropyWithSoftmax
MPI Rank 0: 
MPI Rank 0: 
MPI Rank 0: Allocating matrices for forward and/or backward propagation.
MPI Rank 0: 
MPI Rank 0: Memory Sharing Structure:
MPI Rank 0: 
MPI Rank 0: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 0: 00000003DFE1A470: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 0: 00000003DFE1A5B0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 0: 00000003DFE1A650: {[WQ0 Value[288 x 49292]] }
MPI Rank 0: 00000003DFE1A790: {[SIM Gradient[51 x *1]] }
MPI Rank 0: 00000003DFE1AB50: {[WQ1 Value[64 x 288]] }
MPI Rank 0: 00000003DFE1ABF0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 0: 00000003DFE1AD30: {[WD1_D Gradient[64 x *1]] }
MPI Rank 0: 00000003DFE1AE70: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 0: 00000003DFE1AF10: {[ce Gradient[1]] }
MPI Rank 0: 00000003DFE1B0F0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 0: 00000003DFE1B370: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 0: 00000003DFE1B730: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 0: 00000003DFE1B870: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 0: 00000003DFE1BCD0: {[ce Value[1]] }
MPI Rank 0: 00000003DFE1BE10: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 0: 00000003DFE1BF50: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 0: 00000003DFE1BFF0: {[SIM Value[51 x *1]] }
MPI Rank 0: 00000003DFE1C090: {[WQ0_Q Value[288 x *1]] }
MPI Rank 0: 00000003DFE1C310: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 0: 00000003EE7DEA30: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 0: 00000003EE7DECB0: {[WD1 Value[64 x 288]] }
MPI Rank 0: 00000003EE7DED50: {[Keyword Value[49292 x *1]] }
MPI Rank 0: 00000003EE7DEE90: {[N Value[1 x 1]] }
MPI Rank 0: 00000003EE7DEFD0: {[Query Value[49292 x *1]] }
MPI Rank 0: 00000003EE7DF430: {[S Value[1 x 1]] }
MPI Rank 0: 00000003EE7DF610: {[WD0 Value[288 x 49292]] }
MPI Rank 0: 00000003EE7DF750: {[G Value[1 x 1]] }
MPI Rank 0: 
MPI Rank 0: Parallel training (4 workers) using ModelAveraging
MPI Rank 0: 08/04/2016 09:27:06: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:09: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 0: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.09 seconds
MPI Rank 0: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.05 seconds
MPI Rank 0: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.04 seconds
MPI Rank 0: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.05 seconds
MPI Rank 0: 08/04/2016 09:27:19:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.87912178 * 10240; time = 9.8292s; samplesPerSecond = 1041.8
MPI Rank 0: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.06 seconds
MPI Rank 0: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.07 seconds
MPI Rank 0: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 0.08 seconds
MPI Rank 0: 08/04/2016 09:27:29:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.79176941 * 10240; time = 9.9515s; samplesPerSecond = 1029.0
MPI Rank 0: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 0.08 seconds
MPI Rank 0: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 0.08 seconds
MPI Rank 0: 08/04/2016 09:27:34: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.9694s
MPI Rank 0: 08/04/2016 09:27:36: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 0: 08/04/2016 09:27:36: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 0: 08/04/2016 09:27:37: SGD: Saving checkpoint model 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net'
MPI Rank 0: 08/04/2016 09:27:39: CNTKCommandTrainEnd: train
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:39: Action "train" complete.
MPI Rank 0: 
MPI Rank 0: 08/04/2016 09:27:39: __COMPLETED__
MPI Rank 0: ~MPIWrapper
MPI Rank 1: 08/04/2016 09:27:04: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr_train.logrank1
MPI Rank 1: 08/04/2016 09:27:04: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:27:04: Build info: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: 		Built time: Aug  4 2016 06:18:04
MPI Rank 1: 08/04/2016 09:27:04: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 1: 08/04/2016 09:27:04: 		Build type: Release
MPI Rank 1: 08/04/2016 09:27:04: 		Build target: GPU
MPI Rank 1: 08/04/2016 09:27:04: 		With 1bit-SGD: no
MPI Rank 1: 08/04/2016 09:27:04: 		Math lib: mkl
MPI Rank 1: 08/04/2016 09:27:04: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 1: 08/04/2016 09:27:04: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 1: 08/04/2016 09:27:04: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 1: 08/04/2016 09:27:04: 		Build Branch: HEAD
MPI Rank 1: 08/04/2016 09:27:04: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 1: 08/04/2016 09:27:04: 		Built by svcphil on dphaim-26-new
MPI Rank 1: 08/04/2016 09:27:04: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 1: 08/04/2016 09:27:04: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:27:04: -------------------------------------------------------------------
MPI Rank 1: 08/04/2016 09:27:04: GPU info:
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 1: 08/04/2016 09:27:04: -------------------------------------------------------------------
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: Running on cntk-muc02 at 2016/08/04 09:27:04
MPI Rank 1: 08/04/2016 09:27:04: Command line: 
MPI Rank 1: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 09:27:04: modelPath=$RunDir$/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=$DeviceId$
MPI Rank 1:     minibatchSize = $MBSize$
MPI Rank 1:     modelPath = $modelPath$
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = $LRate$
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = $DataDir$/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: 08/04/2016 09:27:04: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: MBSize=4096
MPI Rank 1: LRate=0.0001
MPI Rank 1: DeviceId=-1
MPI Rank 1: parallelTrain=true
MPI Rank 1: command = train
MPI Rank 1: precision = float
MPI Rank 1: traceGPUMemoryAllocations=0
MPI Rank 1: train = [
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: NDLNetworkBuilder = [
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: reader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: cvReader = [
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: DeviceId=-1
MPI Rank 1: timestamping=true
MPI Rank 1: numCPUThreads=1
MPI Rank 1: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 1: configparameters: dssm.cntk:command=train
MPI Rank 1: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 1: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:cvReader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 1: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 1: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 1: configparameters: dssm.cntk:MBSize=4096
MPI Rank 1: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 1:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 1: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 1: configparameters: dssm.cntk:precision=float
MPI Rank 1: configparameters: dssm.cntk:reader=[
MPI Rank 1:     readerType = LibSVMBinaryReader
MPI Rank 1:     miniBatchMode = Partial
MPI Rank 1:     randomize = 0
MPI Rank 1:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 1: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 1: configparameters: dssm.cntk:timestamping=true
MPI Rank 1: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 1: configparameters: dssm.cntk:train=[
MPI Rank 1:     action = train
MPI Rank 1:     numMBsToShowResult=10
MPI Rank 1:     deviceId=-1
MPI Rank 1:     minibatchSize = 4096
MPI Rank 1:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1:     traceLevel = 1
MPI Rank 1:     SGD = [
MPI Rank 1:         epochSize=102399
MPI Rank 1:         learningRatesPerSample = 0.0001
MPI Rank 1:         momentumPerMB = 0.9
MPI Rank 1:         maxEpochs=3
MPI Rank 1:         ParallelTrain=[
MPI Rank 1:             parallelizationStartEpoch=1
MPI Rank 1:             parallelizationMethod=ModelAveragingSGD
MPI Rank 1:             distributedMBReading=true
MPI Rank 1:             ModelAveragingSGD=[
MPI Rank 1:                 SyncFrequencyInFrames=1024
MPI Rank 1:             ]
MPI Rank 1:         ]
MPI Rank 1: 		gradUpdateType=none
MPI Rank 1: 		gradientClippingWithTruncation=true
MPI Rank 1: 		clippingThresholdPerSample=1#INF
MPI Rank 1: 		keepCheckPointFiles = true
MPI Rank 1:     ]
MPI Rank 1: ]
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 1: 08/04/2016 09:27:04: Commands: train
MPI Rank 1: 08/04/2016 09:27:04: Precision = "float"
MPI Rank 1: 08/04/2016 09:27:04: Using 1 CPU threads.
MPI Rank 1: 08/04/2016 09:27:04: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 1: 08/04/2016 09:27:04: CNTKCommandTrainInfo: train : 3
MPI Rank 1: 08/04/2016 09:27:04: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: ##############################################################################
MPI Rank 1: 08/04/2016 09:27:04: #                                                                            #
MPI Rank 1: 08/04/2016 09:27:04: # Action "train"                                                             #
MPI Rank 1: 08/04/2016 09:27:04: #                                                                            #
MPI Rank 1: 08/04/2016 09:27:04: ##############################################################################
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: CNTKCommandTrainBegin: train
MPI Rank 1: NDLBuilder Using CPU
MPI Rank 1: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:04: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 1: 
MPI Rank 1: Post-processing network...
MPI Rank 1: 
MPI Rank 1: 2 roots:
MPI Rank 1: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 1: 	ce = CrossEntropyWithSoftmax()
MPI Rank 1: 
MPI Rank 1: Validating network. 21 nodes to process in pass 1.
MPI Rank 1: 
MPI Rank 1: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 1: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 1: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 1: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 1: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 1: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 1: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 1: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 1: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 1: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 1: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 1: 
MPI Rank 1: Validating network. 11 nodes to process in pass 2.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Validating network, final pass.
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 1: 
MPI Rank 1: Post-processing network complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:06: Loaded model with 21 nodes on CPU.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:06: Training criterion node(s):
MPI Rank 1: 08/04/2016 09:27:06: 	ce = CrossEntropyWithSoftmax
MPI Rank 1: 
MPI Rank 1: 
MPI Rank 1: Allocating matrices for forward and/or backward propagation.
MPI Rank 1: 
MPI Rank 1: Memory Sharing Structure:
MPI Rank 1: 
MPI Rank 1: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 1: 000000FA672896B0: {[WD0 Value[288 x 49292]] }
MPI Rank 1: 000000FA672899D0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 1: 000000FA67289C50: {[WD1 Value[64 x 288]] }
MPI Rank 1: 000000FA6728A150: {[N Value[1 x 1]] }
MPI Rank 1: 000000FA6728A1F0: {[Keyword Value[49292 x *1]] }
MPI Rank 1: 000000FA6728A290: {[G Value[1 x 1]] }
MPI Rank 1: 000000FA6728A470: {[Query Value[49292 x *1]] }
MPI Rank 1: 000000FA6728A5B0: {[S Value[1 x 1]] }
MPI Rank 1: 000000FA69E285B0: {[SIM Gradient[51 x *1]] }
MPI Rank 1: 000000FA69E286F0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 1: 000000FA69E288D0: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 1: 000000FA69E28A10: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 1: 000000FA69E28AB0: {[WQ0 Value[288 x 49292]] }
MPI Rank 1: 000000FA69E28B50: {[WD0 Gradient[288 x 49292]] }
MPI Rank 1: 000000FA69E28C90: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 1: 000000FA69E28F10: {[WQ0_Q Value[288 x *1]] }
MPI Rank 1: 000000FA69E28FB0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 1: 000000FA69E290F0: {[ce Value[1]] }
MPI Rank 1: 000000FA69E29190: {[ce Gradient[1]] }
MPI Rank 1: 000000FA69E292D0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 1: 000000FA69E294B0: {[WQ1 Value[64 x 288]] }
MPI Rank 1: 000000FA69E297D0: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 1: 000000FA69E29870: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 1: 000000FA69E299B0: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 1: 000000FA69E29B90: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 1: 000000FA69E29CD0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 1: 000000FA69E29EB0: {[SIM Value[51 x *1]] }
MPI Rank 1: 
MPI Rank 1: Parallel training (4 workers) using ModelAveraging
MPI Rank 1: 08/04/2016 09:27:06: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:09: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 1: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
MPI Rank 1: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.05 seconds
MPI Rank 1: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.63 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.08 seconds
MPI Rank 1: 08/04/2016 09:27:19:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.94872780 * 10240; time = 9.8186s; samplesPerSecond = 1042.9
MPI Rank 1: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 0.07 seconds
MPI Rank 1: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 0.08 seconds
MPI Rank 1: 08/04/2016 09:27:29:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.89873848 * 10240; time = 9.9500s; samplesPerSecond = 1029.1
MPI Rank 1: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 0.09 seconds
MPI Rank 1: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 1.87 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.87 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 0.08 seconds
MPI Rank 1: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 0.08 seconds
MPI Rank 1: 08/04/2016 09:27:34: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.9698s
MPI Rank 1: 08/04/2016 09:27:36: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 1: 08/04/2016 09:27:36: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 1: 08/04/2016 09:27:39: CNTKCommandTrainEnd: train
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:39: Action "train" complete.
MPI Rank 1: 
MPI Rank 1: 08/04/2016 09:27:39: __COMPLETED__
MPI Rank 1: ~MPIWrapper
MPI Rank 2: 08/04/2016 09:27:04: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr_train.logrank2
MPI Rank 2: 08/04/2016 09:27:04: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:27:04: Build info: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:04: 		Built time: Aug  4 2016 06:18:04
MPI Rank 2: 08/04/2016 09:27:04: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 2: 08/04/2016 09:27:04: 		Build type: Release
MPI Rank 2: 08/04/2016 09:27:04: 		Build target: GPU
MPI Rank 2: 08/04/2016 09:27:04: 		With 1bit-SGD: no
MPI Rank 2: 08/04/2016 09:27:04: 		Math lib: mkl
MPI Rank 2: 08/04/2016 09:27:04: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 2: 08/04/2016 09:27:04: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 2: 08/04/2016 09:27:04: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 2: 08/04/2016 09:27:04: 		Build Branch: HEAD
MPI Rank 2: 08/04/2016 09:27:04: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 2: 08/04/2016 09:27:04: 		Built by svcphil on dphaim-26-new
MPI Rank 2: 08/04/2016 09:27:04: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 2: 08/04/2016 09:27:04: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:27:05: -------------------------------------------------------------------
MPI Rank 2: 08/04/2016 09:27:05: GPU info:
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 2: 08/04/2016 09:27:05: -------------------------------------------------------------------
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: Running on cntk-muc02 at 2016/08/04 09:27:05
MPI Rank 2: 08/04/2016 09:27:05: Command line: 
MPI Rank 2: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 09:27:05: modelPath=$RunDir$/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=$DeviceId$
MPI Rank 2:     minibatchSize = $MBSize$
MPI Rank 2:     modelPath = $modelPath$
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = $LRate$
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = $DataDir$/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: 08/04/2016 09:27:05: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: MBSize=4096
MPI Rank 2: LRate=0.0001
MPI Rank 2: DeviceId=-1
MPI Rank 2: parallelTrain=true
MPI Rank 2: command = train
MPI Rank 2: precision = float
MPI Rank 2: traceGPUMemoryAllocations=0
MPI Rank 2: train = [
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: NDLNetworkBuilder = [
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: reader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: cvReader = [
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: DeviceId=-1
MPI Rank 2: timestamping=true
MPI Rank 2: numCPUThreads=1
MPI Rank 2: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 2: configparameters: dssm.cntk:command=train
MPI Rank 2: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 2: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:cvReader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 2: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 2: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 2: configparameters: dssm.cntk:MBSize=4096
MPI Rank 2: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 2:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 2: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 2: configparameters: dssm.cntk:precision=float
MPI Rank 2: configparameters: dssm.cntk:reader=[
MPI Rank 2:     readerType = LibSVMBinaryReader
MPI Rank 2:     miniBatchMode = Partial
MPI Rank 2:     randomize = 0
MPI Rank 2:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 2: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 2: configparameters: dssm.cntk:timestamping=true
MPI Rank 2: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 2: configparameters: dssm.cntk:train=[
MPI Rank 2:     action = train
MPI Rank 2:     numMBsToShowResult=10
MPI Rank 2:     deviceId=-1
MPI Rank 2:     minibatchSize = 4096
MPI Rank 2:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2:     traceLevel = 1
MPI Rank 2:     SGD = [
MPI Rank 2:         epochSize=102399
MPI Rank 2:         learningRatesPerSample = 0.0001
MPI Rank 2:         momentumPerMB = 0.9
MPI Rank 2:         maxEpochs=3
MPI Rank 2:         ParallelTrain=[
MPI Rank 2:             parallelizationStartEpoch=1
MPI Rank 2:             parallelizationMethod=ModelAveragingSGD
MPI Rank 2:             distributedMBReading=true
MPI Rank 2:             ModelAveragingSGD=[
MPI Rank 2:                 SyncFrequencyInFrames=1024
MPI Rank 2:             ]
MPI Rank 2:         ]
MPI Rank 2: 		gradUpdateType=none
MPI Rank 2: 		gradientClippingWithTruncation=true
MPI Rank 2: 		clippingThresholdPerSample=1#INF
MPI Rank 2: 		keepCheckPointFiles = true
MPI Rank 2:     ]
MPI Rank 2: ]
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 2: 08/04/2016 09:27:05: Commands: train
MPI Rank 2: 08/04/2016 09:27:05: Precision = "float"
MPI Rank 2: 08/04/2016 09:27:05: Using 1 CPU threads.
MPI Rank 2: 08/04/2016 09:27:05: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 2: 08/04/2016 09:27:05: CNTKCommandTrainInfo: train : 3
MPI Rank 2: 08/04/2016 09:27:05: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: ##############################################################################
MPI Rank 2: 08/04/2016 09:27:05: #                                                                            #
MPI Rank 2: 08/04/2016 09:27:05: # Action "train"                                                             #
MPI Rank 2: 08/04/2016 09:27:05: #                                                                            #
MPI Rank 2: 08/04/2016 09:27:05: ##############################################################################
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: CNTKCommandTrainBegin: train
MPI Rank 2: NDLBuilder Using CPU
MPI Rank 2: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:05: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 2: 
MPI Rank 2: Post-processing network...
MPI Rank 2: 
MPI Rank 2: 2 roots:
MPI Rank 2: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 2: 	ce = CrossEntropyWithSoftmax()
MPI Rank 2: 
MPI Rank 2: Validating network. 21 nodes to process in pass 1.
MPI Rank 2: 
MPI Rank 2: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 2: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 2: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 2: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 2: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 2: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 2: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 2: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 2: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 2: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 2: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 2: 
MPI Rank 2: Validating network. 11 nodes to process in pass 2.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Validating network, final pass.
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 2: 
MPI Rank 2: Post-processing network complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:06: Loaded model with 21 nodes on CPU.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:06: Training criterion node(s):
MPI Rank 2: 08/04/2016 09:27:06: 	ce = CrossEntropyWithSoftmax
MPI Rank 2: 
MPI Rank 2: 
MPI Rank 2: Allocating matrices for forward and/or backward propagation.
MPI Rank 2: 
MPI Rank 2: Memory Sharing Structure:
MPI Rank 2: 
MPI Rank 2: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 2: 00000028AA661870: {[N Value[1 x 1]] }
MPI Rank 2: 00000028AA661910: {[WD0 Value[288 x 49292]] }
MPI Rank 2: 00000028AA661B90: {[WD1 Value[64 x 288]] }
MPI Rank 2: 00000028AA661C30: {[G Value[1 x 1]] }
MPI Rank 2: 00000028AA661D70: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 2: 00000028AA661EB0: {[Keyword Value[49292 x *1]] }
MPI Rank 2: 00000028AA662310: {[Query Value[49292 x *1]] }
MPI Rank 2: 00000028AA6623B0: {[S Value[1 x 1]] }
MPI Rank 2: 00000028AA867050: {[WQ0_Q Value[288 x *1]] }
MPI Rank 2: 00000028AA8670F0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 2: 00000028AA867230: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 2: 00000028AA867410: {[ce Gradient[1]] }
MPI Rank 2: 00000028AA8674B0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 2: 00000028AA867550: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 2: 00000028AA867690: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 2: 00000028AA867730: {[ce Value[1]] }
MPI Rank 2: 00000028AA8679B0: {[WD1_D Gradient[64 x *1]] }
MPI Rank 2: 00000028AA867E10: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 2: 00000028AA867F50: {[WQ1 Value[64 x 288]] }
MPI Rank 2: 00000028AA868310: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 2: 00000028AA8683B0: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 2: 00000028AA8684F0: {[WQ0 Value[288 x 49292]] }
MPI Rank 2: 00000028AA868590: {[SIM Value[51 x *1]] }
MPI Rank 2: 00000028AA868630: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 2: 00000028AA8686D0: {[SIM Gradient[51 x *1]] }
MPI Rank 2: 00000028AA868A90: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 2: 00000028AA868B30: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 2: 
MPI Rank 2: Parallel training (4 workers) using ModelAveraging
MPI Rank 2: 08/04/2016 09:27:06: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:09: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 2: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
MPI Rank 2: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.04 seconds
MPI Rank 2: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.05 seconds
MPI Rank 2: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.06 seconds
MPI Rank 2: 08/04/2016 09:27:19:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.96648235 * 10240; time = 9.8793s; samplesPerSecond = 1036.5
MPI Rank 2: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.08 seconds
MPI Rank 2: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.06 seconds
MPI Rank 2: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 0.07 seconds
MPI Rank 2: 08/04/2016 09:27:29:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.91512318 * 10240; time = 9.9515s; samplesPerSecond = 1029.0
MPI Rank 2: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 0.07 seconds
MPI Rank 2: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 0.07 seconds
MPI Rank 2: 08/04/2016 09:27:34: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.9691s
MPI Rank 2: 08/04/2016 09:27:36: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 2: 08/04/2016 09:27:36: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 2: 08/04/2016 09:27:39: CNTKCommandTrainEnd: train
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:39: Action "train" complete.
MPI Rank 2: 
MPI Rank 2: 08/04/2016 09:27:39: __COMPLETED__
MPI Rank 2: ~MPIWrapper
MPI Rank 3: 08/04/2016 09:27:05: Redirecting stderr to file C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr_train.logrank3
MPI Rank 3: 08/04/2016 09:27:05: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:27:05: Build info: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: 		Built time: Aug  4 2016 06:18:04
MPI Rank 3: 08/04/2016 09:27:05: 		Last modified date: Thu Aug  4 03:39:14 2016
MPI Rank 3: 08/04/2016 09:27:05: 		Build type: Release
MPI Rank 3: 08/04/2016 09:27:05: 		Build target: GPU
MPI Rank 3: 08/04/2016 09:27:05: 		With 1bit-SGD: no
MPI Rank 3: 08/04/2016 09:27:05: 		Math lib: mkl
MPI Rank 3: 08/04/2016 09:27:05: 		CUDA_PATH: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5
MPI Rank 3: 08/04/2016 09:27:05: 		CUB_PATH: C:\src\cub-1.4.1
MPI Rank 3: 08/04/2016 09:27:05: 		CUDNN_PATH: c:\NVIDIA\cudnn-4.0\cuda
MPI Rank 3: 08/04/2016 09:27:05: 		Build Branch: HEAD
MPI Rank 3: 08/04/2016 09:27:05: 		Build SHA1: 0f61695dfc335f2406284d335678f57c215e6e9c
MPI Rank 3: 08/04/2016 09:27:05: 		Built by svcphil on dphaim-26-new
MPI Rank 3: 08/04/2016 09:27:05: 		Build Path: c:\jenkins\workspace\CNTK-Build-Windows@3\Source\CNTK\
MPI Rank 3: 08/04/2016 09:27:05: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:27:05: -------------------------------------------------------------------
MPI Rank 3: 08/04/2016 09:27:05: GPU info:
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: 		Device[0]: cores = 2496; computeCapability = 5.2; type = "Quadro M4000"; memory = 8192 MB
MPI Rank 3: 08/04/2016 09:27:05: -------------------------------------------------------------------
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: Running on cntk-muc02 at 2016/08/04 09:27:05
MPI Rank 3: 08/04/2016 09:27:05: Command line: 
MPI Rank 3: C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.cntk  currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM  OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu  DeviceId=-1  timestamping=true  numCPUThreads=1  stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: >>>>>>>>>>>>>>>>>>>> RAW CONFIG (VARIABLES NOT RESOLVED) >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 09:27:05: modelPath=$RunDir$/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=$DeviceId$
MPI Rank 3:     minibatchSize = $MBSize$
MPI Rank 3:     modelPath = $modelPath$
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = $LRate$
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = $ConfigDir$/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = $DataDir$/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: <<<<<<<<<<<<<<<<<<<< RAW CONFIG (VARIABLES NOT RESOLVED)  <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: >>>>>>>>>>>>>>>>>>>> RAW CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: 08/04/2016 09:27:05: modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: MBSize=4096
MPI Rank 3: LRate=0.0001
MPI Rank 3: DeviceId=-1
MPI Rank 3: parallelTrain=true
MPI Rank 3: command = train
MPI Rank 3: precision = float
MPI Rank 3: traceGPUMemoryAllocations=0
MPI Rank 3: train = [
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: NDLNetworkBuilder = [
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: reader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: cvReader = [
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: DeviceId=-1
MPI Rank 3: timestamping=true
MPI Rank 3: numCPUThreads=1
MPI Rank 3: stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: <<<<<<<<<<<<<<<<<<<< RAW CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: >>>>>>>>>>>>>>>>>>>> PROCESSED CONFIG WITH ALL VARIABLES RESOLVED >>>>>>>>>>>>>>>>>>>>
MPI Rank 3: configparameters: dssm.cntk:command=train
MPI Rank 3: configparameters: dssm.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM
MPI Rank 3: configparameters: dssm.cntk:currentDirectory=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:cvReader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:DataDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData
MPI Rank 3: configparameters: dssm.cntk:DeviceId=-1
MPI Rank 3: configparameters: dssm.cntk:LRate=0.0001
MPI Rank 3: configparameters: dssm.cntk:MBSize=4096
MPI Rank 3: configparameters: dssm.cntk:modelPath=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: configparameters: dssm.cntk:NDLNetworkBuilder=[
MPI Rank 3:     networkDescription = C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Text\SparseDSSM/dssm.ndl
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:numCPUThreads=1
MPI Rank 3: configparameters: dssm.cntk:OutputDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:parallelTrain=true
MPI Rank 3: configparameters: dssm.cntk:precision=float
MPI Rank 3: configparameters: dssm.cntk:reader=[
MPI Rank 3:     readerType = LibSVMBinaryReader
MPI Rank 3:     miniBatchMode = Partial
MPI Rank 3:     randomize = 0
MPI Rank 3:     file = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu\TestData/train.all.bin
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: configparameters: dssm.cntk:RunDir=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu
MPI Rank 3: configparameters: dssm.cntk:stderr=C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/stderr
MPI Rank 3: configparameters: dssm.cntk:timestamping=true
MPI Rank 3: configparameters: dssm.cntk:traceGPUMemoryAllocations=0
MPI Rank 3: configparameters: dssm.cntk:train=[
MPI Rank 3:     action = train
MPI Rank 3:     numMBsToShowResult=10
MPI Rank 3:     deviceId=-1
MPI Rank 3:     minibatchSize = 4096
MPI Rank 3:     modelPath = C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3:     traceLevel = 1
MPI Rank 3:     SGD = [
MPI Rank 3:         epochSize=102399
MPI Rank 3:         learningRatesPerSample = 0.0001
MPI Rank 3:         momentumPerMB = 0.9
MPI Rank 3:         maxEpochs=3
MPI Rank 3:         ParallelTrain=[
MPI Rank 3:             parallelizationStartEpoch=1
MPI Rank 3:             parallelizationMethod=ModelAveragingSGD
MPI Rank 3:             distributedMBReading=true
MPI Rank 3:             ModelAveragingSGD=[
MPI Rank 3:                 SyncFrequencyInFrames=1024
MPI Rank 3:             ]
MPI Rank 3:         ]
MPI Rank 3: 		gradUpdateType=none
MPI Rank 3: 		gradientClippingWithTruncation=true
MPI Rank 3: 		clippingThresholdPerSample=1#INF
MPI Rank 3: 		keepCheckPointFiles = true
MPI Rank 3:     ]
MPI Rank 3: ]
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: <<<<<<<<<<<<<<<<<<<< PROCESSED CONFIG WITH ALL VARIABLES RESOLVED <<<<<<<<<<<<<<<<<<<<
MPI Rank 3: 08/04/2016 09:27:05: Commands: train
MPI Rank 3: 08/04/2016 09:27:05: Precision = "float"
MPI Rank 3: 08/04/2016 09:27:05: Using 1 CPU threads.
MPI Rank 3: 08/04/2016 09:27:05: CNTKModelPath: C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net
MPI Rank 3: 08/04/2016 09:27:05: CNTKCommandTrainInfo: train : 3
MPI Rank 3: 08/04/2016 09:27:05: CNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 3
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: ##############################################################################
MPI Rank 3: 08/04/2016 09:27:05: #                                                                            #
MPI Rank 3: 08/04/2016 09:27:05: # Action "train"                                                             #
MPI Rank 3: 08/04/2016 09:27:05: #                                                                            #
MPI Rank 3: 08/04/2016 09:27:05: ##############################################################################
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: CNTKCommandTrainBegin: train
MPI Rank 3: NDLBuilder Using CPU
MPI Rank 3: WARNING: option syncFrequencyInFrames in ModelAveragingSGD is going to be deprecated. Please use blockSizePerWorker instead
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:05: Starting from checkpoint. Loading network from 'C:\Users\svcphil\AppData\Local\Temp\cntk-test-20160804091806.347762\Text_SparseDSSM@release_cpu/Models/dssm.net.2'.
MPI Rank 3: 
MPI Rank 3: Post-processing network...
MPI Rank 3: 
MPI Rank 3: 2 roots:
MPI Rank 3: 	SIM = CosDistanceWithNegativeSamples()
MPI Rank 3: 	ce = CrossEntropyWithSoftmax()
MPI Rank 3: 
MPI Rank 3: Validating network. 21 nodes to process in pass 1.
MPI Rank 3: 
MPI Rank 3: Validating --> WQ1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WQ0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Query = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WQ0_Q = Times (WQ0, Query) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ0_Q_Tanh = Tanh (WQ0_Q) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WQ1_Q = Times (WQ1, WQ0_Q_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WQ1_Q_Tanh = Tanh (WQ1_Q) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1 = LearnableParameter() :  -> [64 x 288]
MPI Rank 3: Validating --> WD0 = LearnableParameter() :  -> [288 x 49292]
MPI Rank 3: Validating --> Keyword = SparseInputValue() :  -> [49292 x *1]
MPI Rank 3: Validating --> WD0_D = Times (WD0, Keyword) : [288 x 49292], [49292 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD0_D_Tanh = Tanh (WD0_D) : [288 x *1] -> [288 x *1]
MPI Rank 3: Validating --> WD1_D = Times (WD1, WD0_D_Tanh) : [64 x 288], [288 x *1] -> [64 x *1]
MPI Rank 3: Validating --> WD1_D_Tanh = Tanh (WD1_D) : [64 x *1] -> [64 x *1]
MPI Rank 3: Validating --> S = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> N = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM = CosDistanceWithNegativeSamples (WQ1_Q_Tanh, WD1_D_Tanh, S, N) : [64 x *1], [64 x *1], [1 x 1], [1 x 1] -> [51 x *1]
MPI Rank 3: Validating --> DSSMLabel = InputValue() :  -> [51 x 1 x *1]
MPI Rank 3: Validating --> G = LearnableParameter() :  -> [1 x 1]
MPI Rank 3: Validating --> SIM_Scale = ElementTimes (G, SIM) : [1 x 1], [51 x *1] -> [51 x 1 x *1]
MPI Rank 3: Validating --> ce = CrossEntropyWithSoftmax (DSSMLabel, SIM_Scale) : [51 x 1 x *1], [51 x 1 x *1] -> [1]
MPI Rank 3: 
MPI Rank 3: Validating network. 11 nodes to process in pass 2.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Validating network, final pass.
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: 8 out of 21 nodes do not share the minibatch layout with the input data.
MPI Rank 3: 
MPI Rank 3: Post-processing network complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:07: Loaded model with 21 nodes on CPU.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:07: Training criterion node(s):
MPI Rank 3: 08/04/2016 09:27:07: 	ce = CrossEntropyWithSoftmax
MPI Rank 3: 
MPI Rank 3: 
MPI Rank 3: Allocating matrices for forward and/or backward propagation.
MPI Rank 3: 
MPI Rank 3: Memory Sharing Structure:
MPI Rank 3: 
MPI Rank 3: 0000000000000000: {[DSSMLabel Gradient[51 x 1 x *1]] [G Gradient[1 x 1]] [Keyword Gradient[49292 x *1]] [N Gradient[1 x 1]] [Query Gradient[49292 x *1]] [S Gradient[1 x 1]] }
MPI Rank 3: 00000027FE9A0B80: {[WD1 Value[64 x 288]] }
MPI Rank 3: 00000027FE9A0CC0: {[DSSMLabel Value[51 x 1 x *1]] }
MPI Rank 3: 00000027FE9A0EA0: {[S Value[1 x 1]] }
MPI Rank 3: 00000027FE9A0F40: {[WD0 Value[288 x 49292]] }
MPI Rank 3: 00000027FE9A11C0: {[G Value[1 x 1]] }
MPI Rank 3: 00000027FE9A14E0: {[N Value[1 x 1]] }
MPI Rank 3: 00000027FE9A1620: {[Query Value[49292 x *1]] }
MPI Rank 3: 00000027FE9A1940: {[Keyword Value[49292 x *1]] }
MPI Rank 3: 00000027FEB065F0: {[ce Gradient[1]] }
MPI Rank 3: 00000027FEB069B0: {[SIM Value[51 x *1]] }
MPI Rank 3: 00000027FEB06D70: {[WQ1 Gradient[64 x 288]] [WQ1_Q_Tanh Value[64 x *1]] }
MPI Rank 3: 00000027FEB06F50: {[WQ0_Q Value[288 x *1]] }
MPI Rank 3: 00000027FEB07130: {[ce Value[1]] }
MPI Rank 3: 00000027FEB071D0: {[WD0_D_Tanh Value[288 x *1]] }
MPI Rank 3: 00000027FEB07310: {[SIM_Scale Value[51 x 1 x *1]] [WQ0_Q_Tanh Gradient[288 x *1]] [WQ1_Q_Tanh Gradient[64 x *1]] }
MPI Rank 3: 00000027FEB07590: {[WQ1 Value[64 x 288]] }
MPI Rank 3: 00000027FEB076D0: {[SIM Gradient[51 x *1]] }
MPI Rank 3: 00000027FEB07810: {[SIM_Scale Gradient[51 x 1 x *1]] [WD0_D_Tanh Gradient[288 x *1]] [WD1_D_Tanh Gradient[64 x *1]] }
MPI Rank 3: 00000027FEB078B0: {[WQ0_Q Gradient[288 x *1]] [WQ1_Q Value[64 x *1]] }
MPI Rank 3: 00000027FEB07950: {[WD1 Gradient[64 x 288]] [WD1_D_Tanh Value[64 x *1]] }
MPI Rank 3: 00000027FEB07B30: {[WQ0_Q_Tanh Value[288 x *1]] }
MPI Rank 3: 00000027FEB07C70: {[WD1_D Gradient[64 x *1]] }
MPI Rank 3: 00000027FEB07D10: {[WQ0 Gradient[288 x 49292]] }
MPI Rank 3: 00000027FEB07DB0: {[WD0 Gradient[288 x 49292]] }
MPI Rank 3: 00000027FEB07EF0: {[WD0_D Value[288 x *1]] [WQ1_Q Gradient[64 x *1]] }
MPI Rank 3: 00000027FEB080D0: {[WD0_D Gradient[288 x *1]] [WD1_D Value[64 x *1]] }
MPI Rank 3: 00000027FEB08210: {[WQ0 Value[288 x 49292]] }
MPI Rank 3: 
MPI Rank 3: Parallel training (4 workers) using ModelAveraging
MPI Rank 3: 08/04/2016 09:27:07: No PreCompute nodes found, skipping PreCompute step.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:09: Starting Epoch 3: learning rate per sample = 0.000100  effective momentum = 0.900000  momentum as time constant = 38876.0 samples
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:09: Starting minibatch loop, distributed reading is ENABLED.
MPI Rank 3: 		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 2-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 3-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.12 seconds
MPI Rank 3: 		(model aggregation stats): 4-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 5-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.10 seconds
MPI Rank 3: 		(model aggregation stats): 6-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 7-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.11 seconds
MPI Rank 3: 		(model aggregation stats): 8-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.09 seconds
MPI Rank 3: 		(model aggregation stats): 9-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 10-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.09 seconds
MPI Rank 3: 08/04/2016 09:27:19:  Epoch[ 3 of 3]-Minibatch[   1-  10, 40.00%]: ce = 1.91057034 * 10240; time = 9.9483s; samplesPerSecond = 1029.3
MPI Rank 3: 		(model aggregation stats): 11-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 12-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 13-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 14-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 15-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.08 seconds
MPI Rank 3: 		(model aggregation stats): 16-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 17-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 18-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 19-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 20-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 0.07 seconds
MPI Rank 3: 08/04/2016 09:27:29:  Epoch[ 3 of 3]-Minibatch[  11-  20, 80.00%]: ce = 1.90474224 * 10240; time = 9.8966s; samplesPerSecond = 1034.7
MPI Rank 3: 		(model aggregation stats): 21-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 22-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 23-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 24-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 0.07 seconds
MPI Rank 3: 		(model aggregation stats): 25-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 0.07 seconds
MPI Rank 3: 08/04/2016 09:27:34: Finished Epoch[ 3 of 3]: [Training] ce = 1.89311328 * 102399; totalSamplesSeen = 307197; learningRatePerSample = 9.9999997e-005; epochTime=24.9698s
MPI Rank 3: 08/04/2016 09:27:36: Final Results: Minibatch[1-26]: ce = 1.82106100 * 102399; perplexity = 6.17841025
MPI Rank 3: 08/04/2016 09:27:36: Finished Epoch[ 3 of 3]: [Validate] ce = 1.82106100 * 102399
MPI Rank 3: 08/04/2016 09:27:39: CNTKCommandTrainEnd: train
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:39: Action "train" complete.
MPI Rank 3: 
MPI Rank 3: 08/04/2016 09:27:39: __COMPLETED__
MPI Rank 3: ~MPIWrapper